{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_dev.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XMCZL1Cs-XF"
      },
      "source": [
        "# Developing Models\n",
        "\n",
        "After redoing the feature selection and engineering, applying the same previous machine learning models on the dataset cleaned and without the errors that arised in the previous iterration. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XE5o0PsPpN2"
      },
      "source": [
        "# importing the packages \n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import SGDClassifier"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOkFF1djuLNI"
      },
      "source": [
        "## Loading the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKnEsPy9QOTV"
      },
      "source": [
        "final_train = pd.read_csv('ML_Artivatic_dataset/finalTrain.csv')\n",
        "final_test = pd.read_csv('ML_Artivatic_dataset/finalTest.csv')\n",
        "\n",
        "target_train = pd.read_csv('ML_Artivatic_dataset/target_train.csv')\n",
        "test_member_id = pd.read_csv('ML_Artivatic_dataset/test_member_id.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "HYz7HGLFRQZr",
        "outputId": "7f1e6db8-ee73-4948-ce29-a5d15210741b"
      },
      "source": [
        "final_train.drop(columns=['member_id'])\n",
        "final_train"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>member_id</th>\n",
              "      <th>emp_length</th>\n",
              "      <th>loan_amnt</th>\n",
              "      <th>funded_amnt</th>\n",
              "      <th>funded_amnt_inv</th>\n",
              "      <th>sub_grade</th>\n",
              "      <th>int_rate</th>\n",
              "      <th>annual_inc</th>\n",
              "      <th>dti</th>\n",
              "      <th>mths_since_last_delinq</th>\n",
              "      <th>mths_since_last_record</th>\n",
              "      <th>open_acc</th>\n",
              "      <th>revol_bal</th>\n",
              "      <th>revol_util</th>\n",
              "      <th>total_acc</th>\n",
              "      <th>total_rec_int</th>\n",
              "      <th>total_rec_late_fee</th>\n",
              "      <th>mths_since_last_major_derog</th>\n",
              "      <th>last_week_pay</th>\n",
              "      <th>tot_cur_bal</th>\n",
              "      <th>total_rev_hi_lim</th>\n",
              "      <th>tot_coll_amt</th>\n",
              "      <th>recoveries</th>\n",
              "      <th>collection_recovery_fee</th>\n",
              "      <th>term</th>\n",
              "      <th>acc_now_delinq</th>\n",
              "      <th>collections_12_mths_ex_med</th>\n",
              "      <th>loan_to_income</th>\n",
              "      <th>bad_state</th>\n",
              "      <th>avl_lines</th>\n",
              "      <th>int_paid</th>\n",
              "      <th>emi_paid_progress_perc</th>\n",
              "      <th>total_repayment_progress</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>58189336</td>\n",
              "      <td>9.0</td>\n",
              "      <td>14350</td>\n",
              "      <td>14350</td>\n",
              "      <td>14350.0</td>\n",
              "      <td>43</td>\n",
              "      <td>19.19</td>\n",
              "      <td>28700.0</td>\n",
              "      <td>33.88</td>\n",
              "      <td>50.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>22515.0</td>\n",
              "      <td>73.1</td>\n",
              "      <td>28.0</td>\n",
              "      <td>1173.84</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>28699.0</td>\n",
              "      <td>30800.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1173.84</td>\n",
              "      <td>16.560510</td>\n",
              "      <td>16.560510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>70011223</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4800</td>\n",
              "      <td>4800</td>\n",
              "      <td>4800.0</td>\n",
              "      <td>14</td>\n",
              "      <td>10.99</td>\n",
              "      <td>65000.0</td>\n",
              "      <td>3.64</td>\n",
              "      <td>31.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7624.0</td>\n",
              "      <td>23.2</td>\n",
              "      <td>13.0</td>\n",
              "      <td>83.95</td>\n",
              "      <td>0.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9974.0</td>\n",
              "      <td>32900.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.541667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>83.95</td>\n",
              "      <td>5.732484</td>\n",
              "      <td>5.732484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>70255675</td>\n",
              "      <td>2.0</td>\n",
              "      <td>10000</td>\n",
              "      <td>10000</td>\n",
              "      <td>10000.0</td>\n",
              "      <td>4</td>\n",
              "      <td>7.26</td>\n",
              "      <td>45000.0</td>\n",
              "      <td>18.42</td>\n",
              "      <td>31.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10877.0</td>\n",
              "      <td>31.2</td>\n",
              "      <td>19.0</td>\n",
              "      <td>56.47</td>\n",
              "      <td>0.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>38295.0</td>\n",
              "      <td>34900.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>56.47</td>\n",
              "      <td>5.732484</td>\n",
              "      <td>5.732484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1893936</td>\n",
              "      <td>10.0</td>\n",
              "      <td>15000</td>\n",
              "      <td>15000</td>\n",
              "      <td>15000.0</td>\n",
              "      <td>35</td>\n",
              "      <td>19.72</td>\n",
              "      <td>105000.0</td>\n",
              "      <td>14.97</td>\n",
              "      <td>46.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>13712.0</td>\n",
              "      <td>55.5</td>\n",
              "      <td>21.0</td>\n",
              "      <td>4858.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>55564.0</td>\n",
              "      <td>24700.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>4858.62</td>\n",
              "      <td>85.987261</td>\n",
              "      <td>85.987261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7652106</td>\n",
              "      <td>10.0</td>\n",
              "      <td>16000</td>\n",
              "      <td>16000</td>\n",
              "      <td>16000.0</td>\n",
              "      <td>12</td>\n",
              "      <td>10.64</td>\n",
              "      <td>52000.0</td>\n",
              "      <td>20.16</td>\n",
              "      <td>31.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>35835.0</td>\n",
              "      <td>76.2</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2296.41</td>\n",
              "      <td>0.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>47159.0</td>\n",
              "      <td>47033.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2296.41</td>\n",
              "      <td>61.146497</td>\n",
              "      <td>61.146497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>532423</th>\n",
              "      <td>31296187</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20000</td>\n",
              "      <td>20000</td>\n",
              "      <td>20000.0</td>\n",
              "      <td>15</td>\n",
              "      <td>12.49</td>\n",
              "      <td>75000.0</td>\n",
              "      <td>14.53</td>\n",
              "      <td>31.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>15775.0</td>\n",
              "      <td>63.6</td>\n",
              "      <td>34.0</td>\n",
              "      <td>2595.45</td>\n",
              "      <td>0.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>83087.0</td>\n",
              "      <td>24800.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>2595.45</td>\n",
              "      <td>41.401274</td>\n",
              "      <td>41.401274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>532424</th>\n",
              "      <td>29403184</td>\n",
              "      <td>10.0</td>\n",
              "      <td>12000</td>\n",
              "      <td>12000</td>\n",
              "      <td>12000.0</td>\n",
              "      <td>25</td>\n",
              "      <td>14.99</td>\n",
              "      <td>59000.0</td>\n",
              "      <td>22.97</td>\n",
              "      <td>31.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9453.0</td>\n",
              "      <td>53.1</td>\n",
              "      <td>29.0</td>\n",
              "      <td>2182.92</td>\n",
              "      <td>0.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>227812.0</td>\n",
              "      <td>17800.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.916667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>2182.92</td>\n",
              "      <td>26.819923</td>\n",
              "      <td>26.819923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>532425</th>\n",
              "      <td>7357607</td>\n",
              "      <td>8.0</td>\n",
              "      <td>18725</td>\n",
              "      <td>18725</td>\n",
              "      <td>18725.0</td>\n",
              "      <td>41</td>\n",
              "      <td>20.80</td>\n",
              "      <td>42504.0</td>\n",
              "      <td>27.27</td>\n",
              "      <td>26.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12085.0</td>\n",
              "      <td>49.9</td>\n",
              "      <td>26.0</td>\n",
              "      <td>645.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>26010.0</td>\n",
              "      <td>24200.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.269907</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>645.32</td>\n",
              "      <td>3.448276</td>\n",
              "      <td>3.448276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>532426</th>\n",
              "      <td>23182668</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21000</td>\n",
              "      <td>21000</td>\n",
              "      <td>21000.0</td>\n",
              "      <td>32</td>\n",
              "      <td>16.29</td>\n",
              "      <td>50000.0</td>\n",
              "      <td>14.91</td>\n",
              "      <td>31.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>20902.0</td>\n",
              "      <td>89.7</td>\n",
              "      <td>14.0</td>\n",
              "      <td>4619.79</td>\n",
              "      <td>0.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>29197.0</td>\n",
              "      <td>23300.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.380952</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4619.79</td>\n",
              "      <td>29.885057</td>\n",
              "      <td>29.885057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>532427</th>\n",
              "      <td>46122259</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10000</td>\n",
              "      <td>10000</td>\n",
              "      <td>10000.0</td>\n",
              "      <td>2</td>\n",
              "      <td>6.39</td>\n",
              "      <td>53000.0</td>\n",
              "      <td>17.80</td>\n",
              "      <td>31.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>10058.0</td>\n",
              "      <td>46.4</td>\n",
              "      <td>20.0</td>\n",
              "      <td>467.52</td>\n",
              "      <td>0.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>47866.0</td>\n",
              "      <td>21700.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.300000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>467.52</td>\n",
              "      <td>28.025478</td>\n",
              "      <td>28.025478</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>532428 rows × 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        member_id  emp_length  ...  emi_paid_progress_perc  total_repayment_progress\n",
              "0        58189336         9.0  ...               16.560510                 16.560510\n",
              "1        70011223         0.0  ...                5.732484                  5.732484\n",
              "2        70255675         2.0  ...                5.732484                  5.732484\n",
              "3         1893936        10.0  ...               85.987261                 85.987261\n",
              "4         7652106        10.0  ...               61.146497                 61.146497\n",
              "...           ...         ...  ...                     ...                       ...\n",
              "532423   31296187        10.0  ...               41.401274                 41.401274\n",
              "532424   29403184        10.0  ...               26.819923                 26.819923\n",
              "532425    7357607         8.0  ...                3.448276                  3.448276\n",
              "532426   23182668         1.0  ...               29.885057                 29.885057\n",
              "532427   46122259         0.0  ...               28.025478                 28.025478\n",
              "\n",
              "[532428 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r15lH0Z6RSCL"
      },
      "source": [
        "'''\n",
        "Split data set into train-test-cv\n",
        "Train model & predict\n",
        "'''\n",
        "# Split train and cross validation sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(np.array(final_train), np.array(target_train), test_size=0.30)\n",
        "eval_set=[(X_test, y_test)]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD1USRAoXVhz"
      },
      "source": [
        "Algorithms that we would be using "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTs1dqKkXEuy"
      },
      "source": [
        "algos = ['XGBoost','Random Forest', 'SGD Classifier', 'K Nearest Neighbors', 'Gradeient Boosting Classifier' ]\n",
        "performance = []"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFYub8xRSV6N"
      },
      "source": [
        "## XGBoost "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go7A9-AURuBW",
        "outputId": "41f0e88c-9f7c-4e53-f0b4-caa8b04ae3f3"
      },
      "source": [
        "print('Initializing xgboost.sklearn.XGBClassifier and starting training...')\n",
        "\n",
        "st = datetime.now()\n",
        "\n",
        "clf = xgboost.sklearn.XGBClassifier(\n",
        "    objective=\"binary:logistic\", \n",
        "    learning_rate=0.05, \n",
        "    seed=9616, \n",
        "    max_depth=20, \n",
        "    gamma=10, \n",
        "    n_estimators=500)\n",
        "\n",
        "clf.fit(X_train, y_train, early_stopping_rounds=20, eval_metric=\"auc\", eval_set=eval_set, verbose=True)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing xgboost.sklearn.XGBClassifier and starting training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0]\tvalidation_0-auc:0.968353\n",
            "Will train until validation_0-auc hasn't improved in 20 rounds.\n",
            "[1]\tvalidation_0-auc:0.968925\n",
            "[2]\tvalidation_0-auc:0.969234\n",
            "[3]\tvalidation_0-auc:0.97051\n",
            "[4]\tvalidation_0-auc:0.970719\n",
            "[5]\tvalidation_0-auc:0.970814\n",
            "[6]\tvalidation_0-auc:0.970925\n",
            "[7]\tvalidation_0-auc:0.971413\n",
            "[8]\tvalidation_0-auc:0.971546\n",
            "[9]\tvalidation_0-auc:0.971653\n",
            "[10]\tvalidation_0-auc:0.971846\n",
            "[11]\tvalidation_0-auc:0.971999\n",
            "[12]\tvalidation_0-auc:0.972216\n",
            "[13]\tvalidation_0-auc:0.972329\n",
            "[14]\tvalidation_0-auc:0.972553\n",
            "[15]\tvalidation_0-auc:0.972666\n",
            "[16]\tvalidation_0-auc:0.972711\n",
            "[17]\tvalidation_0-auc:0.972778\n",
            "[18]\tvalidation_0-auc:0.972819\n",
            "[19]\tvalidation_0-auc:0.972877\n",
            "[20]\tvalidation_0-auc:0.972931\n",
            "[21]\tvalidation_0-auc:0.972943\n",
            "[22]\tvalidation_0-auc:0.972952\n",
            "[23]\tvalidation_0-auc:0.972991\n",
            "[24]\tvalidation_0-auc:0.973032\n",
            "[25]\tvalidation_0-auc:0.973069\n",
            "[26]\tvalidation_0-auc:0.973079\n",
            "[27]\tvalidation_0-auc:0.97316\n",
            "[28]\tvalidation_0-auc:0.973192\n",
            "[29]\tvalidation_0-auc:0.973221\n",
            "[30]\tvalidation_0-auc:0.973255\n",
            "[31]\tvalidation_0-auc:0.973269\n",
            "[32]\tvalidation_0-auc:0.97332\n",
            "[33]\tvalidation_0-auc:0.973359\n",
            "[34]\tvalidation_0-auc:0.973417\n",
            "[35]\tvalidation_0-auc:0.973428\n",
            "[36]\tvalidation_0-auc:0.973481\n",
            "[37]\tvalidation_0-auc:0.973549\n",
            "[38]\tvalidation_0-auc:0.973613\n",
            "[39]\tvalidation_0-auc:0.973643\n",
            "[40]\tvalidation_0-auc:0.973692\n",
            "[41]\tvalidation_0-auc:0.973742\n",
            "[42]\tvalidation_0-auc:0.973774\n",
            "[43]\tvalidation_0-auc:0.973804\n",
            "[44]\tvalidation_0-auc:0.973843\n",
            "[45]\tvalidation_0-auc:0.973884\n",
            "[46]\tvalidation_0-auc:0.9739\n",
            "[47]\tvalidation_0-auc:0.973924\n",
            "[48]\tvalidation_0-auc:0.973918\n",
            "[49]\tvalidation_0-auc:0.973937\n",
            "[50]\tvalidation_0-auc:0.973977\n",
            "[51]\tvalidation_0-auc:0.973983\n",
            "[52]\tvalidation_0-auc:0.974023\n",
            "[53]\tvalidation_0-auc:0.974072\n",
            "[54]\tvalidation_0-auc:0.97412\n",
            "[55]\tvalidation_0-auc:0.974156\n",
            "[56]\tvalidation_0-auc:0.974185\n",
            "[57]\tvalidation_0-auc:0.974229\n",
            "[58]\tvalidation_0-auc:0.974274\n",
            "[59]\tvalidation_0-auc:0.974297\n",
            "[60]\tvalidation_0-auc:0.97434\n",
            "[61]\tvalidation_0-auc:0.974365\n",
            "[62]\tvalidation_0-auc:0.974398\n",
            "[63]\tvalidation_0-auc:0.974425\n",
            "[64]\tvalidation_0-auc:0.974457\n",
            "[65]\tvalidation_0-auc:0.974463\n",
            "[66]\tvalidation_0-auc:0.974483\n",
            "[67]\tvalidation_0-auc:0.974502\n",
            "[68]\tvalidation_0-auc:0.974518\n",
            "[69]\tvalidation_0-auc:0.974521\n",
            "[70]\tvalidation_0-auc:0.974558\n",
            "[71]\tvalidation_0-auc:0.974606\n",
            "[72]\tvalidation_0-auc:0.974625\n",
            "[73]\tvalidation_0-auc:0.974659\n",
            "[74]\tvalidation_0-auc:0.974673\n",
            "[75]\tvalidation_0-auc:0.974706\n",
            "[76]\tvalidation_0-auc:0.974715\n",
            "[77]\tvalidation_0-auc:0.974725\n",
            "[78]\tvalidation_0-auc:0.974746\n",
            "[79]\tvalidation_0-auc:0.974798\n",
            "[80]\tvalidation_0-auc:0.974813\n",
            "[81]\tvalidation_0-auc:0.974838\n",
            "[82]\tvalidation_0-auc:0.974854\n",
            "[83]\tvalidation_0-auc:0.974861\n",
            "[84]\tvalidation_0-auc:0.97487\n",
            "[85]\tvalidation_0-auc:0.974888\n",
            "[86]\tvalidation_0-auc:0.97491\n",
            "[87]\tvalidation_0-auc:0.974919\n",
            "[88]\tvalidation_0-auc:0.974948\n",
            "[89]\tvalidation_0-auc:0.974968\n",
            "[90]\tvalidation_0-auc:0.974985\n",
            "[91]\tvalidation_0-auc:0.975033\n",
            "[92]\tvalidation_0-auc:0.975048\n",
            "[93]\tvalidation_0-auc:0.975071\n",
            "[94]\tvalidation_0-auc:0.975086\n",
            "[95]\tvalidation_0-auc:0.975109\n",
            "[96]\tvalidation_0-auc:0.97514\n",
            "[97]\tvalidation_0-auc:0.975148\n",
            "[98]\tvalidation_0-auc:0.975157\n",
            "[99]\tvalidation_0-auc:0.975163\n",
            "[100]\tvalidation_0-auc:0.97518\n",
            "[101]\tvalidation_0-auc:0.975188\n",
            "[102]\tvalidation_0-auc:0.975199\n",
            "[103]\tvalidation_0-auc:0.975211\n",
            "[104]\tvalidation_0-auc:0.975216\n",
            "[105]\tvalidation_0-auc:0.975219\n",
            "[106]\tvalidation_0-auc:0.975233\n",
            "[107]\tvalidation_0-auc:0.975243\n",
            "[108]\tvalidation_0-auc:0.975257\n",
            "[109]\tvalidation_0-auc:0.975261\n",
            "[110]\tvalidation_0-auc:0.975261\n",
            "[111]\tvalidation_0-auc:0.975267\n",
            "[112]\tvalidation_0-auc:0.975281\n",
            "[113]\tvalidation_0-auc:0.975287\n",
            "[114]\tvalidation_0-auc:0.975296\n",
            "[115]\tvalidation_0-auc:0.9753\n",
            "[116]\tvalidation_0-auc:0.97531\n",
            "[117]\tvalidation_0-auc:0.975312\n",
            "[118]\tvalidation_0-auc:0.975315\n",
            "[119]\tvalidation_0-auc:0.975326\n",
            "[120]\tvalidation_0-auc:0.975339\n",
            "[121]\tvalidation_0-auc:0.975345\n",
            "[122]\tvalidation_0-auc:0.975353\n",
            "[123]\tvalidation_0-auc:0.975357\n",
            "[124]\tvalidation_0-auc:0.975363\n",
            "[125]\tvalidation_0-auc:0.975367\n",
            "[126]\tvalidation_0-auc:0.975376\n",
            "[127]\tvalidation_0-auc:0.975377\n",
            "[128]\tvalidation_0-auc:0.975384\n",
            "[129]\tvalidation_0-auc:0.975388\n",
            "[130]\tvalidation_0-auc:0.975391\n",
            "[131]\tvalidation_0-auc:0.9754\n",
            "[132]\tvalidation_0-auc:0.975406\n",
            "[133]\tvalidation_0-auc:0.975408\n",
            "[134]\tvalidation_0-auc:0.975411\n",
            "[135]\tvalidation_0-auc:0.975416\n",
            "[136]\tvalidation_0-auc:0.97542\n",
            "[137]\tvalidation_0-auc:0.975418\n",
            "[138]\tvalidation_0-auc:0.975426\n",
            "[139]\tvalidation_0-auc:0.975431\n",
            "[140]\tvalidation_0-auc:0.975429\n",
            "[141]\tvalidation_0-auc:0.975432\n",
            "[142]\tvalidation_0-auc:0.975434\n",
            "[143]\tvalidation_0-auc:0.975445\n",
            "[144]\tvalidation_0-auc:0.975444\n",
            "[145]\tvalidation_0-auc:0.975445\n",
            "[146]\tvalidation_0-auc:0.975451\n",
            "[147]\tvalidation_0-auc:0.975452\n",
            "[148]\tvalidation_0-auc:0.975457\n",
            "[149]\tvalidation_0-auc:0.975461\n",
            "[150]\tvalidation_0-auc:0.975469\n",
            "[151]\tvalidation_0-auc:0.97547\n",
            "[152]\tvalidation_0-auc:0.97547\n",
            "[153]\tvalidation_0-auc:0.975474\n",
            "[154]\tvalidation_0-auc:0.975474\n",
            "[155]\tvalidation_0-auc:0.975475\n",
            "[156]\tvalidation_0-auc:0.975478\n",
            "[157]\tvalidation_0-auc:0.975488\n",
            "[158]\tvalidation_0-auc:0.975491\n",
            "[159]\tvalidation_0-auc:0.975493\n",
            "[160]\tvalidation_0-auc:0.975498\n",
            "[161]\tvalidation_0-auc:0.975499\n",
            "[162]\tvalidation_0-auc:0.975504\n",
            "[163]\tvalidation_0-auc:0.975502\n",
            "[164]\tvalidation_0-auc:0.975504\n",
            "[165]\tvalidation_0-auc:0.975505\n",
            "[166]\tvalidation_0-auc:0.975505\n",
            "[167]\tvalidation_0-auc:0.975528\n",
            "[168]\tvalidation_0-auc:0.975527\n",
            "[169]\tvalidation_0-auc:0.975536\n",
            "[170]\tvalidation_0-auc:0.97554\n",
            "[171]\tvalidation_0-auc:0.975551\n",
            "[172]\tvalidation_0-auc:0.975554\n",
            "[173]\tvalidation_0-auc:0.975557\n",
            "[174]\tvalidation_0-auc:0.975576\n",
            "[175]\tvalidation_0-auc:0.975588\n",
            "[176]\tvalidation_0-auc:0.975594\n",
            "[177]\tvalidation_0-auc:0.975595\n",
            "[178]\tvalidation_0-auc:0.975597\n",
            "[179]\tvalidation_0-auc:0.97561\n",
            "[180]\tvalidation_0-auc:0.975609\n",
            "[181]\tvalidation_0-auc:0.975615\n",
            "[182]\tvalidation_0-auc:0.975615\n",
            "[183]\tvalidation_0-auc:0.975612\n",
            "[184]\tvalidation_0-auc:0.975612\n",
            "[185]\tvalidation_0-auc:0.975612\n",
            "[186]\tvalidation_0-auc:0.975612\n",
            "[187]\tvalidation_0-auc:0.975612\n",
            "[188]\tvalidation_0-auc:0.975612\n",
            "[189]\tvalidation_0-auc:0.975612\n",
            "[190]\tvalidation_0-auc:0.975612\n",
            "[191]\tvalidation_0-auc:0.975612\n",
            "[192]\tvalidation_0-auc:0.975612\n",
            "[193]\tvalidation_0-auc:0.975612\n",
            "[194]\tvalidation_0-auc:0.975612\n",
            "[195]\tvalidation_0-auc:0.975612\n",
            "[196]\tvalidation_0-auc:0.975612\n",
            "[197]\tvalidation_0-auc:0.975612\n",
            "[198]\tvalidation_0-auc:0.975612\n",
            "[199]\tvalidation_0-auc:0.975612\n",
            "[200]\tvalidation_0-auc:0.975612\n",
            "[201]\tvalidation_0-auc:0.975612\n",
            "Stopping. Best iteration:\n",
            "[181]\tvalidation_0-auc:0.975615\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=10,\n",
              "              learning_rate=0.05, max_delta_step=0, max_depth=20,\n",
              "              min_child_weight=1, missing=None, n_estimators=500, n_jobs=1,\n",
              "              nthread=None, objective='binary:logistic', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=9616,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgU2Wyv7R0Jw",
        "outputId": "fc85c60d-2355-4317-c044-6586da16988e"
      },
      "source": [
        "perf_xgboost = roc_auc_score(y_test, clf.predict(X_test))\n",
        "performance.append(xgboost)\n",
        "print(perf_xgboost)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9239974277998122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udIYfX-EaKDo"
      },
      "source": [
        "predictions = clf.predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCzoovWeYkY3"
      },
      "source": [
        "## Random Forest Classifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZI4kXBcYG1Q"
      },
      "source": [
        "final_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "imp = SimpleImputer(strategy='median')\n",
        "final_train = imp.fit_transform(final_train)\n",
        "# imp.fit_transform(target_train)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(final_train, target_train, test_size=0.30)\n",
        "eval_set=[(X_test, y_test)]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSEVcUyjaoEb",
        "outputId": "77eba871-47f8-4d12-bcc6-418fe3c05591"
      },
      "source": [
        "rf = RandomForestClassifier(n_estimators=100, verbose=5, n_jobs=-1)\n",
        "rf.fit(np.array(X_train), np.array(y_train))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  \n",
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building tree 1 of 100\n",
            "building tree 2 of 100\n",
            "building tree 3 of 100\n",
            "building tree 4 of 100\n",
            "building tree 5 of 100\n",
            "building tree 6 of 100\n",
            "building tree 7 of 100\n",
            "building tree 8 of 100\n",
            "building tree 9 of 100\n",
            "building tree 10 of 100\n",
            "building tree 11 of 100\n",
            "building tree 12 of 100\n",
            "building tree 13 of 100\n",
            "building tree 14 of 100\n",
            "building tree 15 of 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:   20.5s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building tree 16 of 100\n",
            "building tree 17 of 100\n",
            "building tree 18 of 100\n",
            "building tree 19 of 100\n",
            "building tree 20 of 100\n",
            "building tree 21 of 100\n",
            "building tree 22 of 100\n",
            "building tree 23 of 100\n",
            "building tree 24 of 100\n",
            "building tree 25 of 100\n",
            "building tree 26 of 100\n",
            "building tree 27 of 100\n",
            "building tree 28 of 100\n",
            "building tree 29 of 100\n",
            "building tree 30 of 100\n",
            "building tree 31 of 100\n",
            "building tree 32 of 100\n",
            "building tree 33 of 100\n",
            "building tree 34 of 100\n",
            "building tree 35 of 100\n",
            "building tree 36 of 100\n",
            "building tree 37 of 100\n",
            "building tree 38 of 100\n",
            "building tree 39 of 100\n",
            "building tree 40 of 100\n",
            "building tree 41 of 100\n",
            "building tree 42 of 100\n",
            "building tree 43 of 100\n",
            "building tree 44 of 100\n",
            "building tree 45 of 100\n",
            "building tree 46 of 100\n",
            "building tree 47 of 100\n",
            "building tree 48 of 100\n",
            "building tree 49 of 100\n",
            "building tree 50 of 100\n",
            "building tree 51 of 100\n",
            "building tree 52 of 100\n",
            "building tree 53 of 100\n",
            "building tree 54 of 100\n",
            "building tree 55 of 100\n",
            "building tree 56 of 100\n",
            "building tree 57 of 100\n",
            "building tree 58 of 100\n",
            "building tree 59 of 100\n",
            "building tree 60 of 100\n",
            "building tree 61 of 100\n",
            "building tree 62 of 100\n",
            "building tree 63 of 100\n",
            "building tree 64 of 100\n",
            "building tree 65 of 100\n",
            "building tree 66 of 100\n",
            "building tree 67 of 100\n",
            "building tree 68 of 100\n",
            "building tree 69 of 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:  1.6min\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building tree 70 of 100\n",
            "building tree 71 of 100\n",
            "building tree 72 of 100\n",
            "building tree 73 of 100\n",
            "building tree 74 of 100\n",
            "building tree 75 of 100\n",
            "building tree 76 of 100\n",
            "building tree 77 of 100\n",
            "building tree 78 of 100\n",
            "building tree 79 of 100\n",
            "building tree 80 of 100\n",
            "building tree 81 of 100\n",
            "building tree 82 of 100\n",
            "building tree 83 of 100\n",
            "building tree 84 of 100\n",
            "building tree 85 of 100\n",
            "building tree 86 of 100\n",
            "building tree 87 of 100\n",
            "building tree 88 of 100\n",
            "building tree 89 of 100\n",
            "building tree 90 of 100\n",
            "building tree 91 of 100\n",
            "building tree 92 of 100\n",
            "building tree 93 of 100\n",
            "building tree 94 of 100\n",
            "building tree 95 of 100\n",
            "building tree 96 of 100\n",
            "building tree 97 of 100\n",
            "building tree 98 of 100\n",
            "building tree 99 of 100\n",
            "building tree 100 of 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=-1, oob_score=False, random_state=None, verbose=5,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etKpiMIXbb1m",
        "outputId": "bc7360d1-b650-41e6-e2b3-10a9cd38956e"
      },
      "source": [
        "rf_perf = roc_auc_score(y_test, rf.predict(X_test))\n",
        "performance.append(rf_perf)\n",
        "rf_perf"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  14 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=2)]: Done  68 tasks      | elapsed:    2.4s\n",
            "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:    3.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8980111672362449"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skRcS3fdcsaS"
      },
      "source": [
        "## SGD Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6FQGu16b3pP",
        "outputId": "ef64c527-a1b8-4bb3-fab7-089d84ffb009"
      },
      "source": [
        "sgd = SGDClassifier(loss='modified_huber', verbose=2, n_jobs=-1, max_iter=1000)\n",
        "sgd.fit(X_train, y_train)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-- Epoch 1\n",
            "Norm: 76625712.73, NNZs: 33, Bias: 19946.827153, T: 372699, Avg. loss: 316667396241369.437500\n",
            "Total training time: 0.10 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 45797147.16, NNZs: 33, Bias: 20835.215308, T: 745398, Avg. loss: 34866904861274.910156\n",
            "Total training time: 0.19 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 33779088.35, NNZs: 33, Bias: 21308.762555, T: 1118097, Avg. loss: 20348585315236.546875\n",
            "Total training time: 0.29 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 27035819.39, NNZs: 33, Bias: 21628.998766, T: 1490796, Avg. loss: 14517982285199.185547\n",
            "Total training time: 0.39 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 22905578.74, NNZs: 33, Bias: 21869.589178, T: 1863495, Avg. loss: 11172080580263.386719\n",
            "Total training time: 0.49 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 20823633.61, NNZs: 33, Bias: 22068.353892, T: 2236194, Avg. loss: 9174988231949.587891\n",
            "Total training time: 0.60 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 18537521.43, NNZs: 33, Bias: 22228.251754, T: 2608893, Avg. loss: 7750670177334.291016\n",
            "Total training time: 0.70 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 17134576.35, NNZs: 33, Bias: 22369.545744, T: 2981592, Avg. loss: 6706571962813.165039\n",
            "Total training time: 0.80 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 15994359.11, NNZs: 33, Bias: 22494.198755, T: 3354291, Avg. loss: 5925573628123.640625\n",
            "Total training time: 0.90 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 14932485.50, NNZs: 33, Bias: 22606.213563, T: 3726990, Avg. loss: 5282220673332.050781\n",
            "Total training time: 0.99 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 14039077.16, NNZs: 33, Bias: 22707.208912, T: 4099689, Avg. loss: 4766828173554.582031\n",
            "Total training time: 1.09 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 13626349.26, NNZs: 33, Bias: 22802.588361, T: 4472388, Avg. loss: 4371803993724.626465\n",
            "Total training time: 1.18 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 13159833.46, NNZs: 33, Bias: 22887.737480, T: 4845087, Avg. loss: 4005632740144.066406\n",
            "Total training time: 1.29 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 12626022.27, NNZs: 33, Bias: 22964.116856, T: 5217786, Avg. loss: 3706099357101.324707\n",
            "Total training time: 1.39 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 12233615.08, NNZs: 33, Bias: 23036.266518, T: 5590485, Avg. loss: 3452534367037.863281\n",
            "Total training time: 1.49 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 11791173.40, NNZs: 33, Bias: 23102.516456, T: 5963184, Avg. loss: 3236383584291.236816\n",
            "Total training time: 1.58 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 11454898.62, NNZs: 33, Bias: 23164.850981, T: 6335883, Avg. loss: 3026935491505.554688\n",
            "Total training time: 1.69 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 11115345.59, NNZs: 33, Bias: 23223.733292, T: 6708582, Avg. loss: 2861657326580.795898\n",
            "Total training time: 1.78 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 10891715.09, NNZs: 33, Bias: 23280.853478, T: 7081281, Avg. loss: 2719214681967.433594\n",
            "Total training time: 1.88 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 10628944.27, NNZs: 33, Bias: 23333.875289, T: 7453980, Avg. loss: 2567418842517.190430\n",
            "Total training time: 1.98 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 10382518.75, NNZs: 33, Bias: 23383.409643, T: 7826679, Avg. loss: 2435557646038.047363\n",
            "Total training time: 2.08 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 10105876.35, NNZs: 33, Bias: 23430.647302, T: 8199378, Avg. loss: 2334049556422.613770\n",
            "Total training time: 2.18 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 9984959.12, NNZs: 33, Bias: 23476.069696, T: 8572077, Avg. loss: 2226115855642.538574\n",
            "Total training time: 2.28 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 9750065.19, NNZs: 33, Bias: 23517.826528, T: 8944776, Avg. loss: 2128614247684.523438\n",
            "Total training time: 2.38 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 9550356.19, NNZs: 33, Bias: 23559.434482, T: 9317475, Avg. loss: 2050474699320.479736\n",
            "Total training time: 2.47 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 9377169.64, NNZs: 33, Bias: 23599.733535, T: 9690174, Avg. loss: 1965911573465.667969\n",
            "Total training time: 2.57 seconds.\n",
            "-- Epoch 27\n",
            "Norm: 9234750.44, NNZs: 33, Bias: 23638.103109, T: 10062873, Avg. loss: 1894752995926.592773\n",
            "Total training time: 2.66 seconds.\n",
            "-- Epoch 28\n",
            "Norm: 9095736.56, NNZs: 33, Bias: 23675.305391, T: 10435572, Avg. loss: 1816726053341.255127\n",
            "Total training time: 2.76 seconds.\n",
            "-- Epoch 29\n",
            "Norm: 8907333.66, NNZs: 33, Bias: 23709.224124, T: 10808271, Avg. loss: 1757147830957.789307\n",
            "Total training time: 2.86 seconds.\n",
            "-- Epoch 30\n",
            "Norm: 8765046.22, NNZs: 33, Bias: 23744.060106, T: 11180970, Avg. loss: 1698332616983.407227\n",
            "Total training time: 2.95 seconds.\n",
            "-- Epoch 31\n",
            "Norm: 8651742.87, NNZs: 33, Bias: 23778.049233, T: 11553669, Avg. loss: 1640604102314.216797\n",
            "Total training time: 3.05 seconds.\n",
            "-- Epoch 32\n",
            "Norm: 8537197.37, NNZs: 33, Bias: 23810.790731, T: 11926368, Avg. loss: 1592421678885.097412\n",
            "Total training time: 3.14 seconds.\n",
            "-- Epoch 33\n",
            "Norm: 8416410.99, NNZs: 33, Bias: 23842.342154, T: 12299067, Avg. loss: 1540685614439.920166\n",
            "Total training time: 3.24 seconds.\n",
            "-- Epoch 34\n",
            "Norm: 8278059.87, NNZs: 33, Bias: 23872.701774, T: 12671766, Avg. loss: 1491250223145.012939\n",
            "Total training time: 3.34 seconds.\n",
            "-- Epoch 35\n",
            "Norm: 8177577.95, NNZs: 33, Bias: 23902.162647, T: 13044465, Avg. loss: 1449930150735.010498\n",
            "Total training time: 3.44 seconds.\n",
            "-- Epoch 36\n",
            "Norm: 8103380.71, NNZs: 33, Bias: 23930.888552, T: 13417164, Avg. loss: 1406718212826.174561\n",
            "Total training time: 3.54 seconds.\n",
            "-- Epoch 37\n",
            "Norm: 8014115.70, NNZs: 33, Bias: 23958.667125, T: 13789863, Avg. loss: 1368277851467.987549\n",
            "Total training time: 3.64 seconds.\n",
            "-- Epoch 38\n",
            "Norm: 7915445.69, NNZs: 33, Bias: 23985.470450, T: 14162562, Avg. loss: 1334352677720.330811\n",
            "Total training time: 3.74 seconds.\n",
            "-- Epoch 39\n",
            "Norm: 7803679.54, NNZs: 33, Bias: 24010.981573, T: 14535261, Avg. loss: 1298940058722.455322\n",
            "Total training time: 3.84 seconds.\n",
            "-- Epoch 40\n",
            "Norm: 7721555.05, NNZs: 33, Bias: 24036.374142, T: 14907960, Avg. loss: 1265879835244.563232\n",
            "Total training time: 3.94 seconds.\n",
            "-- Epoch 41\n",
            "Norm: 7627424.55, NNZs: 33, Bias: 24061.125662, T: 15280659, Avg. loss: 1231641957187.912598\n",
            "Total training time: 4.05 seconds.\n",
            "-- Epoch 42\n",
            "Norm: 7531097.12, NNZs: 33, Bias: 24085.213522, T: 15653358, Avg. loss: 1207103109488.921143\n",
            "Total training time: 4.15 seconds.\n",
            "-- Epoch 43\n",
            "Norm: 7468286.95, NNZs: 33, Bias: 24109.366136, T: 16026057, Avg. loss: 1181057631567.352295\n",
            "Total training time: 4.25 seconds.\n",
            "-- Epoch 44\n",
            "Norm: 7408962.32, NNZs: 33, Bias: 24132.696611, T: 16398756, Avg. loss: 1148769133300.476562\n",
            "Total training time: 4.35 seconds.\n",
            "-- Epoch 45\n",
            "Norm: 7316288.14, NNZs: 33, Bias: 24154.818104, T: 16771455, Avg. loss: 1121431296441.182617\n",
            "Total training time: 4.45 seconds.\n",
            "-- Epoch 46\n",
            "Norm: 7242850.65, NNZs: 33, Bias: 24176.814645, T: 17144154, Avg. loss: 1097200363080.877197\n",
            "Total training time: 4.55 seconds.\n",
            "-- Epoch 47\n",
            "Norm: 7171979.84, NNZs: 33, Bias: 24198.379499, T: 17516853, Avg. loss: 1076876660307.417236\n",
            "Total training time: 4.65 seconds.\n",
            "-- Epoch 48\n",
            "Norm: 7103389.68, NNZs: 33, Bias: 24219.508301, T: 17889552, Avg. loss: 1052864621838.505615\n",
            "Total training time: 4.74 seconds.\n",
            "-- Epoch 49\n",
            "Norm: 7042249.97, NNZs: 33, Bias: 24240.430302, T: 18262251, Avg. loss: 1031108949642.944580\n",
            "Total training time: 4.84 seconds.\n",
            "-- Epoch 50\n",
            "Norm: 6995346.12, NNZs: 33, Bias: 24260.959588, T: 18634950, Avg. loss: 1007901566793.307617\n",
            "Total training time: 4.93 seconds.\n",
            "-- Epoch 51\n",
            "Norm: 6941191.28, NNZs: 33, Bias: 24280.432654, T: 19007649, Avg. loss: 990457674736.457031\n",
            "Total training time: 5.03 seconds.\n",
            "-- Epoch 52\n",
            "Norm: 6874647.67, NNZs: 33, Bias: 24299.627114, T: 19380348, Avg. loss: 968908386395.381104\n",
            "Total training time: 5.13 seconds.\n",
            "-- Epoch 53\n",
            "Norm: 6821271.05, NNZs: 33, Bias: 24318.674088, T: 19753047, Avg. loss: 952734698717.542114\n",
            "Total training time: 5.22 seconds.\n",
            "-- Epoch 54\n",
            "Norm: 6773166.71, NNZs: 33, Bias: 24337.572655, T: 20125746, Avg. loss: 935198517427.708252\n",
            "Total training time: 5.32 seconds.\n",
            "-- Epoch 55\n",
            "Norm: 6722674.80, NNZs: 33, Bias: 24355.732980, T: 20498445, Avg. loss: 913510188950.538452\n",
            "Total training time: 5.42 seconds.\n",
            "-- Epoch 56\n",
            "Norm: 6658820.91, NNZs: 33, Bias: 24373.167136, T: 20871144, Avg. loss: 898651752708.463013\n",
            "Total training time: 5.52 seconds.\n",
            "-- Epoch 57\n",
            "Norm: 6625631.03, NNZs: 33, Bias: 24391.240586, T: 21243843, Avg. loss: 884859307150.214722\n",
            "Total training time: 5.62 seconds.\n",
            "-- Epoch 58\n",
            "Norm: 6564207.45, NNZs: 33, Bias: 24408.565525, T: 21616542, Avg. loss: 870033422577.651855\n",
            "Total training time: 5.72 seconds.\n",
            "-- Epoch 59\n",
            "Norm: 6515720.86, NNZs: 33, Bias: 24425.828252, T: 21989241, Avg. loss: 854329647468.590942\n",
            "Total training time: 5.81 seconds.\n",
            "-- Epoch 60\n",
            "Norm: 6477332.90, NNZs: 33, Bias: 24442.866638, T: 22361940, Avg. loss: 838352909027.208862\n",
            "Total training time: 5.91 seconds.\n",
            "-- Epoch 61\n",
            "Norm: 6433320.44, NNZs: 33, Bias: 24459.502305, T: 22734639, Avg. loss: 827123973297.415161\n",
            "Total training time: 6.01 seconds.\n",
            "-- Epoch 62\n",
            "Norm: 6392002.70, NNZs: 33, Bias: 24475.615505, T: 23107338, Avg. loss: 810490373030.776123\n",
            "Total training time: 6.10 seconds.\n",
            "-- Epoch 63\n",
            "Norm: 6352601.54, NNZs: 33, Bias: 24491.262908, T: 23480037, Avg. loss: 795488354965.296875\n",
            "Total training time: 6.20 seconds.\n",
            "-- Epoch 64\n",
            "Norm: 6322730.09, NNZs: 33, Bias: 24507.048791, T: 23852736, Avg. loss: 785588102246.008423\n",
            "Total training time: 6.29 seconds.\n",
            "-- Epoch 65\n",
            "Norm: 6283284.75, NNZs: 33, Bias: 24522.758973, T: 24225435, Avg. loss: 775781366620.794434\n",
            "Total training time: 6.39 seconds.\n",
            "-- Epoch 66\n",
            "Norm: 6243272.15, NNZs: 33, Bias: 24538.109006, T: 24598134, Avg. loss: 763952180005.354370\n",
            "Total training time: 6.50 seconds.\n",
            "-- Epoch 67\n",
            "Norm: 6209377.71, NNZs: 33, Bias: 24553.012684, T: 24970833, Avg. loss: 749409289305.125122\n",
            "Total training time: 6.59 seconds.\n",
            "-- Epoch 68\n",
            "Norm: 6175242.61, NNZs: 33, Bias: 24567.723229, T: 25343532, Avg. loss: 739142891389.041016\n",
            "Total training time: 6.69 seconds.\n",
            "-- Epoch 69\n",
            "Norm: 6136130.94, NNZs: 33, Bias: 24582.227972, T: 25716231, Avg. loss: 726639237443.494995\n",
            "Total training time: 6.79 seconds.\n",
            "-- Epoch 70\n",
            "Norm: 6100784.58, NNZs: 33, Bias: 24596.779292, T: 26088930, Avg. loss: 720973271049.766968\n",
            "Total training time: 6.88 seconds.\n",
            "-- Epoch 71\n",
            "Norm: 6072822.96, NNZs: 33, Bias: 24610.989434, T: 26461629, Avg. loss: 705812459978.387451\n",
            "Total training time: 6.98 seconds.\n",
            "-- Epoch 72\n",
            "Norm: 6040231.79, NNZs: 33, Bias: 24625.115154, T: 26834328, Avg. loss: 698491107423.511230\n",
            "Total training time: 7.08 seconds.\n",
            "-- Epoch 73\n",
            "Norm: 6012400.61, NNZs: 33, Bias: 24638.642039, T: 27207027, Avg. loss: 685702874354.835693\n",
            "Total training time: 7.17 seconds.\n",
            "-- Epoch 74\n",
            "Norm: 5989543.96, NNZs: 33, Bias: 24652.156462, T: 27579726, Avg. loss: 680527085010.627441\n",
            "Total training time: 7.27 seconds.\n",
            "-- Epoch 75\n",
            "Norm: 5960778.88, NNZs: 33, Bias: 24665.501450, T: 27952425, Avg. loss: 672435743278.594116\n",
            "Total training time: 7.37 seconds.\n",
            "-- Epoch 76\n",
            "Norm: 5931430.25, NNZs: 33, Bias: 24678.670717, T: 28325124, Avg. loss: 659582020389.126953\n",
            "Total training time: 7.47 seconds.\n",
            "-- Epoch 77\n",
            "Norm: 5910142.04, NNZs: 33, Bias: 24691.780035, T: 28697823, Avg. loss: 653205354964.542603\n",
            "Total training time: 7.56 seconds.\n",
            "-- Epoch 78\n",
            "Norm: 5882911.48, NNZs: 33, Bias: 24704.641403, T: 29070522, Avg. loss: 642482426533.501953\n",
            "Total training time: 7.66 seconds.\n",
            "-- Epoch 79\n",
            "Norm: 5855631.49, NNZs: 33, Bias: 24717.324786, T: 29443221, Avg. loss: 634402784294.872314\n",
            "Total training time: 7.75 seconds.\n",
            "-- Epoch 80\n",
            "Norm: 5834081.87, NNZs: 33, Bias: 24729.630359, T: 29815920, Avg. loss: 631517277444.522949\n",
            "Total training time: 7.85 seconds.\n",
            "-- Epoch 81\n",
            "Norm: 5802259.76, NNZs: 33, Bias: 24741.653587, T: 30188619, Avg. loss: 619080363086.125977\n",
            "Total training time: 7.95 seconds.\n",
            "-- Epoch 82\n",
            "Norm: 5775369.55, NNZs: 33, Bias: 24753.745908, T: 30561318, Avg. loss: 608955296341.281006\n",
            "Total training time: 8.04 seconds.\n",
            "-- Epoch 83\n",
            "Norm: 5755233.44, NNZs: 33, Bias: 24765.779189, T: 30934017, Avg. loss: 604446206097.956177\n",
            "Total training time: 8.14 seconds.\n",
            "-- Epoch 84\n",
            "Norm: 5735822.40, NNZs: 33, Bias: 24777.921553, T: 31306716, Avg. loss: 597500909414.270142\n",
            "Total training time: 8.24 seconds.\n",
            "-- Epoch 85\n",
            "Norm: 5712777.78, NNZs: 33, Bias: 24789.651577, T: 31679415, Avg. loss: 591003206715.402100\n",
            "Total training time: 8.34 seconds.\n",
            "-- Epoch 86\n",
            "Norm: 5694222.76, NNZs: 33, Bias: 24801.411738, T: 32052114, Avg. loss: 583444471241.249023\n",
            "Total training time: 8.44 seconds.\n",
            "-- Epoch 87\n",
            "Norm: 5668161.51, NNZs: 33, Bias: 24812.656714, T: 32424813, Avg. loss: 575368211974.329224\n",
            "Total training time: 8.54 seconds.\n",
            "-- Epoch 88\n",
            "Norm: 5645523.73, NNZs: 33, Bias: 24823.903347, T: 32797512, Avg. loss: 571577129182.922485\n",
            "Total training time: 8.63 seconds.\n",
            "-- Epoch 89\n",
            "Norm: 5625001.16, NNZs: 33, Bias: 24835.270129, T: 33170211, Avg. loss: 565437320549.042969\n",
            "Total training time: 8.73 seconds.\n",
            "-- Epoch 90\n",
            "Norm: 5610384.12, NNZs: 33, Bias: 24846.406755, T: 33542910, Avg. loss: 557707723609.447510\n",
            "Total training time: 8.82 seconds.\n",
            "-- Epoch 91\n",
            "Norm: 5591507.57, NNZs: 33, Bias: 24857.072340, T: 33915609, Avg. loss: 549428266484.085876\n",
            "Total training time: 8.92 seconds.\n",
            "-- Epoch 92\n",
            "Norm: 5570153.69, NNZs: 33, Bias: 24867.784861, T: 34288308, Avg. loss: 544499856575.690918\n",
            "Total training time: 9.02 seconds.\n",
            "-- Epoch 93\n",
            "Norm: 5554349.41, NNZs: 33, Bias: 24878.761947, T: 34661007, Avg. loss: 539285086696.401062\n",
            "Total training time: 9.12 seconds.\n",
            "-- Epoch 94\n",
            "Norm: 5539030.29, NNZs: 33, Bias: 24889.429732, T: 35033706, Avg. loss: 532018794643.867004\n",
            "Total training time: 9.21 seconds.\n",
            "-- Epoch 95\n",
            "Norm: 5513131.96, NNZs: 33, Bias: 24899.683006, T: 35406405, Avg. loss: 529362719605.611694\n",
            "Total training time: 9.31 seconds.\n",
            "-- Epoch 96\n",
            "Norm: 5496728.24, NNZs: 33, Bias: 24910.017044, T: 35779104, Avg. loss: 522442063806.136292\n",
            "Total training time: 9.41 seconds.\n",
            "-- Epoch 97\n",
            "Norm: 5477932.68, NNZs: 33, Bias: 24920.319829, T: 36151803, Avg. loss: 517985149027.507568\n",
            "Total training time: 9.51 seconds.\n",
            "-- Epoch 98\n",
            "Norm: 5457466.96, NNZs: 33, Bias: 24930.327922, T: 36524502, Avg. loss: 508965829491.724548\n",
            "Total training time: 9.60 seconds.\n",
            "-- Epoch 99\n",
            "Norm: 5440949.17, NNZs: 33, Bias: 24940.737190, T: 36897201, Avg. loss: 506617715163.337524\n",
            "Total training time: 9.70 seconds.\n",
            "-- Epoch 100\n",
            "Norm: 5426069.74, NNZs: 33, Bias: 24950.797569, T: 37269900, Avg. loss: 503857936603.799622\n",
            "Total training time: 9.80 seconds.\n",
            "-- Epoch 101\n",
            "Norm: 5409684.52, NNZs: 33, Bias: 24960.632349, T: 37642599, Avg. loss: 495430113083.232422\n",
            "Total training time: 9.89 seconds.\n",
            "-- Epoch 102\n",
            "Norm: 5394647.91, NNZs: 33, Bias: 24970.323740, T: 38015298, Avg. loss: 490649547177.024597\n",
            "Total training time: 9.99 seconds.\n",
            "-- Epoch 103\n",
            "Norm: 5377044.76, NNZs: 33, Bias: 24979.866992, T: 38387997, Avg. loss: 486153090318.639954\n",
            "Total training time: 10.09 seconds.\n",
            "-- Epoch 104\n",
            "Norm: 5360131.47, NNZs: 33, Bias: 24989.228659, T: 38760696, Avg. loss: 482020256415.049194\n",
            "Total training time: 10.19 seconds.\n",
            "-- Epoch 105\n",
            "Norm: 5342259.56, NNZs: 33, Bias: 24998.778263, T: 39133395, Avg. loss: 478907794158.412292\n",
            "Total training time: 10.29 seconds.\n",
            "-- Epoch 106\n",
            "Norm: 5330464.88, NNZs: 33, Bias: 25008.383431, T: 39506094, Avg. loss: 471624006540.205444\n",
            "Total training time: 10.38 seconds.\n",
            "-- Epoch 107\n",
            "Norm: 5313370.62, NNZs: 33, Bias: 25017.772290, T: 39878793, Avg. loss: 468482163541.574707\n",
            "Total training time: 10.48 seconds.\n",
            "-- Epoch 108\n",
            "Norm: 5297910.63, NNZs: 33, Bias: 25027.181777, T: 40251492, Avg. loss: 462951565968.064514\n",
            "Total training time: 10.58 seconds.\n",
            "-- Epoch 109\n",
            "Norm: 5281169.45, NNZs: 33, Bias: 25036.372448, T: 40624191, Avg. loss: 459625009548.109131\n",
            "Total training time: 10.68 seconds.\n",
            "-- Epoch 110\n",
            "Norm: 5269780.71, NNZs: 33, Bias: 25045.430086, T: 40996890, Avg. loss: 454421870246.812561\n",
            "Total training time: 10.78 seconds.\n",
            "-- Epoch 111\n",
            "Norm: 5255939.74, NNZs: 33, Bias: 25054.193959, T: 41369589, Avg. loss: 449188372933.382751\n",
            "Total training time: 10.88 seconds.\n",
            "-- Epoch 112\n",
            "Norm: 5243392.08, NNZs: 33, Bias: 25063.340129, T: 41742288, Avg. loss: 446986186070.691650\n",
            "Total training time: 10.98 seconds.\n",
            "-- Epoch 113\n",
            "Norm: 5231836.52, NNZs: 33, Bias: 25072.256394, T: 42114987, Avg. loss: 443549756773.447388\n",
            "Total training time: 11.08 seconds.\n",
            "-- Epoch 114\n",
            "Norm: 5217118.12, NNZs: 33, Bias: 25080.755559, T: 42487686, Avg. loss: 437709356905.680054\n",
            "Total training time: 11.17 seconds.\n",
            "-- Epoch 115\n",
            "Norm: 5205125.10, NNZs: 33, Bias: 25089.306259, T: 42860385, Avg. loss: 435138631150.275513\n",
            "Total training time: 11.27 seconds.\n",
            "-- Epoch 116\n",
            "Norm: 5192516.71, NNZs: 33, Bias: 25097.848068, T: 43233084, Avg. loss: 431554663967.101807\n",
            "Total training time: 11.36 seconds.\n",
            "-- Epoch 117\n",
            "Norm: 5177771.96, NNZs: 33, Bias: 25106.413267, T: 43605783, Avg. loss: 426789104295.330566\n",
            "Total training time: 11.46 seconds.\n",
            "-- Epoch 118\n",
            "Norm: 5167418.16, NNZs: 33, Bias: 25114.898520, T: 43978482, Avg. loss: 423797235011.588501\n",
            "Total training time: 11.56 seconds.\n",
            "-- Epoch 119\n",
            "Norm: 5153967.44, NNZs: 33, Bias: 25123.304926, T: 44351181, Avg. loss: 420876316764.069397\n",
            "Total training time: 11.65 seconds.\n",
            "-- Epoch 120\n",
            "Norm: 5139360.57, NNZs: 33, Bias: 25131.496594, T: 44723880, Avg. loss: 417841541154.852295\n",
            "Total training time: 11.75 seconds.\n",
            "-- Epoch 121\n",
            "Norm: 5126965.28, NNZs: 33, Bias: 25139.810252, T: 45096579, Avg. loss: 413953418711.601746\n",
            "Total training time: 11.84 seconds.\n",
            "-- Epoch 122\n",
            "Norm: 5115877.01, NNZs: 33, Bias: 25148.090959, T: 45469278, Avg. loss: 409847918642.120789\n",
            "Total training time: 11.94 seconds.\n",
            "-- Epoch 123\n",
            "Norm: 5101597.58, NNZs: 33, Bias: 25155.898093, T: 45841977, Avg. loss: 404524069134.711365\n",
            "Total training time: 12.04 seconds.\n",
            "-- Epoch 124\n",
            "Norm: 5092518.93, NNZs: 33, Bias: 25163.902860, T: 46214676, Avg. loss: 401220245360.589478\n",
            "Total training time: 12.13 seconds.\n",
            "-- Epoch 125\n",
            "Norm: 5079185.15, NNZs: 33, Bias: 25171.938788, T: 46587375, Avg. loss: 398949107260.861877\n",
            "Total training time: 12.23 seconds.\n",
            "-- Epoch 126\n",
            "Norm: 5069828.18, NNZs: 33, Bias: 25179.966310, T: 46960074, Avg. loss: 398365579124.393005\n",
            "Total training time: 12.33 seconds.\n",
            "-- Epoch 127\n",
            "Norm: 5056544.68, NNZs: 33, Bias: 25187.705477, T: 47332773, Avg. loss: 393564528827.484192\n",
            "Total training time: 12.42 seconds.\n",
            "-- Epoch 128\n",
            "Norm: 5045672.25, NNZs: 33, Bias: 25195.594262, T: 47705472, Avg. loss: 391158124424.985413\n",
            "Total training time: 12.53 seconds.\n",
            "-- Epoch 129\n",
            "Norm: 5036593.81, NNZs: 33, Bias: 25203.325087, T: 48078171, Avg. loss: 388378734987.860840\n",
            "Total training time: 12.63 seconds.\n",
            "-- Epoch 130\n",
            "Norm: 5025450.31, NNZs: 33, Bias: 25211.038803, T: 48450870, Avg. loss: 382277477263.003723\n",
            "Total training time: 12.72 seconds.\n",
            "-- Epoch 131\n",
            "Norm: 5013597.11, NNZs: 33, Bias: 25218.482915, T: 48823569, Avg. loss: 380037745326.208191\n",
            "Total training time: 12.81 seconds.\n",
            "-- Epoch 132\n",
            "Norm: 4998123.41, NNZs: 33, Bias: 25225.824125, T: 49196268, Avg. loss: 378383554317.376831\n",
            "Total training time: 12.91 seconds.\n",
            "-- Epoch 133\n",
            "Norm: 4989254.71, NNZs: 33, Bias: 25233.374614, T: 49568967, Avg. loss: 374740454577.011414\n",
            "Total training time: 13.00 seconds.\n",
            "-- Epoch 134\n",
            "Norm: 4978261.61, NNZs: 33, Bias: 25240.834231, T: 49941666, Avg. loss: 371481291503.851013\n",
            "Total training time: 13.09 seconds.\n",
            "-- Epoch 135\n",
            "Norm: 4968947.26, NNZs: 33, Bias: 25248.326027, T: 50314365, Avg. loss: 370816458352.518066\n",
            "Total training time: 13.19 seconds.\n",
            "-- Epoch 136\n",
            "Norm: 4958567.46, NNZs: 33, Bias: 25255.592279, T: 50687064, Avg. loss: 368121007130.583313\n",
            "Total training time: 13.29 seconds.\n",
            "-- Epoch 137\n",
            "Norm: 4948063.27, NNZs: 33, Bias: 25262.862403, T: 51059763, Avg. loss: 365911967266.393372\n",
            "Total training time: 13.38 seconds.\n",
            "-- Epoch 138\n",
            "Norm: 4939613.86, NNZs: 33, Bias: 25270.168606, T: 51432462, Avg. loss: 362851891605.146240\n",
            "Total training time: 13.48 seconds.\n",
            "-- Epoch 139\n",
            "Norm: 4927084.28, NNZs: 33, Bias: 25277.156476, T: 51805161, Avg. loss: 359277746667.408264\n",
            "Total training time: 13.58 seconds.\n",
            "-- Epoch 140\n",
            "Norm: 4916216.61, NNZs: 33, Bias: 25284.315308, T: 52177860, Avg. loss: 356595406350.128296\n",
            "Total training time: 13.67 seconds.\n",
            "-- Epoch 141\n",
            "Norm: 4906017.04, NNZs: 33, Bias: 25291.211063, T: 52550559, Avg. loss: 352970116297.751465\n",
            "Total training time: 13.77 seconds.\n",
            "-- Epoch 142\n",
            "Norm: 4894342.05, NNZs: 33, Bias: 25298.350228, T: 52923258, Avg. loss: 351647676314.933899\n",
            "Total training time: 13.86 seconds.\n",
            "-- Epoch 143\n",
            "Norm: 4884768.65, NNZs: 33, Bias: 25305.305824, T: 53295957, Avg. loss: 349051871150.341003\n",
            "Total training time: 13.96 seconds.\n",
            "-- Epoch 144\n",
            "Norm: 4876866.72, NNZs: 33, Bias: 25312.279759, T: 53668656, Avg. loss: 346466110583.125122\n",
            "Total training time: 14.05 seconds.\n",
            "-- Epoch 145\n",
            "Norm: 4868137.20, NNZs: 33, Bias: 25319.289171, T: 54041355, Avg. loss: 344025145494.893555\n",
            "Total training time: 14.15 seconds.\n",
            "-- Epoch 146\n",
            "Norm: 4858812.57, NNZs: 33, Bias: 25325.968934, T: 54414054, Avg. loss: 342189092887.160522\n",
            "Total training time: 14.24 seconds.\n",
            "-- Epoch 147\n",
            "Norm: 4848237.88, NNZs: 33, Bias: 25332.668930, T: 54786753, Avg. loss: 340280581190.990112\n",
            "Total training time: 14.34 seconds.\n",
            "-- Epoch 148\n",
            "Norm: 4840366.96, NNZs: 33, Bias: 25339.348400, T: 55159452, Avg. loss: 338096243745.850586\n",
            "Total training time: 14.44 seconds.\n",
            "-- Epoch 149\n",
            "Norm: 4829304.45, NNZs: 33, Bias: 25345.876889, T: 55532151, Avg. loss: 335874185894.981689\n",
            "Total training time: 14.54 seconds.\n",
            "-- Epoch 150\n",
            "Norm: 4822359.61, NNZs: 33, Bias: 25352.523192, T: 55904850, Avg. loss: 332079282097.389771\n",
            "Total training time: 14.64 seconds.\n",
            "-- Epoch 151\n",
            "Norm: 4811985.21, NNZs: 33, Bias: 25359.102623, T: 56277549, Avg. loss: 331060835014.091003\n",
            "Total training time: 14.75 seconds.\n",
            "-- Epoch 152\n",
            "Norm: 4802982.67, NNZs: 33, Bias: 25365.631361, T: 56650248, Avg. loss: 328146965196.024231\n",
            "Total training time: 14.84 seconds.\n",
            "-- Epoch 153\n",
            "Norm: 4794315.29, NNZs: 33, Bias: 25372.218630, T: 57022947, Avg. loss: 326685637562.182556\n",
            "Total training time: 14.94 seconds.\n",
            "-- Epoch 154\n",
            "Norm: 4786294.10, NNZs: 33, Bias: 25378.686884, T: 57395646, Avg. loss: 324485792973.208496\n",
            "Total training time: 15.04 seconds.\n",
            "-- Epoch 155\n",
            "Norm: 4778627.35, NNZs: 33, Bias: 25385.001653, T: 57768345, Avg. loss: 322219260705.125977\n",
            "Total training time: 15.14 seconds.\n",
            "-- Epoch 156\n",
            "Norm: 4770581.43, NNZs: 33, Bias: 25391.354019, T: 58141044, Avg. loss: 319509046836.955139\n",
            "Total training time: 15.23 seconds.\n",
            "-- Epoch 157\n",
            "Norm: 4759799.29, NNZs: 33, Bias: 25397.473493, T: 58513743, Avg. loss: 316610350045.883850\n",
            "Total training time: 15.33 seconds.\n",
            "-- Epoch 158\n",
            "Norm: 4752311.76, NNZs: 33, Bias: 25403.848551, T: 58886442, Avg. loss: 317273174249.419312\n",
            "Total training time: 15.43 seconds.\n",
            "-- Epoch 159\n",
            "Norm: 4743636.23, NNZs: 33, Bias: 25410.042838, T: 59259141, Avg. loss: 313458023871.290955\n",
            "Total training time: 15.53 seconds.\n",
            "-- Epoch 160\n",
            "Norm: 4736087.77, NNZs: 33, Bias: 25416.304626, T: 59631840, Avg. loss: 311550595284.152466\n",
            "Total training time: 15.63 seconds.\n",
            "-- Epoch 161\n",
            "Norm: 4727391.96, NNZs: 33, Bias: 25422.433260, T: 60004539, Avg. loss: 308605050960.348389\n",
            "Total training time: 15.72 seconds.\n",
            "-- Epoch 162\n",
            "Norm: 4719344.07, NNZs: 33, Bias: 25428.487592, T: 60377238, Avg. loss: 307152991113.410522\n",
            "Total training time: 15.82 seconds.\n",
            "-- Epoch 163\n",
            "Norm: 4711456.12, NNZs: 33, Bias: 25434.678788, T: 60749937, Avg. loss: 306290991363.428711\n",
            "Total training time: 15.92 seconds.\n",
            "-- Epoch 164\n",
            "Norm: 4701439.39, NNZs: 33, Bias: 25440.577844, T: 61122636, Avg. loss: 304347087268.190674\n",
            "Total training time: 16.01 seconds.\n",
            "-- Epoch 165\n",
            "Norm: 4691666.45, NNZs: 33, Bias: 25446.632958, T: 61495335, Avg. loss: 302753719265.844788\n",
            "Total training time: 16.11 seconds.\n",
            "-- Epoch 166\n",
            "Norm: 4684267.67, NNZs: 33, Bias: 25452.696897, T: 61868034, Avg. loss: 299014419403.983521\n",
            "Total training time: 16.21 seconds.\n",
            "-- Epoch 167\n",
            "Norm: 4676339.43, NNZs: 33, Bias: 25458.615783, T: 62240733, Avg. loss: 298089835622.069153\n",
            "Total training time: 16.31 seconds.\n",
            "-- Epoch 168\n",
            "Norm: 4669028.55, NNZs: 33, Bias: 25464.595941, T: 62613432, Avg. loss: 297582333272.932434\n",
            "Total training time: 16.41 seconds.\n",
            "-- Epoch 169\n",
            "Norm: 4659181.02, NNZs: 33, Bias: 25470.302486, T: 62986131, Avg. loss: 296113810204.959045\n",
            "Total training time: 16.51 seconds.\n",
            "-- Epoch 170\n",
            "Norm: 4651137.76, NNZs: 33, Bias: 25476.097954, T: 63358830, Avg. loss: 293769122390.633789\n",
            "Total training time: 16.61 seconds.\n",
            "-- Epoch 171\n",
            "Norm: 4643875.24, NNZs: 33, Bias: 25481.933942, T: 63731529, Avg. loss: 291588273796.710510\n",
            "Total training time: 16.71 seconds.\n",
            "-- Epoch 172\n",
            "Norm: 4636477.77, NNZs: 33, Bias: 25487.773424, T: 64104228, Avg. loss: 290405762554.844788\n",
            "Total training time: 16.81 seconds.\n",
            "-- Epoch 173\n",
            "Norm: 4628108.31, NNZs: 33, Bias: 25493.509058, T: 64476927, Avg. loss: 288083956837.377930\n",
            "Total training time: 16.91 seconds.\n",
            "-- Epoch 174\n",
            "Norm: 4621224.02, NNZs: 33, Bias: 25499.229889, T: 64849626, Avg. loss: 286466268173.632019\n",
            "Total training time: 17.01 seconds.\n",
            "-- Epoch 175\n",
            "Norm: 4613327.59, NNZs: 33, Bias: 25504.949137, T: 65222325, Avg. loss: 284083762208.527039\n",
            "Total training time: 17.11 seconds.\n",
            "-- Epoch 176\n",
            "Norm: 4605238.55, NNZs: 33, Bias: 25510.447961, T: 65595024, Avg. loss: 282666804655.029846\n",
            "Total training time: 17.21 seconds.\n",
            "-- Epoch 177\n",
            "Norm: 4599154.54, NNZs: 33, Bias: 25516.152309, T: 65967723, Avg. loss: 281678378539.868347\n",
            "Total training time: 17.31 seconds.\n",
            "-- Epoch 178\n",
            "Norm: 4592065.70, NNZs: 33, Bias: 25521.582684, T: 66340422, Avg. loss: 280118476113.310669\n",
            "Total training time: 17.41 seconds.\n",
            "-- Epoch 179\n",
            "Norm: 4583571.29, NNZs: 33, Bias: 25527.112844, T: 66713121, Avg. loss: 277597395056.476501\n",
            "Total training time: 17.50 seconds.\n",
            "-- Epoch 180\n",
            "Norm: 4575869.07, NNZs: 33, Bias: 25532.561051, T: 67085820, Avg. loss: 276942723653.929138\n",
            "Total training time: 17.60 seconds.\n",
            "-- Epoch 181\n",
            "Norm: 4570001.99, NNZs: 33, Bias: 25538.128304, T: 67458519, Avg. loss: 275133482714.636597\n",
            "Total training time: 17.70 seconds.\n",
            "-- Epoch 182\n",
            "Norm: 4562654.33, NNZs: 33, Bias: 25543.517594, T: 67831218, Avg. loss: 273046674083.964630\n",
            "Total training time: 17.79 seconds.\n",
            "-- Epoch 183\n",
            "Norm: 4554261.66, NNZs: 33, Bias: 25549.039544, T: 68203917, Avg. loss: 272108982738.816803\n",
            "Total training time: 17.89 seconds.\n",
            "-- Epoch 184\n",
            "Norm: 4546568.40, NNZs: 33, Bias: 25554.392322, T: 68576616, Avg. loss: 270672112061.202332\n",
            "Total training time: 17.99 seconds.\n",
            "-- Epoch 185\n",
            "Norm: 4540810.87, NNZs: 33, Bias: 25559.895245, T: 68949315, Avg. loss: 269794380499.203308\n",
            "Total training time: 18.08 seconds.\n",
            "-- Epoch 186\n",
            "Norm: 4534429.17, NNZs: 33, Bias: 25565.247365, T: 69322014, Avg. loss: 268845058695.981720\n",
            "Total training time: 18.18 seconds.\n",
            "-- Epoch 187\n",
            "Norm: 4526203.56, NNZs: 33, Bias: 25570.425518, T: 69694713, Avg. loss: 265681373619.772186\n",
            "Total training time: 18.28 seconds.\n",
            "-- Epoch 188\n",
            "Norm: 4518671.83, NNZs: 33, Bias: 25575.671532, T: 70067412, Avg. loss: 264420240678.884918\n",
            "Total training time: 18.37 seconds.\n",
            "-- Epoch 189\n",
            "Norm: 4512109.93, NNZs: 33, Bias: 25580.875789, T: 70440111, Avg. loss: 263950116785.070496\n",
            "Total training time: 18.47 seconds.\n",
            "-- Epoch 190\n",
            "Norm: 4503391.58, NNZs: 33, Bias: 25586.016500, T: 70812810, Avg. loss: 262365321735.737457\n",
            "Total training time: 18.57 seconds.\n",
            "-- Epoch 191\n",
            "Norm: 4496309.45, NNZs: 33, Bias: 25591.343818, T: 71185509, Avg. loss: 260599178392.488953\n",
            "Total training time: 18.67 seconds.\n",
            "-- Epoch 192\n",
            "Norm: 4490411.21, NNZs: 33, Bias: 25596.560201, T: 71558208, Avg. loss: 259707589344.681702\n",
            "Total training time: 18.76 seconds.\n",
            "-- Epoch 193\n",
            "Norm: 4482913.83, NNZs: 33, Bias: 25601.635916, T: 71930907, Avg. loss: 258335990815.944916\n",
            "Total training time: 18.86 seconds.\n",
            "-- Epoch 194\n",
            "Norm: 4476291.92, NNZs: 33, Bias: 25606.757265, T: 72303606, Avg. loss: 257580881194.895966\n",
            "Total training time: 18.96 seconds.\n",
            "-- Epoch 195\n",
            "Norm: 4470326.63, NNZs: 33, Bias: 25611.717501, T: 72676305, Avg. loss: 254538210008.239532\n",
            "Total training time: 19.05 seconds.\n",
            "-- Epoch 196\n",
            "Norm: 4462537.35, NNZs: 33, Bias: 25616.801328, T: 73049004, Avg. loss: 254169189581.171844\n",
            "Total training time: 19.15 seconds.\n",
            "-- Epoch 197\n",
            "Norm: 4455890.44, NNZs: 33, Bias: 25621.650214, T: 73421703, Avg. loss: 252644469767.748291\n",
            "Total training time: 19.25 seconds.\n",
            "-- Epoch 198\n",
            "Norm: 4448594.05, NNZs: 33, Bias: 25626.599248, T: 73794402, Avg. loss: 252293924115.375732\n",
            "Total training time: 19.34 seconds.\n",
            "-- Epoch 199\n",
            "Norm: 4441374.50, NNZs: 33, Bias: 25631.640451, T: 74167101, Avg. loss: 250413453326.089050\n",
            "Total training time: 19.44 seconds.\n",
            "-- Epoch 200\n",
            "Norm: 4435703.64, NNZs: 33, Bias: 25636.701872, T: 74539800, Avg. loss: 248844000293.204529\n",
            "Total training time: 19.53 seconds.\n",
            "-- Epoch 201\n",
            "Norm: 4429753.42, NNZs: 33, Bias: 25641.641782, T: 74912499, Avg. loss: 247584786403.255005\n",
            "Total training time: 19.63 seconds.\n",
            "-- Epoch 202\n",
            "Norm: 4422686.12, NNZs: 33, Bias: 25646.490384, T: 75285198, Avg. loss: 246973470780.475281\n",
            "Total training time: 19.73 seconds.\n",
            "-- Epoch 203\n",
            "Norm: 4416371.23, NNZs: 33, Bias: 25651.350811, T: 75657897, Avg. loss: 245699350008.926636\n",
            "Total training time: 19.83 seconds.\n",
            "-- Epoch 204\n",
            "Norm: 4408809.90, NNZs: 33, Bias: 25656.091104, T: 76030596, Avg. loss: 242596725699.850342\n",
            "Total training time: 19.92 seconds.\n",
            "-- Epoch 205\n",
            "Norm: 4402709.01, NNZs: 33, Bias: 25660.933040, T: 76403295, Avg. loss: 241950377495.562439\n",
            "Total training time: 20.02 seconds.\n",
            "-- Epoch 206\n",
            "Norm: 4395996.28, NNZs: 33, Bias: 25665.787299, T: 76775994, Avg. loss: 241022572994.034454\n",
            "Total training time: 20.11 seconds.\n",
            "-- Epoch 207\n",
            "Norm: 4390697.20, NNZs: 33, Bias: 25670.524874, T: 77148693, Avg. loss: 241021663612.263824\n",
            "Total training time: 20.21 seconds.\n",
            "-- Epoch 208\n",
            "Norm: 4384037.81, NNZs: 33, Bias: 25675.272293, T: 77521392, Avg. loss: 240022338552.204376\n",
            "Total training time: 20.30 seconds.\n",
            "-- Epoch 209\n",
            "Norm: 4377789.80, NNZs: 33, Bias: 25679.981330, T: 77894091, Avg. loss: 237937870100.385956\n",
            "Total training time: 20.40 seconds.\n",
            "-- Epoch 210\n",
            "Norm: 4371280.18, NNZs: 33, Bias: 25684.654340, T: 78266790, Avg. loss: 238381822863.748657\n",
            "Total training time: 20.49 seconds.\n",
            "-- Epoch 211\n",
            "Norm: 4364472.14, NNZs: 33, Bias: 25689.397659, T: 78639489, Avg. loss: 234508740276.564880\n",
            "Total training time: 20.59 seconds.\n",
            "-- Epoch 212\n",
            "Norm: 4358032.83, NNZs: 33, Bias: 25694.078761, T: 79012188, Avg. loss: 234458042024.309540\n",
            "Total training time: 20.69 seconds.\n",
            "-- Epoch 213\n",
            "Norm: 4352046.21, NNZs: 33, Bias: 25698.703043, T: 79384887, Avg. loss: 233929595027.141205\n",
            "Total training time: 20.79 seconds.\n",
            "-- Epoch 214\n",
            "Norm: 4344838.62, NNZs: 33, Bias: 25703.256794, T: 79757586, Avg. loss: 232485734163.789062\n",
            "Total training time: 20.88 seconds.\n",
            "-- Epoch 215\n",
            "Norm: 4339612.21, NNZs: 33, Bias: 25707.978000, T: 80130285, Avg. loss: 231578950478.615723\n",
            "Total training time: 20.98 seconds.\n",
            "-- Epoch 216\n",
            "Norm: 4333887.23, NNZs: 33, Bias: 25712.572758, T: 80502984, Avg. loss: 230690054779.255371\n",
            "Total training time: 21.07 seconds.\n",
            "-- Epoch 217\n",
            "Norm: 4327148.37, NNZs: 33, Bias: 25717.188506, T: 80875683, Avg. loss: 229607727475.077271\n",
            "Total training time: 21.17 seconds.\n",
            "-- Epoch 218\n",
            "Norm: 4320521.91, NNZs: 33, Bias: 25721.767003, T: 81248382, Avg. loss: 228505412270.256409\n",
            "Total training time: 21.26 seconds.\n",
            "-- Epoch 219\n",
            "Norm: 4314082.55, NNZs: 33, Bias: 25726.361126, T: 81621081, Avg. loss: 227216532802.502625\n",
            "Total training time: 21.36 seconds.\n",
            "-- Epoch 220\n",
            "Norm: 4307382.91, NNZs: 33, Bias: 25730.827606, T: 81993780, Avg. loss: 226041965053.577209\n",
            "Total training time: 21.45 seconds.\n",
            "-- Epoch 221\n",
            "Norm: 4301697.80, NNZs: 33, Bias: 25735.307939, T: 82366479, Avg. loss: 224846324233.915314\n",
            "Total training time: 21.55 seconds.\n",
            "-- Epoch 222\n",
            "Norm: 4295425.47, NNZs: 33, Bias: 25739.777474, T: 82739178, Avg. loss: 224064030990.058838\n",
            "Total training time: 21.65 seconds.\n",
            "-- Epoch 223\n",
            "Norm: 4288400.15, NNZs: 33, Bias: 25744.220411, T: 83111877, Avg. loss: 223582974498.749756\n",
            "Total training time: 21.75 seconds.\n",
            "-- Epoch 224\n",
            "Norm: 4283215.01, NNZs: 33, Bias: 25748.707504, T: 83484576, Avg. loss: 223731896902.187134\n",
            "Total training time: 21.84 seconds.\n",
            "-- Epoch 225\n",
            "Norm: 4277135.66, NNZs: 33, Bias: 25753.073487, T: 83857275, Avg. loss: 221171480478.203217\n",
            "Total training time: 21.94 seconds.\n",
            "-- Epoch 226\n",
            "Norm: 4270956.57, NNZs: 33, Bias: 25757.390998, T: 84229974, Avg. loss: 220267013526.645111\n",
            "Total training time: 22.03 seconds.\n",
            "-- Epoch 227\n",
            "Norm: 4264768.82, NNZs: 33, Bias: 25761.821982, T: 84602673, Avg. loss: 219418277234.033936\n",
            "Total training time: 22.13 seconds.\n",
            "-- Epoch 228\n",
            "Norm: 4258835.44, NNZs: 33, Bias: 25766.173637, T: 84975372, Avg. loss: 218512339769.210938\n",
            "Total training time: 22.22 seconds.\n",
            "-- Epoch 229\n",
            "Norm: 4253260.17, NNZs: 33, Bias: 25770.568495, T: 85348071, Avg. loss: 217700113190.613892\n",
            "Total training time: 22.32 seconds.\n",
            "-- Epoch 230\n",
            "Norm: 4247280.45, NNZs: 33, Bias: 25774.845549, T: 85720770, Avg. loss: 216330248460.514557\n",
            "Total training time: 22.42 seconds.\n",
            "-- Epoch 231\n",
            "Norm: 4241381.87, NNZs: 33, Bias: 25779.098529, T: 86093469, Avg. loss: 214956384973.177521\n",
            "Total training time: 22.51 seconds.\n",
            "-- Epoch 232\n",
            "Norm: 4236908.63, NNZs: 33, Bias: 25783.316195, T: 86466168, Avg. loss: 213870495070.639160\n",
            "Total training time: 22.61 seconds.\n",
            "-- Epoch 233\n",
            "Norm: 4229691.35, NNZs: 33, Bias: 25787.552731, T: 86838867, Avg. loss: 213530698090.504944\n",
            "Total training time: 22.71 seconds.\n",
            "-- Epoch 234\n",
            "Norm: 4222893.87, NNZs: 33, Bias: 25791.721214, T: 87211566, Avg. loss: 212031604036.004547\n",
            "Total training time: 22.81 seconds.\n",
            "-- Epoch 235\n",
            "Norm: 4218060.34, NNZs: 33, Bias: 25795.975759, T: 87584265, Avg. loss: 211939046702.156525\n",
            "Total training time: 22.90 seconds.\n",
            "-- Epoch 236\n",
            "Norm: 4212256.61, NNZs: 33, Bias: 25800.173566, T: 87956964, Avg. loss: 210241982841.952057\n",
            "Total training time: 23.00 seconds.\n",
            "-- Epoch 237\n",
            "Norm: 4206615.59, NNZs: 33, Bias: 25804.345174, T: 88329663, Avg. loss: 209853970298.344727\n",
            "Total training time: 23.09 seconds.\n",
            "-- Epoch 238\n",
            "Norm: 4200331.94, NNZs: 33, Bias: 25808.529166, T: 88702362, Avg. loss: 209144157324.622040\n",
            "Total training time: 23.19 seconds.\n",
            "-- Epoch 239\n",
            "Norm: 4195301.16, NNZs: 33, Bias: 25812.647899, T: 89075061, Avg. loss: 207739243259.227448\n",
            "Total training time: 23.29 seconds.\n",
            "-- Epoch 240\n",
            "Norm: 4188983.64, NNZs: 33, Bias: 25816.732493, T: 89447760, Avg. loss: 207571042907.862671\n",
            "Total training time: 23.38 seconds.\n",
            "-- Epoch 241\n",
            "Norm: 4183935.86, NNZs: 33, Bias: 25820.866553, T: 89820459, Avg. loss: 205866215161.636475\n",
            "Total training time: 23.48 seconds.\n",
            "-- Epoch 242\n",
            "Norm: 4178318.32, NNZs: 33, Bias: 25824.926985, T: 90193158, Avg. loss: 205092190880.392181\n",
            "Total training time: 23.58 seconds.\n",
            "-- Epoch 243\n",
            "Norm: 4171951.20, NNZs: 33, Bias: 25828.982521, T: 90565857, Avg. loss: 203874153744.659058\n",
            "Total training time: 23.68 seconds.\n",
            "-- Epoch 244\n",
            "Norm: 4165779.80, NNZs: 33, Bias: 25833.000751, T: 90938556, Avg. loss: 203646848428.723846\n",
            "Total training time: 23.77 seconds.\n",
            "-- Epoch 245\n",
            "Norm: 4160468.44, NNZs: 33, Bias: 25837.120455, T: 91311255, Avg. loss: 202975333034.677582\n",
            "Total training time: 23.87 seconds.\n",
            "-- Epoch 246\n",
            "Norm: 4154350.49, NNZs: 33, Bias: 25841.181317, T: 91683954, Avg. loss: 202685240304.977356\n",
            "Total training time: 23.96 seconds.\n",
            "-- Epoch 247\n",
            "Norm: 4149110.90, NNZs: 33, Bias: 25845.223703, T: 92056653, Avg. loss: 201461019147.340027\n",
            "Total training time: 24.07 seconds.\n",
            "-- Epoch 248\n",
            "Norm: 4143271.68, NNZs: 33, Bias: 25849.172735, T: 92429352, Avg. loss: 200563279614.998413\n",
            "Total training time: 24.17 seconds.\n",
            "-- Epoch 249\n",
            "Norm: 4138005.61, NNZs: 33, Bias: 25853.106936, T: 92802051, Avg. loss: 199734633435.776398\n",
            "Total training time: 24.26 seconds.\n",
            "-- Epoch 250\n",
            "Norm: 4133201.02, NNZs: 33, Bias: 25857.105705, T: 93174750, Avg. loss: 197864962335.783630\n",
            "Total training time: 24.36 seconds.\n",
            "-- Epoch 251\n",
            "Norm: 4127411.40, NNZs: 33, Bias: 25861.023313, T: 93547449, Avg. loss: 197028146786.789734\n",
            "Total training time: 24.46 seconds.\n",
            "-- Epoch 252\n",
            "Norm: 4122162.98, NNZs: 33, Bias: 25864.937395, T: 93920148, Avg. loss: 197332831141.951019\n",
            "Total training time: 24.55 seconds.\n",
            "-- Epoch 253\n",
            "Norm: 4116820.74, NNZs: 33, Bias: 25868.824085, T: 94292847, Avg. loss: 196402015342.883301\n",
            "Total training time: 24.65 seconds.\n",
            "-- Epoch 254\n",
            "Norm: 4112135.02, NNZs: 33, Bias: 25872.717753, T: 94665546, Avg. loss: 195402023142.908447\n",
            "Total training time: 24.75 seconds.\n",
            "-- Epoch 255\n",
            "Norm: 4106520.25, NNZs: 33, Bias: 25876.587387, T: 95038245, Avg. loss: 195611769421.434448\n",
            "Total training time: 24.85 seconds.\n",
            "-- Epoch 256\n",
            "Norm: 4100859.41, NNZs: 33, Bias: 25880.373327, T: 95410944, Avg. loss: 193915856078.118683\n",
            "Total training time: 24.94 seconds.\n",
            "-- Epoch 257\n",
            "Norm: 4095436.27, NNZs: 33, Bias: 25884.262795, T: 95783643, Avg. loss: 192523843532.538940\n",
            "Total training time: 25.04 seconds.\n",
            "-- Epoch 258\n",
            "Norm: 4089459.58, NNZs: 33, Bias: 25888.036278, T: 96156342, Avg. loss: 191850820084.893372\n",
            "Total training time: 25.14 seconds.\n",
            "-- Epoch 259\n",
            "Norm: 4083750.37, NNZs: 33, Bias: 25891.828241, T: 96529041, Avg. loss: 192472733579.346130\n",
            "Total training time: 25.24 seconds.\n",
            "-- Epoch 260\n",
            "Norm: 4079752.28, NNZs: 33, Bias: 25895.736193, T: 96901740, Avg. loss: 191103645796.648010\n",
            "Total training time: 25.34 seconds.\n",
            "-- Epoch 261\n",
            "Norm: 4074109.63, NNZs: 33, Bias: 25899.465010, T: 97274439, Avg. loss: 190068405474.228241\n",
            "Total training time: 25.44 seconds.\n",
            "-- Epoch 262\n",
            "Norm: 4068248.93, NNZs: 33, Bias: 25903.200627, T: 97647138, Avg. loss: 189193815176.270447\n",
            "Total training time: 25.53 seconds.\n",
            "-- Epoch 263\n",
            "Norm: 4063622.89, NNZs: 33, Bias: 25906.999061, T: 98019837, Avg. loss: 188770999625.667511\n",
            "Total training time: 25.63 seconds.\n",
            "-- Epoch 264\n",
            "Norm: 4058708.01, NNZs: 33, Bias: 25910.757557, T: 98392536, Avg. loss: 188270626792.347626\n",
            "Total training time: 25.73 seconds.\n",
            "-- Epoch 265\n",
            "Norm: 4053969.42, NNZs: 33, Bias: 25914.497445, T: 98765235, Avg. loss: 186909082985.742981\n",
            "Total training time: 25.83 seconds.\n",
            "-- Epoch 266\n",
            "Norm: 4048809.25, NNZs: 33, Bias: 25918.141525, T: 99137934, Avg. loss: 186588807835.244751\n",
            "Total training time: 25.92 seconds.\n",
            "-- Epoch 267\n",
            "Norm: 4043344.69, NNZs: 33, Bias: 25921.786844, T: 99510633, Avg. loss: 186010894080.166473\n",
            "Total training time: 26.02 seconds.\n",
            "-- Epoch 268\n",
            "Norm: 4039476.04, NNZs: 33, Bias: 25925.504867, T: 99883332, Avg. loss: 185183441491.079041\n",
            "Total training time: 26.11 seconds.\n",
            "-- Epoch 269\n",
            "Norm: 4034313.08, NNZs: 33, Bias: 25929.094294, T: 100256031, Avg. loss: 184072675409.196808\n",
            "Total training time: 26.21 seconds.\n",
            "-- Epoch 270\n",
            "Norm: 4029814.22, NNZs: 33, Bias: 25932.761650, T: 100628730, Avg. loss: 183757017912.183990\n",
            "Total training time: 26.31 seconds.\n",
            "-- Epoch 271\n",
            "Norm: 4024223.14, NNZs: 33, Bias: 25936.368897, T: 101001429, Avg. loss: 183367468544.173492\n",
            "Total training time: 26.41 seconds.\n",
            "-- Epoch 272\n",
            "Norm: 4019436.38, NNZs: 33, Bias: 25940.009988, T: 101374128, Avg. loss: 182268719978.470276\n",
            "Total training time: 26.51 seconds.\n",
            "-- Epoch 273\n",
            "Norm: 4015087.58, NNZs: 33, Bias: 25943.669186, T: 101746827, Avg. loss: 181783243239.781647\n",
            "Total training time: 26.60 seconds.\n",
            "-- Epoch 274\n",
            "Norm: 4010257.59, NNZs: 33, Bias: 25947.198918, T: 102119526, Avg. loss: 181847129154.534973\n",
            "Total training time: 26.70 seconds.\n",
            "-- Epoch 275\n",
            "Norm: 4005073.69, NNZs: 33, Bias: 25950.849138, T: 102492225, Avg. loss: 180512044635.635010\n",
            "Total training time: 26.80 seconds.\n",
            "-- Epoch 276\n",
            "Norm: 3999796.28, NNZs: 33, Bias: 25954.377645, T: 102864924, Avg. loss: 179953237104.683258\n",
            "Total training time: 26.90 seconds.\n",
            "-- Epoch 277\n",
            "Norm: 3994253.74, NNZs: 33, Bias: 25957.923726, T: 103237623, Avg. loss: 179630084522.351532\n",
            "Total training time: 26.99 seconds.\n",
            "-- Epoch 278\n",
            "Norm: 3989277.51, NNZs: 33, Bias: 25961.496650, T: 103610322, Avg. loss: 178088833590.340759\n",
            "Total training time: 27.09 seconds.\n",
            "-- Epoch 279\n",
            "Norm: 3984793.58, NNZs: 33, Bias: 25964.977355, T: 103983021, Avg. loss: 177633788916.765289\n",
            "Total training time: 27.19 seconds.\n",
            "-- Epoch 280\n",
            "Norm: 3979807.39, NNZs: 33, Bias: 25968.485499, T: 104355720, Avg. loss: 177905777847.981659\n",
            "Total training time: 27.29 seconds.\n",
            "-- Epoch 281\n",
            "Norm: 3974786.81, NNZs: 33, Bias: 25972.045840, T: 104728419, Avg. loss: 176488815570.792236\n",
            "Total training time: 27.39 seconds.\n",
            "-- Epoch 282\n",
            "Norm: 3969580.21, NNZs: 33, Bias: 25975.527829, T: 105101118, Avg. loss: 176692945929.615936\n",
            "Total training time: 27.49 seconds.\n",
            "-- Epoch 283\n",
            "Norm: 3964559.26, NNZs: 33, Bias: 25979.114945, T: 105473817, Avg. loss: 174844337523.053284\n",
            "Total training time: 27.58 seconds.\n",
            "-- Epoch 284\n",
            "Norm: 3960272.23, NNZs: 33, Bias: 25982.650026, T: 105846516, Avg. loss: 175330814055.622620\n",
            "Total training time: 27.68 seconds.\n",
            "-- Epoch 285\n",
            "Norm: 3954814.25, NNZs: 33, Bias: 25986.102853, T: 106219215, Avg. loss: 174048821197.473236\n",
            "Total training time: 27.78 seconds.\n",
            "-- Epoch 286\n",
            "Norm: 3950284.86, NNZs: 33, Bias: 25989.569911, T: 106591914, Avg. loss: 174145420560.117157\n",
            "Total training time: 27.87 seconds.\n",
            "-- Epoch 287\n",
            "Norm: 3945435.83, NNZs: 33, Bias: 25993.029110, T: 106964613, Avg. loss: 172978166224.709412\n",
            "Total training time: 27.97 seconds.\n",
            "-- Epoch 288\n",
            "Norm: 3940657.78, NNZs: 33, Bias: 25996.481388, T: 107337312, Avg. loss: 172597221269.994873\n",
            "Total training time: 28.07 seconds.\n",
            "-- Epoch 289\n",
            "Norm: 3935788.33, NNZs: 33, Bias: 25999.918728, T: 107710011, Avg. loss: 171597875133.467773\n",
            "Total training time: 28.16 seconds.\n",
            "-- Epoch 290\n",
            "Norm: 3931986.26, NNZs: 33, Bias: 26003.396924, T: 108082710, Avg. loss: 171916918306.846710\n",
            "Total training time: 28.26 seconds.\n",
            "-- Epoch 291\n",
            "Norm: 3926300.22, NNZs: 33, Bias: 26006.671548, T: 108455409, Avg. loss: 170509560460.563354\n",
            "Total training time: 28.36 seconds.\n",
            "-- Epoch 292\n",
            "Norm: 3921855.73, NNZs: 33, Bias: 26010.045916, T: 108828108, Avg. loss: 170631788281.054871\n",
            "Total training time: 28.45 seconds.\n",
            "-- Epoch 293\n",
            "Norm: 3916962.15, NNZs: 33, Bias: 26013.439620, T: 109200807, Avg. loss: 169618853952.648468\n",
            "Total training time: 28.55 seconds.\n",
            "-- Epoch 294\n",
            "Norm: 3912559.80, NNZs: 33, Bias: 26016.771987, T: 109573506, Avg. loss: 168838235303.138184\n",
            "Total training time: 28.65 seconds.\n",
            "-- Epoch 295\n",
            "Norm: 3907174.67, NNZs: 33, Bias: 26020.074841, T: 109946205, Avg. loss: 167731507898.110352\n",
            "Total training time: 28.75 seconds.\n",
            "-- Epoch 296\n",
            "Norm: 3902946.65, NNZs: 33, Bias: 26023.452005, T: 110318904, Avg. loss: 167608524539.592316\n",
            "Total training time: 28.84 seconds.\n",
            "-- Epoch 297\n",
            "Norm: 3898473.54, NNZs: 33, Bias: 26026.733922, T: 110691603, Avg. loss: 166946740964.559448\n",
            "Total training time: 28.94 seconds.\n",
            "-- Epoch 298\n",
            "Norm: 3894024.49, NNZs: 33, Bias: 26030.020490, T: 111064302, Avg. loss: 167055384335.690338\n",
            "Total training time: 29.03 seconds.\n",
            "-- Epoch 299\n",
            "Norm: 3888639.49, NNZs: 33, Bias: 26033.268336, T: 111437001, Avg. loss: 165738948802.403687\n",
            "Total training time: 29.13 seconds.\n",
            "-- Epoch 300\n",
            "Norm: 3884006.28, NNZs: 33, Bias: 26036.662623, T: 111809700, Avg. loss: 165200814210.887115\n",
            "Total training time: 29.23 seconds.\n",
            "-- Epoch 301\n",
            "Norm: 3879302.11, NNZs: 33, Bias: 26039.940240, T: 112182399, Avg. loss: 164800371445.195923\n",
            "Total training time: 29.32 seconds.\n",
            "-- Epoch 302\n",
            "Norm: 3875068.43, NNZs: 33, Bias: 26043.191613, T: 112555098, Avg. loss: 164443552750.545197\n",
            "Total training time: 29.42 seconds.\n",
            "-- Epoch 303\n",
            "Norm: 3870171.41, NNZs: 33, Bias: 26046.478819, T: 112927797, Avg. loss: 164311180081.803406\n",
            "Total training time: 29.52 seconds.\n",
            "-- Epoch 304\n",
            "Norm: 3864889.27, NNZs: 33, Bias: 26049.630303, T: 113300496, Avg. loss: 163140326276.717682\n",
            "Total training time: 29.61 seconds.\n",
            "-- Epoch 305\n",
            "Norm: 3861517.42, NNZs: 33, Bias: 26052.882470, T: 113673195, Avg. loss: 163506225019.378052\n",
            "Total training time: 29.71 seconds.\n",
            "-- Epoch 306\n",
            "Norm: 3856280.28, NNZs: 33, Bias: 26056.032531, T: 114045894, Avg. loss: 162419156453.747284\n",
            "Total training time: 29.81 seconds.\n",
            "-- Epoch 307\n",
            "Norm: 3851746.86, NNZs: 33, Bias: 26059.243143, T: 114418593, Avg. loss: 160883669769.885223\n",
            "Total training time: 29.90 seconds.\n",
            "-- Epoch 308\n",
            "Norm: 3847526.18, NNZs: 33, Bias: 26062.476405, T: 114791292, Avg. loss: 161584998393.579773\n",
            "Total training time: 30.00 seconds.\n",
            "-- Epoch 309\n",
            "Norm: 3842344.81, NNZs: 33, Bias: 26065.671483, T: 115163991, Avg. loss: 160915568067.167267\n",
            "Total training time: 30.10 seconds.\n",
            "-- Epoch 310\n",
            "Norm: 3838222.82, NNZs: 33, Bias: 26068.877309, T: 115536690, Avg. loss: 160150157539.546661\n",
            "Total training time: 30.19 seconds.\n",
            "-- Epoch 311\n",
            "Norm: 3833179.86, NNZs: 33, Bias: 26072.000915, T: 115909389, Avg. loss: 159384989242.066742\n",
            "Total training time: 30.29 seconds.\n",
            "-- Epoch 312\n",
            "Norm: 3828539.48, NNZs: 33, Bias: 26075.159791, T: 116282088, Avg. loss: 159270479267.697815\n",
            "Total training time: 30.38 seconds.\n",
            "-- Epoch 313\n",
            "Norm: 3824997.58, NNZs: 33, Bias: 26078.276111, T: 116654787, Avg. loss: 158312029113.691650\n",
            "Total training time: 30.48 seconds.\n",
            "-- Epoch 314\n",
            "Norm: 3820562.49, NNZs: 33, Bias: 26081.432141, T: 117027486, Avg. loss: 158214121775.406372\n",
            "Total training time: 30.58 seconds.\n",
            "-- Epoch 315\n",
            "Norm: 3816302.60, NNZs: 33, Bias: 26084.549389, T: 117400185, Avg. loss: 157544599156.151917\n",
            "Total training time: 30.68 seconds.\n",
            "-- Epoch 316\n",
            "Norm: 3811320.66, NNZs: 33, Bias: 26087.648662, T: 117772884, Avg. loss: 156557237403.023315\n",
            "Total training time: 30.78 seconds.\n",
            "-- Epoch 317\n",
            "Norm: 3806203.88, NNZs: 33, Bias: 26090.752115, T: 118145583, Avg. loss: 156257768003.739227\n",
            "Total training time: 30.87 seconds.\n",
            "-- Epoch 318\n",
            "Norm: 3802654.28, NNZs: 33, Bias: 26093.875126, T: 118518282, Avg. loss: 155944341990.622559\n",
            "Total training time: 30.97 seconds.\n",
            "-- Epoch 319\n",
            "Norm: 3797486.39, NNZs: 33, Bias: 26096.884979, T: 118890981, Avg. loss: 155353418710.213287\n",
            "Total training time: 31.06 seconds.\n",
            "-- Epoch 320\n",
            "Norm: 3793121.62, NNZs: 33, Bias: 26099.983254, T: 119263680, Avg. loss: 154599888931.814301\n",
            "Total training time: 31.16 seconds.\n",
            "-- Epoch 321\n",
            "Norm: 3788903.55, NNZs: 33, Bias: 26103.074697, T: 119636379, Avg. loss: 154852775787.445099\n",
            "Total training time: 31.26 seconds.\n",
            "-- Epoch 322\n",
            "Norm: 3785099.22, NNZs: 33, Bias: 26106.186168, T: 120009078, Avg. loss: 154337815663.141541\n",
            "Total training time: 31.35 seconds.\n",
            "-- Epoch 323\n",
            "Norm: 3779957.30, NNZs: 33, Bias: 26109.108799, T: 120381777, Avg. loss: 153573302914.754456\n",
            "Total training time: 31.45 seconds.\n",
            "-- Epoch 324\n",
            "Norm: 3775283.67, NNZs: 33, Bias: 26112.156100, T: 120754476, Avg. loss: 152701436879.010193\n",
            "Total training time: 31.55 seconds.\n",
            "-- Epoch 325\n",
            "Norm: 3771141.03, NNZs: 33, Bias: 26115.218437, T: 121127175, Avg. loss: 152417670566.227234\n",
            "Total training time: 31.64 seconds.\n",
            "-- Epoch 326\n",
            "Norm: 3766693.39, NNZs: 33, Bias: 26118.224885, T: 121499874, Avg. loss: 152265585961.838318\n",
            "Total training time: 31.74 seconds.\n",
            "-- Epoch 327\n",
            "Norm: 3762561.08, NNZs: 33, Bias: 26121.270810, T: 121872573, Avg. loss: 151895159884.288788\n",
            "Total training time: 31.83 seconds.\n",
            "-- Epoch 328\n",
            "Norm: 3758256.06, NNZs: 33, Bias: 26124.267115, T: 122245272, Avg. loss: 151364216415.732727\n",
            "Total training time: 31.93 seconds.\n",
            "-- Epoch 329\n",
            "Norm: 3754396.29, NNZs: 33, Bias: 26127.360034, T: 122617971, Avg. loss: 151300045376.080170\n",
            "Total training time: 32.03 seconds.\n",
            "-- Epoch 330\n",
            "Norm: 3749551.47, NNZs: 33, Bias: 26130.277100, T: 122990670, Avg. loss: 150226111487.874603\n",
            "Total training time: 32.12 seconds.\n",
            "-- Epoch 331\n",
            "Norm: 3745295.33, NNZs: 33, Bias: 26133.275656, T: 123363369, Avg. loss: 149509730009.812561\n",
            "Total training time: 32.22 seconds.\n",
            "-- Epoch 332\n",
            "Norm: 3741434.40, NNZs: 33, Bias: 26136.176441, T: 123736068, Avg. loss: 149391414467.994263\n",
            "Total training time: 32.31 seconds.\n",
            "-- Epoch 333\n",
            "Norm: 3736492.84, NNZs: 33, Bias: 26139.113773, T: 124108767, Avg. loss: 149447138029.570282\n",
            "Total training time: 32.41 seconds.\n",
            "-- Epoch 334\n",
            "Norm: 3732632.91, NNZs: 33, Bias: 26142.104686, T: 124481466, Avg. loss: 148899023471.112152\n",
            "Total training time: 32.50 seconds.\n",
            "-- Epoch 335\n",
            "Norm: 3728404.91, NNZs: 33, Bias: 26145.004503, T: 124854165, Avg. loss: 147822477254.380585\n",
            "Total training time: 32.60 seconds.\n",
            "-- Epoch 336\n",
            "Norm: 3724165.42, NNZs: 33, Bias: 26147.924734, T: 125226864, Avg. loss: 147409862799.978241\n",
            "Total training time: 32.69 seconds.\n",
            "-- Epoch 337\n",
            "Norm: 3719501.35, NNZs: 33, Bias: 26150.805102, T: 125599563, Avg. loss: 147041243103.046021\n",
            "Total training time: 32.80 seconds.\n",
            "-- Epoch 338\n",
            "Norm: 3714962.32, NNZs: 33, Bias: 26153.727509, T: 125972262, Avg. loss: 146778782357.954346\n",
            "Total training time: 32.89 seconds.\n",
            "-- Epoch 339\n",
            "Norm: 3711350.32, NNZs: 33, Bias: 26156.636530, T: 126344961, Avg. loss: 146050447023.283539\n",
            "Total training time: 32.99 seconds.\n",
            "-- Epoch 340\n",
            "Norm: 3708005.32, NNZs: 33, Bias: 26159.588434, T: 126717660, Avg. loss: 145775022802.836212\n",
            "Total training time: 33.09 seconds.\n",
            "-- Epoch 341\n",
            "Norm: 3704018.99, NNZs: 33, Bias: 26162.450733, T: 127090359, Avg. loss: 145068974083.665405\n",
            "Total training time: 33.19 seconds.\n",
            "-- Epoch 342\n",
            "Norm: 3699551.91, NNZs: 33, Bias: 26165.295484, T: 127463058, Avg. loss: 144773139741.436523\n",
            "Total training time: 33.29 seconds.\n",
            "-- Epoch 343\n",
            "Norm: 3695041.91, NNZs: 33, Bias: 26168.119191, T: 127835757, Avg. loss: 144918177436.666840\n",
            "Total training time: 33.39 seconds.\n",
            "-- Epoch 344\n",
            "Norm: 3691051.06, NNZs: 33, Bias: 26170.941529, T: 128208456, Avg. loss: 144086664272.857880\n",
            "Total training time: 33.48 seconds.\n",
            "-- Epoch 345\n",
            "Norm: 3686824.98, NNZs: 33, Bias: 26173.768061, T: 128581155, Avg. loss: 144016248152.529114\n",
            "Total training time: 33.58 seconds.\n",
            "-- Epoch 346\n",
            "Norm: 3683567.20, NNZs: 33, Bias: 26176.686456, T: 128953854, Avg. loss: 143366832987.578949\n",
            "Total training time: 33.67 seconds.\n",
            "-- Epoch 347\n",
            "Norm: 3679334.60, NNZs: 33, Bias: 26179.495403, T: 129326553, Avg. loss: 143452734872.930145\n",
            "Total training time: 33.78 seconds.\n",
            "-- Epoch 348\n",
            "Norm: 3675234.10, NNZs: 33, Bias: 26182.365841, T: 129699252, Avg. loss: 142088705300.163025\n",
            "Total training time: 33.87 seconds.\n",
            "-- Epoch 349\n",
            "Norm: 3671186.78, NNZs: 33, Bias: 26185.218483, T: 130071951, Avg. loss: 141640489594.163879\n",
            "Total training time: 33.97 seconds.\n",
            "-- Epoch 350\n",
            "Norm: 3667678.70, NNZs: 33, Bias: 26188.048829, T: 130444650, Avg. loss: 142482043971.923737\n",
            "Total training time: 34.07 seconds.\n",
            "-- Epoch 351\n",
            "Norm: 3663548.44, NNZs: 33, Bias: 26190.850307, T: 130817349, Avg. loss: 141235214695.217865\n",
            "Total training time: 34.16 seconds.\n",
            "-- Epoch 352\n",
            "Norm: 3659495.92, NNZs: 33, Bias: 26193.624879, T: 131190048, Avg. loss: 140918225205.833435\n",
            "Total training time: 34.26 seconds.\n",
            "-- Epoch 353\n",
            "Norm: 3655966.02, NNZs: 33, Bias: 26196.414365, T: 131562747, Avg. loss: 140805199293.784454\n",
            "Total training time: 34.35 seconds.\n",
            "-- Epoch 354\n",
            "Norm: 3652179.89, NNZs: 33, Bias: 26199.209704, T: 131935446, Avg. loss: 140195831408.728363\n",
            "Total training time: 34.45 seconds.\n",
            "-- Epoch 355\n",
            "Norm: 3648293.92, NNZs: 33, Bias: 26201.987672, T: 132308145, Avg. loss: 139914711070.011383\n",
            "Total training time: 34.54 seconds.\n",
            "-- Epoch 356\n",
            "Norm: 3644336.94, NNZs: 33, Bias: 26204.742456, T: 132680844, Avg. loss: 139277234575.748718\n",
            "Total training time: 34.65 seconds.\n",
            "-- Epoch 357\n",
            "Norm: 3640316.72, NNZs: 33, Bias: 26207.501817, T: 133053543, Avg. loss: 139203662177.849091\n",
            "Total training time: 34.74 seconds.\n",
            "-- Epoch 358\n",
            "Norm: 3636078.23, NNZs: 33, Bias: 26210.279932, T: 133426242, Avg. loss: 138403651471.761261\n",
            "Total training time: 34.84 seconds.\n",
            "-- Epoch 359\n",
            "Norm: 3631637.79, NNZs: 33, Bias: 26212.972833, T: 133798941, Avg. loss: 137866172548.435455\n",
            "Total training time: 34.94 seconds.\n",
            "-- Epoch 360\n",
            "Norm: 3628026.55, NNZs: 33, Bias: 26215.741701, T: 134171640, Avg. loss: 138026288246.552948\n",
            "Total training time: 35.04 seconds.\n",
            "-- Epoch 361\n",
            "Norm: 3623960.83, NNZs: 33, Bias: 26218.547935, T: 134544339, Avg. loss: 137374978045.604507\n",
            "Total training time: 35.14 seconds.\n",
            "-- Epoch 362\n",
            "Norm: 3620361.58, NNZs: 33, Bias: 26221.269788, T: 134917038, Avg. loss: 136892030371.881027\n",
            "Total training time: 35.24 seconds.\n",
            "-- Epoch 363\n",
            "Norm: 3616082.05, NNZs: 33, Bias: 26223.951531, T: 135289737, Avg. loss: 136395776990.867249\n",
            "Total training time: 35.33 seconds.\n",
            "-- Epoch 364\n",
            "Norm: 3612857.39, NNZs: 33, Bias: 26226.704454, T: 135662436, Avg. loss: 136360808595.483749\n",
            "Total training time: 35.43 seconds.\n",
            "-- Epoch 365\n",
            "Norm: 3608712.97, NNZs: 33, Bias: 26229.355694, T: 136035135, Avg. loss: 136291275235.751923\n",
            "Total training time: 35.53 seconds.\n",
            "-- Epoch 366\n",
            "Norm: 3604652.02, NNZs: 33, Bias: 26232.029481, T: 136407834, Avg. loss: 135245135949.549377\n",
            "Total training time: 35.63 seconds.\n",
            "-- Epoch 367\n",
            "Norm: 3600620.70, NNZs: 33, Bias: 26234.763059, T: 136780533, Avg. loss: 135445991413.029510\n",
            "Total training time: 35.74 seconds.\n",
            "-- Epoch 368\n",
            "Norm: 3596878.70, NNZs: 33, Bias: 26237.413037, T: 137153232, Avg. loss: 134554088265.532791\n",
            "Total training time: 35.84 seconds.\n",
            "-- Epoch 369\n",
            "Norm: 3593064.10, NNZs: 33, Bias: 26240.037758, T: 137525931, Avg. loss: 134279857607.440598\n",
            "Total training time: 35.94 seconds.\n",
            "-- Epoch 370\n",
            "Norm: 3589773.15, NNZs: 33, Bias: 26242.759969, T: 137898630, Avg. loss: 134202835002.637558\n",
            "Total training time: 36.04 seconds.\n",
            "-- Epoch 371\n",
            "Norm: 3585717.81, NNZs: 33, Bias: 26245.364907, T: 138271329, Avg. loss: 133856328338.733185\n",
            "Total training time: 36.14 seconds.\n",
            "-- Epoch 372\n",
            "Norm: 3581932.25, NNZs: 33, Bias: 26248.007434, T: 138644028, Avg. loss: 133530843047.680771\n",
            "Total training time: 36.23 seconds.\n",
            "-- Epoch 373\n",
            "Norm: 3578402.17, NNZs: 33, Bias: 26250.693799, T: 139016727, Avg. loss: 133550720333.494492\n",
            "Total training time: 36.33 seconds.\n",
            "-- Epoch 374\n",
            "Norm: 3574320.87, NNZs: 33, Bias: 26253.327336, T: 139389426, Avg. loss: 133073815342.229431\n",
            "Total training time: 36.43 seconds.\n",
            "-- Epoch 375\n",
            "Norm: 3570357.36, NNZs: 33, Bias: 26255.985908, T: 139762125, Avg. loss: 132099723383.051788\n",
            "Total training time: 36.52 seconds.\n",
            "-- Epoch 376\n",
            "Norm: 3566585.53, NNZs: 33, Bias: 26258.582391, T: 140134824, Avg. loss: 131955226229.988708\n",
            "Total training time: 36.61 seconds.\n",
            "-- Epoch 377\n",
            "Norm: 3562703.25, NNZs: 33, Bias: 26261.203292, T: 140507523, Avg. loss: 131521836775.054031\n",
            "Total training time: 36.71 seconds.\n",
            "-- Epoch 378\n",
            "Norm: 3559235.18, NNZs: 33, Bias: 26263.774846, T: 140880222, Avg. loss: 130996433462.229736\n",
            "Total training time: 36.81 seconds.\n",
            "-- Epoch 379\n",
            "Norm: 3555494.08, NNZs: 33, Bias: 26266.386160, T: 141252921, Avg. loss: 130830333158.641815\n",
            "Total training time: 36.90 seconds.\n",
            "-- Epoch 380\n",
            "Norm: 3551763.60, NNZs: 33, Bias: 26268.931642, T: 141625620, Avg. loss: 130454896329.204010\n",
            "Total training time: 37.00 seconds.\n",
            "-- Epoch 381\n",
            "Norm: 3548034.78, NNZs: 33, Bias: 26271.485375, T: 141998319, Avg. loss: 129950041321.712219\n",
            "Total training time: 37.09 seconds.\n",
            "-- Epoch 382\n",
            "Norm: 3543856.68, NNZs: 33, Bias: 26273.955345, T: 142371018, Avg. loss: 129910109030.975189\n",
            "Total training time: 37.19 seconds.\n",
            "-- Epoch 383\n",
            "Norm: 3540331.47, NNZs: 33, Bias: 26276.559191, T: 142743717, Avg. loss: 130281967672.722900\n",
            "Total training time: 37.29 seconds.\n",
            "-- Epoch 384\n",
            "Norm: 3536259.33, NNZs: 33, Bias: 26279.105871, T: 143116416, Avg. loss: 128861236720.623947\n",
            "Total training time: 37.39 seconds.\n",
            "-- Epoch 385\n",
            "Norm: 3532540.96, NNZs: 33, Bias: 26281.759225, T: 143489115, Avg. loss: 129117535314.511841\n",
            "Total training time: 37.49 seconds.\n",
            "-- Epoch 386\n",
            "Norm: 3528531.25, NNZs: 33, Bias: 26284.315778, T: 143861814, Avg. loss: 128705766922.232559\n",
            "Total training time: 37.58 seconds.\n",
            "-- Epoch 387\n",
            "Norm: 3525487.60, NNZs: 33, Bias: 26286.906053, T: 144234513, Avg. loss: 128082825433.429642\n",
            "Total training time: 37.68 seconds.\n",
            "-- Epoch 388\n",
            "Norm: 3521366.10, NNZs: 33, Bias: 26289.468264, T: 144607212, Avg. loss: 128189708588.124695\n",
            "Total training time: 37.78 seconds.\n",
            "-- Epoch 389\n",
            "Norm: 3517867.60, NNZs: 33, Bias: 26291.993782, T: 144979911, Avg. loss: 127491332929.646194\n",
            "Total training time: 37.88 seconds.\n",
            "-- Epoch 390\n",
            "Norm: 3513840.18, NNZs: 33, Bias: 26294.531540, T: 145352610, Avg. loss: 127307118838.677063\n",
            "Total training time: 37.97 seconds.\n",
            "-- Epoch 391\n",
            "Norm: 3510667.58, NNZs: 33, Bias: 26297.158150, T: 145725309, Avg. loss: 127109148515.627533\n",
            "Total training time: 38.07 seconds.\n",
            "-- Epoch 392\n",
            "Norm: 3507446.47, NNZs: 33, Bias: 26299.639087, T: 146098008, Avg. loss: 126373412021.382568\n",
            "Total training time: 38.16 seconds.\n",
            "-- Epoch 393\n",
            "Norm: 3503355.37, NNZs: 33, Bias: 26302.076758, T: 146470707, Avg. loss: 126120149602.795258\n",
            "Total training time: 38.26 seconds.\n",
            "-- Epoch 394\n",
            "Norm: 3499599.82, NNZs: 33, Bias: 26304.570194, T: 146843406, Avg. loss: 125831178706.572968\n",
            "Total training time: 38.36 seconds.\n",
            "-- Epoch 395\n",
            "Norm: 3496715.31, NNZs: 33, Bias: 26307.069523, T: 147216105, Avg. loss: 125276887068.429184\n",
            "Total training time: 38.46 seconds.\n",
            "-- Epoch 396\n",
            "Norm: 3492737.60, NNZs: 33, Bias: 26309.536796, T: 147588804, Avg. loss: 124955894146.850067\n",
            "Total training time: 38.55 seconds.\n",
            "-- Epoch 397\n",
            "Norm: 3489275.45, NNZs: 33, Bias: 26311.995638, T: 147961503, Avg. loss: 124786889752.458099\n",
            "Total training time: 38.65 seconds.\n",
            "-- Epoch 398\n",
            "Norm: 3485957.71, NNZs: 33, Bias: 26314.467179, T: 148334202, Avg. loss: 124192261855.899826\n",
            "Total training time: 38.74 seconds.\n",
            "-- Epoch 399\n",
            "Norm: 3482106.31, NNZs: 33, Bias: 26316.902114, T: 148706901, Avg. loss: 123701212140.172546\n",
            "Total training time: 38.85 seconds.\n",
            "-- Epoch 400\n",
            "Norm: 3479222.40, NNZs: 33, Bias: 26319.389466, T: 149079600, Avg. loss: 124151857563.210297\n",
            "Total training time: 38.94 seconds.\n",
            "-- Epoch 401\n",
            "Norm: 3475605.45, NNZs: 33, Bias: 26321.855603, T: 149452299, Avg. loss: 123406200226.482117\n",
            "Total training time: 39.03 seconds.\n",
            "-- Epoch 402\n",
            "Norm: 3471311.34, NNZs: 33, Bias: 26324.266481, T: 149824998, Avg. loss: 123409626698.893402\n",
            "Total training time: 39.13 seconds.\n",
            "-- Epoch 403\n",
            "Norm: 3467965.66, NNZs: 33, Bias: 26326.706539, T: 150197697, Avg. loss: 122880386946.388565\n",
            "Total training time: 39.23 seconds.\n",
            "-- Epoch 404\n",
            "Norm: 3464424.27, NNZs: 33, Bias: 26329.143974, T: 150570396, Avg. loss: 122715984586.949341\n",
            "Total training time: 39.32 seconds.\n",
            "-- Epoch 405\n",
            "Norm: 3460676.17, NNZs: 33, Bias: 26331.532155, T: 150943095, Avg. loss: 122300345373.960449\n",
            "Total training time: 39.42 seconds.\n",
            "-- Epoch 406\n",
            "Norm: 3457153.40, NNZs: 33, Bias: 26333.979327, T: 151315794, Avg. loss: 121758098443.271530\n",
            "Total training time: 39.52 seconds.\n",
            "-- Epoch 407\n",
            "Norm: 3453626.87, NNZs: 33, Bias: 26336.422626, T: 151688493, Avg. loss: 121672642508.603729\n",
            "Total training time: 39.62 seconds.\n",
            "-- Epoch 408\n",
            "Norm: 3450129.55, NNZs: 33, Bias: 26338.775878, T: 152061192, Avg. loss: 121090238615.197739\n",
            "Total training time: 39.72 seconds.\n",
            "-- Epoch 409\n",
            "Norm: 3446650.69, NNZs: 33, Bias: 26341.176955, T: 152433891, Avg. loss: 120726394880.734833\n",
            "Total training time: 39.81 seconds.\n",
            "-- Epoch 410\n",
            "Norm: 3443198.46, NNZs: 33, Bias: 26343.590474, T: 152806590, Avg. loss: 120790121630.905396\n",
            "Total training time: 39.91 seconds.\n",
            "-- Epoch 411\n",
            "Norm: 3439918.65, NNZs: 33, Bias: 26346.000542, T: 153179289, Avg. loss: 120618348021.612961\n",
            "Total training time: 40.01 seconds.\n",
            "-- Epoch 412\n",
            "Norm: 3436042.79, NNZs: 33, Bias: 26348.367676, T: 153551988, Avg. loss: 120414517794.438797\n",
            "Total training time: 40.11 seconds.\n",
            "-- Epoch 413\n",
            "Norm: 3432194.67, NNZs: 33, Bias: 26350.752503, T: 153924687, Avg. loss: 119522074240.859848\n",
            "Total training time: 40.20 seconds.\n",
            "-- Epoch 414\n",
            "Norm: 3429260.52, NNZs: 33, Bias: 26353.196201, T: 154297386, Avg. loss: 119608916331.338181\n",
            "Total training time: 40.30 seconds.\n",
            "-- Epoch 415\n",
            "Norm: 3425822.41, NNZs: 33, Bias: 26355.609650, T: 154670085, Avg. loss: 119134145775.138367\n",
            "Total training time: 40.39 seconds.\n",
            "-- Epoch 416\n",
            "Norm: 3422455.22, NNZs: 33, Bias: 26358.008074, T: 155042784, Avg. loss: 118643434486.722031\n",
            "Total training time: 40.49 seconds.\n",
            "-- Epoch 417\n",
            "Norm: 3418959.57, NNZs: 33, Bias: 26360.346028, T: 155415483, Avg. loss: 119104583833.307419\n",
            "Total training time: 40.58 seconds.\n",
            "-- Epoch 418\n",
            "Norm: 3416129.29, NNZs: 33, Bias: 26362.730807, T: 155788182, Avg. loss: 118630533134.575104\n",
            "Total training time: 40.68 seconds.\n",
            "-- Epoch 419\n",
            "Norm: 3412635.60, NNZs: 33, Bias: 26365.052455, T: 156160881, Avg. loss: 118483672144.743362\n",
            "Total training time: 40.77 seconds.\n",
            "-- Epoch 420\n",
            "Norm: 3408575.05, NNZs: 33, Bias: 26367.353201, T: 156533580, Avg. loss: 117980341233.560211\n",
            "Total training time: 40.87 seconds.\n",
            "-- Epoch 421\n",
            "Norm: 3405445.51, NNZs: 33, Bias: 26369.717131, T: 156906279, Avg. loss: 117595639700.128311\n",
            "Total training time: 40.97 seconds.\n",
            "-- Epoch 422\n",
            "Norm: 3402143.48, NNZs: 33, Bias: 26372.065259, T: 157278978, Avg. loss: 117234879207.517181\n",
            "Total training time: 41.06 seconds.\n",
            "-- Epoch 423\n",
            "Norm: 3398692.94, NNZs: 33, Bias: 26374.425127, T: 157651677, Avg. loss: 117040207962.105682\n",
            "Total training time: 41.16 seconds.\n",
            "-- Epoch 424\n",
            "Norm: 3394883.30, NNZs: 33, Bias: 26376.724179, T: 158024376, Avg. loss: 116546188817.218079\n",
            "Total training time: 41.26 seconds.\n",
            "-- Epoch 425\n",
            "Norm: 3391506.11, NNZs: 33, Bias: 26379.054465, T: 158397075, Avg. loss: 116733031752.521240\n",
            "Total training time: 41.35 seconds.\n",
            "-- Epoch 426\n",
            "Norm: 3387903.99, NNZs: 33, Bias: 26381.386340, T: 158769774, Avg. loss: 116088696657.993073\n",
            "Total training time: 41.46 seconds.\n",
            "-- Epoch 427\n",
            "Norm: 3384659.69, NNZs: 33, Bias: 26383.713298, T: 159142473, Avg. loss: 116136365854.798126\n",
            "Total training time: 41.55 seconds.\n",
            "-- Epoch 428\n",
            "Norm: 3381721.28, NNZs: 33, Bias: 26386.027996, T: 159515172, Avg. loss: 115900091242.735382\n",
            "Total training time: 41.65 seconds.\n",
            "-- Epoch 429\n",
            "Norm: 3378429.46, NNZs: 33, Bias: 26388.328811, T: 159887871, Avg. loss: 115550364492.619492\n",
            "Total training time: 41.75 seconds.\n",
            "-- Epoch 430\n",
            "Norm: 3375398.72, NNZs: 33, Bias: 26390.597432, T: 160260570, Avg. loss: 115326339651.012497\n",
            "Total training time: 41.85 seconds.\n",
            "-- Epoch 431\n",
            "Norm: 3371765.66, NNZs: 33, Bias: 26392.846114, T: 160633269, Avg. loss: 115024219246.779648\n",
            "Total training time: 41.94 seconds.\n",
            "-- Epoch 432\n",
            "Norm: 3368197.28, NNZs: 33, Bias: 26395.103021, T: 161005968, Avg. loss: 114430652725.869247\n",
            "Total training time: 42.04 seconds.\n",
            "-- Epoch 433\n",
            "Norm: 3364820.04, NNZs: 33, Bias: 26397.374834, T: 161378667, Avg. loss: 113977000016.939377\n",
            "Total training time: 42.14 seconds.\n",
            "-- Epoch 434\n",
            "Norm: 3361474.47, NNZs: 33, Bias: 26399.696863, T: 161751366, Avg. loss: 113857742178.345230\n",
            "Total training time: 42.24 seconds.\n",
            "-- Epoch 435\n",
            "Norm: 3358239.43, NNZs: 33, Bias: 26401.974991, T: 162124065, Avg. loss: 113586017014.064392\n",
            "Total training time: 42.34 seconds.\n",
            "-- Epoch 436\n",
            "Norm: 3355109.95, NNZs: 33, Bias: 26404.256489, T: 162496764, Avg. loss: 113980413227.188828\n",
            "Total training time: 42.43 seconds.\n",
            "-- Epoch 437\n",
            "Norm: 3352204.53, NNZs: 33, Bias: 26406.555654, T: 162869463, Avg. loss: 113617576697.450394\n",
            "Total training time: 42.53 seconds.\n",
            "-- Epoch 438\n",
            "Norm: 3348452.86, NNZs: 33, Bias: 26408.748494, T: 163242162, Avg. loss: 113220513180.999664\n",
            "Total training time: 42.62 seconds.\n",
            "-- Epoch 439\n",
            "Norm: 3344678.70, NNZs: 33, Bias: 26410.928776, T: 163614861, Avg. loss: 112768166851.255905\n",
            "Total training time: 42.72 seconds.\n",
            "-- Epoch 440\n",
            "Norm: 3341774.45, NNZs: 33, Bias: 26413.185900, T: 163987560, Avg. loss: 112459756385.121933\n",
            "Total training time: 42.82 seconds.\n",
            "-- Epoch 441\n",
            "Norm: 3338958.55, NNZs: 33, Bias: 26415.424027, T: 164360259, Avg. loss: 112568160450.204453\n",
            "Total training time: 42.92 seconds.\n",
            "-- Epoch 442\n",
            "Norm: 3335630.18, NNZs: 33, Bias: 26417.687183, T: 164732958, Avg. loss: 112437856224.543350\n",
            "Total training time: 43.01 seconds.\n",
            "-- Epoch 443\n",
            "Norm: 3332124.41, NNZs: 33, Bias: 26419.887061, T: 165105657, Avg. loss: 111680203470.742599\n",
            "Total training time: 43.11 seconds.\n",
            "-- Epoch 444\n",
            "Norm: 3329244.04, NNZs: 33, Bias: 26422.200547, T: 165478356, Avg. loss: 111783825898.104065\n",
            "Total training time: 43.20 seconds.\n",
            "-- Epoch 445\n",
            "Norm: 3326285.09, NNZs: 33, Bias: 26424.430329, T: 165851055, Avg. loss: 111221362840.171356\n",
            "Total training time: 43.30 seconds.\n",
            "-- Epoch 446\n",
            "Norm: 3323002.24, NNZs: 33, Bias: 26426.628116, T: 166223754, Avg. loss: 110827491249.982361\n",
            "Total training time: 43.40 seconds.\n",
            "-- Epoch 447\n",
            "Norm: 3319681.32, NNZs: 33, Bias: 26428.797237, T: 166596453, Avg. loss: 111182377237.806595\n",
            "Total training time: 43.50 seconds.\n",
            "-- Epoch 448\n",
            "Norm: 3316202.94, NNZs: 33, Bias: 26430.930528, T: 166969152, Avg. loss: 110456959107.501404\n",
            "Total training time: 43.59 seconds.\n",
            "-- Epoch 449\n",
            "Norm: 3313351.33, NNZs: 33, Bias: 26433.141865, T: 167341851, Avg. loss: 110158446068.860733\n",
            "Total training time: 43.69 seconds.\n",
            "-- Epoch 450\n",
            "Norm: 3309927.07, NNZs: 33, Bias: 26435.309623, T: 167714550, Avg. loss: 109827742076.527130\n",
            "Total training time: 43.79 seconds.\n",
            "-- Epoch 451\n",
            "Norm: 3306886.76, NNZs: 33, Bias: 26437.498515, T: 168087249, Avg. loss: 109881210162.017456\n",
            "Total training time: 43.89 seconds.\n",
            "-- Epoch 452\n",
            "Norm: 3303917.62, NNZs: 33, Bias: 26439.678032, T: 168459948, Avg. loss: 109366464507.515839\n",
            "Total training time: 43.98 seconds.\n",
            "-- Epoch 453\n",
            "Norm: 3300767.13, NNZs: 33, Bias: 26441.852037, T: 168832647, Avg. loss: 109086023823.062454\n",
            "Total training time: 44.08 seconds.\n",
            "-- Epoch 454\n",
            "Norm: 3297316.30, NNZs: 33, Bias: 26444.021461, T: 169205346, Avg. loss: 109236694236.048523\n",
            "Total training time: 44.18 seconds.\n",
            "-- Epoch 455\n",
            "Norm: 3294055.16, NNZs: 33, Bias: 26446.145545, T: 169578045, Avg. loss: 108823826430.267746\n",
            "Total training time: 44.28 seconds.\n",
            "-- Epoch 456\n",
            "Norm: 3290789.86, NNZs: 33, Bias: 26448.338943, T: 169950744, Avg. loss: 108953788065.179443\n",
            "Total training time: 44.38 seconds.\n",
            "-- Epoch 457\n",
            "Norm: 3287710.89, NNZs: 33, Bias: 26450.485198, T: 170323443, Avg. loss: 108324324204.659851\n",
            "Total training time: 44.48 seconds.\n",
            "-- Epoch 458\n",
            "Norm: 3284510.74, NNZs: 33, Bias: 26452.585036, T: 170696142, Avg. loss: 108317268758.600906\n",
            "Total training time: 44.57 seconds.\n",
            "-- Epoch 459\n",
            "Norm: 3281514.46, NNZs: 33, Bias: 26454.736468, T: 171068841, Avg. loss: 107983648511.600830\n",
            "Total training time: 44.67 seconds.\n",
            "-- Epoch 460\n",
            "Norm: 3278587.09, NNZs: 33, Bias: 26456.855645, T: 171441540, Avg. loss: 107906621034.316650\n",
            "Total training time: 44.76 seconds.\n",
            "-- Epoch 461\n",
            "Norm: 3275559.47, NNZs: 33, Bias: 26458.983724, T: 171814239, Avg. loss: 107993909768.993103\n",
            "Total training time: 44.86 seconds.\n",
            "-- Epoch 462\n",
            "Norm: 3272332.26, NNZs: 33, Bias: 26461.125171, T: 172186938, Avg. loss: 107360329247.558075\n",
            "Total training time: 44.96 seconds.\n",
            "-- Epoch 463\n",
            "Norm: 3269456.04, NNZs: 33, Bias: 26463.250329, T: 172559637, Avg. loss: 106763694347.114685\n",
            "Total training time: 45.06 seconds.\n",
            "-- Epoch 464\n",
            "Norm: 3266563.26, NNZs: 33, Bias: 26465.384550, T: 172932336, Avg. loss: 106578054509.694046\n",
            "Total training time: 45.15 seconds.\n",
            "-- Epoch 465\n",
            "Norm: 3263735.09, NNZs: 33, Bias: 26467.497820, T: 173305035, Avg. loss: 106463263260.534882\n",
            "Total training time: 45.25 seconds.\n",
            "-- Epoch 466\n",
            "Norm: 3260528.25, NNZs: 33, Bias: 26469.580665, T: 173677734, Avg. loss: 106388398547.341187\n",
            "Total training time: 45.35 seconds.\n",
            "-- Epoch 467\n",
            "Norm: 3257549.73, NNZs: 33, Bias: 26471.681612, T: 174050433, Avg. loss: 106166246596.039612\n",
            "Total training time: 45.44 seconds.\n",
            "-- Epoch 468\n",
            "Norm: 3254099.36, NNZs: 33, Bias: 26473.820286, T: 174423132, Avg. loss: 106177534521.621582\n",
            "Total training time: 45.54 seconds.\n",
            "-- Epoch 469\n",
            "Norm: 3251696.33, NNZs: 33, Bias: 26475.900852, T: 174795831, Avg. loss: 105729572288.957977\n",
            "Total training time: 45.64 seconds.\n",
            "-- Epoch 470\n",
            "Norm: 3248775.94, NNZs: 33, Bias: 26478.032721, T: 175168530, Avg. loss: 105295452484.732971\n",
            "Total training time: 45.73 seconds.\n",
            "-- Epoch 471\n",
            "Norm: 3245742.15, NNZs: 33, Bias: 26480.162124, T: 175541229, Avg. loss: 105291267514.678665\n",
            "Total training time: 45.83 seconds.\n",
            "-- Epoch 472\n",
            "Norm: 3242403.61, NNZs: 33, Bias: 26482.231004, T: 175913928, Avg. loss: 104694364565.726654\n",
            "Total training time: 45.93 seconds.\n",
            "-- Epoch 473\n",
            "Norm: 3239516.74, NNZs: 33, Bias: 26484.300502, T: 176286627, Avg. loss: 104367477338.881744\n",
            "Total training time: 46.02 seconds.\n",
            "-- Epoch 474\n",
            "Norm: 3236296.47, NNZs: 33, Bias: 26486.353393, T: 176659326, Avg. loss: 104685460263.828644\n",
            "Total training time: 46.12 seconds.\n",
            "-- Epoch 475\n",
            "Norm: 3233554.19, NNZs: 33, Bias: 26488.479540, T: 177032025, Avg. loss: 104338383464.224335\n",
            "Total training time: 46.22 seconds.\n",
            "-- Epoch 476\n",
            "Norm: 3230580.05, NNZs: 33, Bias: 26490.544347, T: 177404724, Avg. loss: 104139480311.526764\n",
            "Total training time: 46.32 seconds.\n",
            "-- Epoch 477\n",
            "Norm: 3227102.42, NNZs: 33, Bias: 26492.589726, T: 177777423, Avg. loss: 104096601726.986404\n",
            "Total training time: 46.42 seconds.\n",
            "-- Epoch 478\n",
            "Norm: 3224442.86, NNZs: 33, Bias: 26494.687692, T: 178150122, Avg. loss: 103769739345.818649\n",
            "Total training time: 46.52 seconds.\n",
            "-- Epoch 479\n",
            "Norm: 3221200.25, NNZs: 33, Bias: 26496.718456, T: 178522821, Avg. loss: 102946214911.334045\n",
            "Total training time: 46.62 seconds.\n",
            "-- Epoch 480\n",
            "Norm: 3218373.84, NNZs: 33, Bias: 26498.734200, T: 178895520, Avg. loss: 103304428461.898987\n",
            "Total training time: 46.73 seconds.\n",
            "-- Epoch 481\n",
            "Norm: 3215506.88, NNZs: 33, Bias: 26500.807627, T: 179268219, Avg. loss: 102622471678.561935\n",
            "Total training time: 46.82 seconds.\n",
            "-- Epoch 482\n",
            "Norm: 3212192.98, NNZs: 33, Bias: 26502.814152, T: 179640918, Avg. loss: 102708703637.022949\n",
            "Total training time: 46.93 seconds.\n",
            "-- Epoch 483\n",
            "Norm: 3208921.68, NNZs: 33, Bias: 26504.837858, T: 180013617, Avg. loss: 102719779205.567490\n",
            "Total training time: 47.02 seconds.\n",
            "-- Epoch 484\n",
            "Norm: 3205957.03, NNZs: 33, Bias: 26506.902444, T: 180386316, Avg. loss: 102266091266.456055\n",
            "Total training time: 47.12 seconds.\n",
            "-- Epoch 485\n",
            "Norm: 3203531.00, NNZs: 33, Bias: 26508.980737, T: 180759015, Avg. loss: 102393872484.556946\n",
            "Total training time: 47.21 seconds.\n",
            "-- Epoch 486\n",
            "Norm: 3200424.66, NNZs: 33, Bias: 26511.007183, T: 181131714, Avg. loss: 101752225528.528809\n",
            "Total training time: 47.32 seconds.\n",
            "-- Epoch 487\n",
            "Norm: 3197653.19, NNZs: 33, Bias: 26513.042286, T: 181504413, Avg. loss: 101575631634.091660\n",
            "Total training time: 47.41 seconds.\n",
            "-- Epoch 488\n",
            "Norm: 3194564.03, NNZs: 33, Bias: 26515.018115, T: 181877112, Avg. loss: 100747936918.338303\n",
            "Total training time: 47.51 seconds.\n",
            "-- Epoch 489\n",
            "Norm: 3191792.68, NNZs: 33, Bias: 26517.056510, T: 182249811, Avg. loss: 101279027355.522644\n",
            "Total training time: 47.60 seconds.\n",
            "-- Epoch 490\n",
            "Norm: 3188986.62, NNZs: 33, Bias: 26519.075386, T: 182622510, Avg. loss: 100958527950.924515\n",
            "Total training time: 47.70 seconds.\n",
            "-- Epoch 491\n",
            "Norm: 3186175.84, NNZs: 33, Bias: 26521.107660, T: 182995209, Avg. loss: 100877445698.154175\n",
            "Total training time: 47.79 seconds.\n",
            "-- Epoch 492\n",
            "Norm: 3183232.17, NNZs: 33, Bias: 26523.133826, T: 183367908, Avg. loss: 100374709447.904968\n",
            "Total training time: 47.89 seconds.\n",
            "-- Epoch 493\n",
            "Norm: 3179851.15, NNZs: 33, Bias: 26525.101653, T: 183740607, Avg. loss: 100232916554.830856\n",
            "Total training time: 47.98 seconds.\n",
            "-- Epoch 494\n",
            "Norm: 3176893.17, NNZs: 33, Bias: 26527.086987, T: 184113306, Avg. loss: 99986685869.668228\n",
            "Total training time: 48.08 seconds.\n",
            "-- Epoch 495\n",
            "Norm: 3173916.78, NNZs: 33, Bias: 26529.071813, T: 184486005, Avg. loss: 99611602601.374359\n",
            "Total training time: 48.18 seconds.\n",
            "-- Epoch 496\n",
            "Norm: 3171270.76, NNZs: 33, Bias: 26531.110474, T: 184858704, Avg. loss: 100005361833.895599\n",
            "Total training time: 48.27 seconds.\n",
            "-- Epoch 497\n",
            "Norm: 3168533.58, NNZs: 33, Bias: 26533.095732, T: 185231403, Avg. loss: 99556387418.500229\n",
            "Total training time: 48.37 seconds.\n",
            "-- Epoch 498\n",
            "Norm: 3165809.47, NNZs: 33, Bias: 26535.075915, T: 185604102, Avg. loss: 99440169129.774551\n",
            "Total training time: 48.48 seconds.\n",
            "-- Epoch 499\n",
            "Norm: 3162845.34, NNZs: 33, Bias: 26537.042830, T: 185976801, Avg. loss: 99030962002.791504\n",
            "Total training time: 48.57 seconds.\n",
            "-- Epoch 500\n",
            "Norm: 3159804.16, NNZs: 33, Bias: 26538.959676, T: 186349500, Avg. loss: 98851438840.927353\n",
            "Total training time: 48.67 seconds.\n",
            "-- Epoch 501\n",
            "Norm: 3156819.52, NNZs: 33, Bias: 26540.910174, T: 186722199, Avg. loss: 98517958888.278229\n",
            "Total training time: 48.77 seconds.\n",
            "-- Epoch 502\n",
            "Norm: 3154076.90, NNZs: 33, Bias: 26542.916947, T: 187094898, Avg. loss: 98631391578.487030\n",
            "Total training time: 48.86 seconds.\n",
            "-- Epoch 503\n",
            "Norm: 3151274.66, NNZs: 33, Bias: 26544.908997, T: 187467597, Avg. loss: 98368564093.267181\n",
            "Total training time: 48.96 seconds.\n",
            "-- Epoch 504\n",
            "Norm: 3148145.57, NNZs: 33, Bias: 26546.838914, T: 187840296, Avg. loss: 98282668749.108444\n",
            "Total training time: 49.05 seconds.\n",
            "-- Epoch 505\n",
            "Norm: 3145430.25, NNZs: 33, Bias: 26548.791847, T: 188212995, Avg. loss: 98152131097.990875\n",
            "Total training time: 49.15 seconds.\n",
            "-- Epoch 506\n",
            "Norm: 3142636.04, NNZs: 33, Bias: 26550.726421, T: 188585694, Avg. loss: 98026715303.762573\n",
            "Total training time: 49.25 seconds.\n",
            "-- Epoch 507\n",
            "Norm: 3139762.28, NNZs: 33, Bias: 26552.686192, T: 188958393, Avg. loss: 97850158026.074600\n",
            "Total training time: 49.34 seconds.\n",
            "-- Epoch 508\n",
            "Norm: 3136897.02, NNZs: 33, Bias: 26554.634519, T: 189331092, Avg. loss: 97456402603.845016\n",
            "Total training time: 49.44 seconds.\n",
            "-- Epoch 509\n",
            "Norm: 3134127.14, NNZs: 33, Bias: 26556.573745, T: 189703791, Avg. loss: 97050298997.732300\n",
            "Total training time: 49.54 seconds.\n",
            "-- Epoch 510\n",
            "Norm: 3131557.98, NNZs: 33, Bias: 26558.471024, T: 190076490, Avg. loss: 97103772800.020065\n",
            "Total training time: 49.64 seconds.\n",
            "-- Epoch 511\n",
            "Norm: 3128580.04, NNZs: 33, Bias: 26560.369228, T: 190449189, Avg. loss: 96704347308.105957\n",
            "Total training time: 49.73 seconds.\n",
            "-- Epoch 512\n",
            "Norm: 3125437.15, NNZs: 33, Bias: 26562.253247, T: 190821888, Avg. loss: 96695088541.069763\n",
            "Total training time: 49.83 seconds.\n",
            "-- Epoch 513\n",
            "Norm: 3122769.92, NNZs: 33, Bias: 26564.208120, T: 191194587, Avg. loss: 96665747499.271652\n",
            "Total training time: 49.93 seconds.\n",
            "-- Epoch 514\n",
            "Norm: 3119794.62, NNZs: 33, Bias: 26566.143085, T: 191567286, Avg. loss: 96328330302.538681\n",
            "Total training time: 50.03 seconds.\n",
            "-- Epoch 515\n",
            "Norm: 3117078.73, NNZs: 33, Bias: 26568.083048, T: 191939985, Avg. loss: 96549073520.301010\n",
            "Total training time: 50.13 seconds.\n",
            "-- Epoch 516\n",
            "Norm: 3114286.50, NNZs: 33, Bias: 26570.019238, T: 192312684, Avg. loss: 95690690040.378403\n",
            "Total training time: 50.22 seconds.\n",
            "-- Epoch 517\n",
            "Norm: 3111700.59, NNZs: 33, Bias: 26571.943628, T: 192685383, Avg. loss: 95484927073.569580\n",
            "Total training time: 50.32 seconds.\n",
            "-- Epoch 518\n",
            "Norm: 3109055.40, NNZs: 33, Bias: 26573.864756, T: 193058082, Avg. loss: 95860125046.429245\n",
            "Total training time: 50.42 seconds.\n",
            "-- Epoch 519\n",
            "Norm: 3106386.27, NNZs: 33, Bias: 26575.785227, T: 193430781, Avg. loss: 95276965345.376862\n",
            "Total training time: 50.51 seconds.\n",
            "-- Epoch 520\n",
            "Norm: 3103882.58, NNZs: 33, Bias: 26577.728678, T: 193803480, Avg. loss: 95201551436.032120\n",
            "Total training time: 50.61 seconds.\n",
            "-- Epoch 521\n",
            "Norm: 3100984.58, NNZs: 33, Bias: 26579.613950, T: 194176179, Avg. loss: 95023872654.438629\n",
            "Total training time: 50.71 seconds.\n",
            "-- Epoch 522\n",
            "Norm: 3098510.33, NNZs: 33, Bias: 26581.544584, T: 194548878, Avg. loss: 94575165971.084183\n",
            "Total training time: 50.81 seconds.\n",
            "-- Epoch 523\n",
            "Norm: 3095807.83, NNZs: 33, Bias: 26583.460372, T: 194921577, Avg. loss: 94922402084.830444\n",
            "Total training time: 50.91 seconds.\n",
            "-- Epoch 524\n",
            "Norm: 3092813.71, NNZs: 33, Bias: 26585.333819, T: 195294276, Avg. loss: 94544725251.201080\n",
            "Total training time: 51.01 seconds.\n",
            "-- Epoch 525\n",
            "Norm: 3089829.95, NNZs: 33, Bias: 26587.231906, T: 195666975, Avg. loss: 94610531365.984375\n",
            "Total training time: 51.11 seconds.\n",
            "-- Epoch 526\n",
            "Norm: 3087310.68, NNZs: 33, Bias: 26589.119436, T: 196039674, Avg. loss: 94263155430.334656\n",
            "Total training time: 51.21 seconds.\n",
            "-- Epoch 527\n",
            "Norm: 3084281.06, NNZs: 33, Bias: 26590.981758, T: 196412373, Avg. loss: 93873477420.326874\n",
            "Total training time: 51.30 seconds.\n",
            "-- Epoch 528\n",
            "Norm: 3081680.28, NNZs: 33, Bias: 26592.869859, T: 196785072, Avg. loss: 93840074712.790634\n",
            "Total training time: 51.40 seconds.\n",
            "-- Epoch 529\n",
            "Norm: 3079340.55, NNZs: 33, Bias: 26594.754359, T: 197157771, Avg. loss: 93458593224.600281\n",
            "Total training time: 51.50 seconds.\n",
            "-- Epoch 530\n",
            "Norm: 3076531.46, NNZs: 33, Bias: 26596.608168, T: 197530470, Avg. loss: 92919603168.257339\n",
            "Total training time: 51.59 seconds.\n",
            "-- Epoch 531\n",
            "Norm: 3073225.87, NNZs: 33, Bias: 26598.395558, T: 197903169, Avg. loss: 93220972203.395462\n",
            "Total training time: 51.70 seconds.\n",
            "-- Epoch 532\n",
            "Norm: 3070758.15, NNZs: 33, Bias: 26600.251286, T: 198275868, Avg. loss: 93284479923.331314\n",
            "Total training time: 51.80 seconds.\n",
            "-- Epoch 533\n",
            "Norm: 3068248.09, NNZs: 33, Bias: 26602.136955, T: 198648567, Avg. loss: 92795785341.665680\n",
            "Total training time: 51.89 seconds.\n",
            "-- Epoch 534\n",
            "Norm: 3065616.01, NNZs: 33, Bias: 26603.992318, T: 199021266, Avg. loss: 92394190261.930786\n",
            "Total training time: 52.00 seconds.\n",
            "-- Epoch 535\n",
            "Norm: 3063009.11, NNZs: 33, Bias: 26605.842043, T: 199393965, Avg. loss: 92511695202.088745\n",
            "Total training time: 52.09 seconds.\n",
            "-- Epoch 536\n",
            "Norm: 3060161.48, NNZs: 33, Bias: 26607.703096, T: 199766664, Avg. loss: 92311316038.557693\n",
            "Total training time: 52.19 seconds.\n",
            "-- Epoch 537\n",
            "Norm: 3057423.44, NNZs: 33, Bias: 26609.551500, T: 200139363, Avg. loss: 92549278402.695404\n",
            "Total training time: 52.29 seconds.\n",
            "-- Epoch 538\n",
            "Norm: 3054536.47, NNZs: 33, Bias: 26611.394282, T: 200512062, Avg. loss: 91816678335.692780\n",
            "Total training time: 52.38 seconds.\n",
            "-- Epoch 539\n",
            "Norm: 3052373.40, NNZs: 33, Bias: 26613.229078, T: 200884761, Avg. loss: 91324318475.398819\n",
            "Total training time: 52.48 seconds.\n",
            "-- Epoch 540\n",
            "Norm: 3049602.21, NNZs: 33, Bias: 26615.040944, T: 201257460, Avg. loss: 91690405578.011139\n",
            "Total training time: 52.58 seconds.\n",
            "-- Epoch 541\n",
            "Norm: 3046832.22, NNZs: 33, Bias: 26616.863753, T: 201630159, Avg. loss: 91409886119.491959\n",
            "Total training time: 52.68 seconds.\n",
            "-- Epoch 542\n",
            "Norm: 3043978.42, NNZs: 33, Bias: 26618.660023, T: 202002858, Avg. loss: 91077863725.927063\n",
            "Total training time: 52.77 seconds.\n",
            "-- Epoch 543\n",
            "Norm: 3041460.02, NNZs: 33, Bias: 26620.517873, T: 202375557, Avg. loss: 90987388869.342697\n",
            "Total training time: 52.88 seconds.\n",
            "-- Epoch 544\n",
            "Norm: 3038960.10, NNZs: 33, Bias: 26622.324532, T: 202748256, Avg. loss: 90905517126.855774\n",
            "Total training time: 52.97 seconds.\n",
            "-- Epoch 545\n",
            "Norm: 3036114.52, NNZs: 33, Bias: 26624.114673, T: 203120955, Avg. loss: 90952942381.380157\n",
            "Total training time: 53.07 seconds.\n",
            "-- Epoch 546\n",
            "Norm: 3033867.94, NNZs: 33, Bias: 26625.985358, T: 203493654, Avg. loss: 90822600984.775711\n",
            "Total training time: 53.18 seconds.\n",
            "-- Epoch 547\n",
            "Norm: 3031195.56, NNZs: 33, Bias: 26627.779921, T: 203866353, Avg. loss: 90737005745.045212\n",
            "Total training time: 53.28 seconds.\n",
            "-- Epoch 548\n",
            "Norm: 3028672.39, NNZs: 33, Bias: 26629.603734, T: 204239052, Avg. loss: 90300070233.337128\n",
            "Total training time: 53.37 seconds.\n",
            "-- Epoch 549\n",
            "Norm: 3026043.84, NNZs: 33, Bias: 26631.419557, T: 204611751, Avg. loss: 90489925985.591461\n",
            "Total training time: 53.48 seconds.\n",
            "-- Epoch 550\n",
            "Norm: 3022967.68, NNZs: 33, Bias: 26633.185729, T: 204984450, Avg. loss: 89927525016.884216\n",
            "Total training time: 53.58 seconds.\n",
            "-- Epoch 551\n",
            "Norm: 3020581.17, NNZs: 33, Bias: 26635.004720, T: 205357149, Avg. loss: 90012456274.080124\n",
            "Total training time: 53.67 seconds.\n",
            "-- Epoch 552\n",
            "Norm: 3018011.45, NNZs: 33, Bias: 26636.804806, T: 205729848, Avg. loss: 89936828123.980438\n",
            "Total training time: 53.77 seconds.\n",
            "-- Epoch 553\n",
            "Norm: 3015318.78, NNZs: 33, Bias: 26638.574994, T: 206102547, Avg. loss: 89732401041.735504\n",
            "Total training time: 53.86 seconds.\n",
            "-- Epoch 554\n",
            "Norm: 3012907.43, NNZs: 33, Bias: 26640.342210, T: 206475246, Avg. loss: 89434258758.808167\n",
            "Total training time: 53.96 seconds.\n",
            "-- Epoch 555\n",
            "Norm: 3010588.09, NNZs: 33, Bias: 26642.126776, T: 206847945, Avg. loss: 89109480300.189560\n",
            "Total training time: 54.06 seconds.\n",
            "-- Epoch 556\n",
            "Norm: 3008066.91, NNZs: 33, Bias: 26643.885310, T: 207220644, Avg. loss: 89003939643.658066\n",
            "Total training time: 54.16 seconds.\n",
            "-- Epoch 557\n",
            "Norm: 3005554.64, NNZs: 33, Bias: 26645.627560, T: 207593343, Avg. loss: 88765488142.272400\n",
            "Total training time: 54.26 seconds.\n",
            "-- Epoch 558\n",
            "Norm: 3003087.96, NNZs: 33, Bias: 26647.387480, T: 207966042, Avg. loss: 88710653232.766998\n",
            "Total training time: 54.36 seconds.\n",
            "-- Epoch 559\n",
            "Norm: 3000292.30, NNZs: 33, Bias: 26649.099284, T: 208338741, Avg. loss: 88450859066.726822\n",
            "Total training time: 54.45 seconds.\n",
            "-- Epoch 560\n",
            "Norm: 2998018.17, NNZs: 33, Bias: 26650.926005, T: 208711440, Avg. loss: 88498634554.225754\n",
            "Total training time: 54.55 seconds.\n",
            "-- Epoch 561\n",
            "Norm: 2995326.27, NNZs: 33, Bias: 26652.688024, T: 209084139, Avg. loss: 87963071334.394348\n",
            "Total training time: 54.65 seconds.\n",
            "-- Epoch 562\n",
            "Norm: 2992660.15, NNZs: 33, Bias: 26654.478802, T: 209456838, Avg. loss: 87851651501.724380\n",
            "Total training time: 54.74 seconds.\n",
            "-- Epoch 563\n",
            "Norm: 2990011.95, NNZs: 33, Bias: 26656.208400, T: 209829537, Avg. loss: 87864970650.081512\n",
            "Total training time: 54.84 seconds.\n",
            "-- Epoch 564\n",
            "Norm: 2987643.03, NNZs: 33, Bias: 26658.008423, T: 210202236, Avg. loss: 87999550016.379913\n",
            "Total training time: 54.95 seconds.\n",
            "-- Epoch 565\n",
            "Norm: 2985253.10, NNZs: 33, Bias: 26659.748423, T: 210574935, Avg. loss: 87363254885.092346\n",
            "Total training time: 55.05 seconds.\n",
            "-- Epoch 566\n",
            "Norm: 2982845.39, NNZs: 33, Bias: 26661.470144, T: 210947634, Avg. loss: 87505750584.270676\n",
            "Total training time: 55.15 seconds.\n",
            "-- Epoch 567\n",
            "Norm: 2980148.76, NNZs: 33, Bias: 26663.236415, T: 211320333, Avg. loss: 87173184542.947845\n",
            "Total training time: 55.25 seconds.\n",
            "-- Epoch 568\n",
            "Norm: 2977450.51, NNZs: 33, Bias: 26664.986344, T: 211693032, Avg. loss: 87104995412.519363\n",
            "Total training time: 55.34 seconds.\n",
            "-- Epoch 569\n",
            "Norm: 2974796.39, NNZs: 33, Bias: 26666.693743, T: 212065731, Avg. loss: 87034477585.614349\n",
            "Total training time: 55.44 seconds.\n",
            "-- Epoch 570\n",
            "Norm: 2972310.51, NNZs: 33, Bias: 26668.429576, T: 212438430, Avg. loss: 86631971060.877869\n",
            "Total training time: 55.54 seconds.\n",
            "-- Epoch 571\n",
            "Norm: 2969942.31, NNZs: 33, Bias: 26670.168786, T: 212811129, Avg. loss: 86501084267.379578\n",
            "Total training time: 55.63 seconds.\n",
            "-- Epoch 572\n",
            "Norm: 2967309.78, NNZs: 33, Bias: 26671.853296, T: 213183828, Avg. loss: 86638974949.704636\n",
            "Total training time: 55.73 seconds.\n",
            "-- Epoch 573\n",
            "Norm: 2964707.29, NNZs: 33, Bias: 26673.541808, T: 213556527, Avg. loss: 86702771774.534973\n",
            "Total training time: 55.83 seconds.\n",
            "-- Epoch 574\n",
            "Norm: 2962371.84, NNZs: 33, Bias: 26675.273799, T: 213929226, Avg. loss: 86433001216.677551\n",
            "Total training time: 55.92 seconds.\n",
            "-- Epoch 575\n",
            "Norm: 2959556.56, NNZs: 33, Bias: 26676.955101, T: 214301925, Avg. loss: 85886667171.871033\n",
            "Total training time: 56.03 seconds.\n",
            "-- Epoch 576\n",
            "Norm: 2957526.53, NNZs: 33, Bias: 26678.686481, T: 214674624, Avg. loss: 85594128009.277313\n",
            "Total training time: 56.13 seconds.\n",
            "-- Epoch 577\n",
            "Norm: 2954940.95, NNZs: 33, Bias: 26680.387667, T: 215047323, Avg. loss: 86005284694.085190\n",
            "Total training time: 56.22 seconds.\n",
            "-- Epoch 578\n",
            "Norm: 2952318.17, NNZs: 33, Bias: 26682.097796, T: 215420022, Avg. loss: 85904977330.206726\n",
            "Total training time: 56.32 seconds.\n",
            "-- Epoch 579\n",
            "Norm: 2949820.54, NNZs: 33, Bias: 26683.839112, T: 215792721, Avg. loss: 85693350776.376343\n",
            "Total training time: 56.42 seconds.\n",
            "-- Epoch 580\n",
            "Norm: 2947588.40, NNZs: 33, Bias: 26685.547604, T: 216165420, Avg. loss: 85624754131.758469\n",
            "Total training time: 56.51 seconds.\n",
            "-- Epoch 581\n",
            "Norm: 2945220.00, NNZs: 33, Bias: 26687.213030, T: 216538119, Avg. loss: 85061923252.771469\n",
            "Total training time: 56.61 seconds.\n",
            "-- Epoch 582\n",
            "Norm: 2942557.42, NNZs: 33, Bias: 26688.938354, T: 216910818, Avg. loss: 85482279150.463196\n",
            "Total training time: 56.71 seconds.\n",
            "-- Epoch 583\n",
            "Norm: 2940005.18, NNZs: 33, Bias: 26690.634719, T: 217283517, Avg. loss: 84593178939.214294\n",
            "Total training time: 56.80 seconds.\n",
            "-- Epoch 584\n",
            "Norm: 2937868.42, NNZs: 33, Bias: 26692.363890, T: 217656216, Avg. loss: 84767681692.787674\n",
            "Total training time: 56.91 seconds.\n",
            "-- Epoch 585\n",
            "Norm: 2935451.84, NNZs: 33, Bias: 26694.071364, T: 218028915, Avg. loss: 84873490433.524429\n",
            "Total training time: 57.01 seconds.\n",
            "-- Epoch 586\n",
            "Norm: 2932758.99, NNZs: 33, Bias: 26695.730628, T: 218401614, Avg. loss: 84169949565.444977\n",
            "Total training time: 57.11 seconds.\n",
            "-- Epoch 587\n",
            "Norm: 2930219.82, NNZs: 33, Bias: 26697.407004, T: 218774313, Avg. loss: 84005038088.655624\n",
            "Total training time: 57.21 seconds.\n",
            "-- Epoch 588\n",
            "Norm: 2928108.98, NNZs: 33, Bias: 26699.093499, T: 219147012, Avg. loss: 83910792057.687561\n",
            "Total training time: 57.31 seconds.\n",
            "-- Epoch 589\n",
            "Norm: 2925533.86, NNZs: 33, Bias: 26700.720049, T: 219519711, Avg. loss: 83986962031.503082\n",
            "Total training time: 57.42 seconds.\n",
            "-- Epoch 590\n",
            "Norm: 2923345.82, NNZs: 33, Bias: 26702.392304, T: 219892410, Avg. loss: 83713072921.479279\n",
            "Total training time: 57.51 seconds.\n",
            "-- Epoch 591\n",
            "Norm: 2920761.16, NNZs: 33, Bias: 26704.041538, T: 220265109, Avg. loss: 83470388638.723328\n",
            "Total training time: 57.61 seconds.\n",
            "-- Epoch 592\n",
            "Norm: 2918353.14, NNZs: 33, Bias: 26705.707391, T: 220637808, Avg. loss: 83356115468.044617\n",
            "Total training time: 57.71 seconds.\n",
            "-- Epoch 593\n",
            "Norm: 2916004.18, NNZs: 33, Bias: 26707.380199, T: 221010507, Avg. loss: 83797409445.032410\n",
            "Total training time: 57.80 seconds.\n",
            "-- Epoch 594\n",
            "Norm: 2913873.02, NNZs: 33, Bias: 26709.010613, T: 221383206, Avg. loss: 83274898231.884506\n",
            "Total training time: 57.90 seconds.\n",
            "-- Epoch 595\n",
            "Norm: 2911283.53, NNZs: 33, Bias: 26710.620919, T: 221755905, Avg. loss: 82964197958.635284\n",
            "Total training time: 57.99 seconds.\n",
            "-- Epoch 596\n",
            "Norm: 2909128.07, NNZs: 33, Bias: 26712.289108, T: 222128604, Avg. loss: 82808483512.011642\n",
            "Total training time: 58.10 seconds.\n",
            "-- Epoch 597\n",
            "Norm: 2906547.38, NNZs: 33, Bias: 26713.924083, T: 222501303, Avg. loss: 82658835753.561768\n",
            "Total training time: 58.20 seconds.\n",
            "-- Epoch 598\n",
            "Norm: 2904188.31, NNZs: 33, Bias: 26715.564769, T: 222874002, Avg. loss: 82587810465.442093\n",
            "Total training time: 58.30 seconds.\n",
            "-- Epoch 599\n",
            "Norm: 2901878.76, NNZs: 33, Bias: 26717.183307, T: 223246701, Avg. loss: 82141881449.710846\n",
            "Total training time: 58.39 seconds.\n",
            "-- Epoch 600\n",
            "Norm: 2899392.89, NNZs: 33, Bias: 26718.827285, T: 223619400, Avg. loss: 82460089516.497986\n",
            "Total training time: 58.50 seconds.\n",
            "-- Epoch 601\n",
            "Norm: 2897062.75, NNZs: 33, Bias: 26720.480660, T: 223992099, Avg. loss: 82214566399.752899\n",
            "Total training time: 58.59 seconds.\n",
            "-- Epoch 602\n",
            "Norm: 2894668.71, NNZs: 33, Bias: 26722.107408, T: 224364798, Avg. loss: 82137531718.734161\n",
            "Total training time: 58.69 seconds.\n",
            "-- Epoch 603\n",
            "Norm: 2892435.18, NNZs: 33, Bias: 26723.729172, T: 224737497, Avg. loss: 81943754504.450363\n",
            "Total training time: 58.79 seconds.\n",
            "-- Epoch 604\n",
            "Norm: 2890117.13, NNZs: 33, Bias: 26725.375589, T: 225110196, Avg. loss: 81998573783.334335\n",
            "Total training time: 58.89 seconds.\n",
            "-- Epoch 605\n",
            "Norm: 2887958.13, NNZs: 33, Bias: 26727.026193, T: 225482895, Avg. loss: 81843100152.037933\n",
            "Total training time: 58.98 seconds.\n",
            "-- Epoch 606\n",
            "Norm: 2885577.41, NNZs: 33, Bias: 26728.664131, T: 225855594, Avg. loss: 81442864619.875336\n",
            "Total training time: 59.08 seconds.\n",
            "-- Epoch 607\n",
            "Norm: 2883571.68, NNZs: 33, Bias: 26730.330377, T: 226228293, Avg. loss: 81596114106.315399\n",
            "Total training time: 59.19 seconds.\n",
            "-- Epoch 608\n",
            "Norm: 2881024.81, NNZs: 33, Bias: 26731.945993, T: 226600992, Avg. loss: 81436005870.396729\n",
            "Total training time: 59.29 seconds.\n",
            "-- Epoch 609\n",
            "Norm: 2879034.01, NNZs: 33, Bias: 26733.570782, T: 226973691, Avg. loss: 81186347442.290649\n",
            "Total training time: 59.38 seconds.\n",
            "-- Epoch 610\n",
            "Norm: 2876780.86, NNZs: 33, Bias: 26735.193066, T: 227346390, Avg. loss: 80972686768.676712\n",
            "Total training time: 59.48 seconds.\n",
            "-- Epoch 611\n",
            "Norm: 2874251.41, NNZs: 33, Bias: 26736.819014, T: 227719089, Avg. loss: 80783203023.542374\n",
            "Total training time: 59.58 seconds.\n",
            "-- Epoch 612\n",
            "Norm: 2871983.58, NNZs: 33, Bias: 26738.426674, T: 228091788, Avg. loss: 80943176460.332993\n",
            "Total training time: 59.68 seconds.\n",
            "-- Epoch 613\n",
            "Norm: 2869609.88, NNZs: 33, Bias: 26740.036643, T: 228464487, Avg. loss: 80737079732.922165\n",
            "Total training time: 59.77 seconds.\n",
            "-- Epoch 614\n",
            "Norm: 2867226.43, NNZs: 33, Bias: 26741.666719, T: 228837186, Avg. loss: 80367727468.552139\n",
            "Total training time: 59.87 seconds.\n",
            "-- Epoch 615\n",
            "Norm: 2864848.88, NNZs: 33, Bias: 26743.259396, T: 229209885, Avg. loss: 80304364840.514557\n",
            "Total training time: 59.97 seconds.\n",
            "-- Epoch 616\n",
            "Norm: 2862679.99, NNZs: 33, Bias: 26744.873369, T: 229582584, Avg. loss: 80487573755.437073\n",
            "Total training time: 60.07 seconds.\n",
            "-- Epoch 617\n",
            "Norm: 2860434.74, NNZs: 33, Bias: 26746.464359, T: 229955283, Avg. loss: 80261936463.424911\n",
            "Total training time: 60.19 seconds.\n",
            "-- Epoch 618\n",
            "Norm: 2858210.97, NNZs: 33, Bias: 26748.036425, T: 230327982, Avg. loss: 80027384787.018570\n",
            "Total training time: 60.28 seconds.\n",
            "-- Epoch 619\n",
            "Norm: 2855911.52, NNZs: 33, Bias: 26749.619650, T: 230700681, Avg. loss: 79873524862.412445\n",
            "Total training time: 60.38 seconds.\n",
            "-- Epoch 620\n",
            "Norm: 2853753.57, NNZs: 33, Bias: 26751.201538, T: 231073380, Avg. loss: 79701429757.277588\n",
            "Total training time: 60.49 seconds.\n",
            "-- Epoch 621\n",
            "Norm: 2851306.55, NNZs: 33, Bias: 26752.775695, T: 231446079, Avg. loss: 79696652288.300034\n",
            "Total training time: 60.59 seconds.\n",
            "-- Epoch 622\n",
            "Norm: 2848897.48, NNZs: 33, Bias: 26754.298293, T: 231818778, Avg. loss: 79274036214.880478\n",
            "Total training time: 60.69 seconds.\n",
            "-- Epoch 623\n",
            "Norm: 2846557.33, NNZs: 33, Bias: 26755.883923, T: 232191477, Avg. loss: 79368590253.744293\n",
            "Total training time: 60.79 seconds.\n",
            "-- Epoch 624\n",
            "Norm: 2844403.70, NNZs: 33, Bias: 26757.480807, T: 232564176, Avg. loss: 79237298838.679520\n",
            "Total training time: 60.89 seconds.\n",
            "-- Epoch 625\n",
            "Norm: 2842267.54, NNZs: 33, Bias: 26759.092318, T: 232936875, Avg. loss: 79215859578.992432\n",
            "Total training time: 60.99 seconds.\n",
            "-- Epoch 626\n",
            "Norm: 2840035.62, NNZs: 33, Bias: 26760.661780, T: 233309574, Avg. loss: 79311689818.869400\n",
            "Total training time: 61.10 seconds.\n",
            "-- Epoch 627\n",
            "Norm: 2837596.62, NNZs: 33, Bias: 26762.241747, T: 233682273, Avg. loss: 78867523700.981064\n",
            "Total training time: 61.19 seconds.\n",
            "-- Epoch 628\n",
            "Norm: 2835435.20, NNZs: 33, Bias: 26763.783492, T: 234054972, Avg. loss: 78755251448.250549\n",
            "Total training time: 61.29 seconds.\n",
            "-- Epoch 629\n",
            "Norm: 2833101.40, NNZs: 33, Bias: 26765.356393, T: 234427671, Avg. loss: 78607899571.274078\n",
            "Total training time: 61.39 seconds.\n",
            "-- Epoch 630\n",
            "Norm: 2830964.25, NNZs: 33, Bias: 26766.911450, T: 234800370, Avg. loss: 78313006392.729080\n",
            "Total training time: 61.49 seconds.\n",
            "-- Epoch 631\n",
            "Norm: 2828799.39, NNZs: 33, Bias: 26768.479848, T: 235173069, Avg. loss: 78326843303.791245\n",
            "Total training time: 61.58 seconds.\n",
            "-- Epoch 632\n",
            "Norm: 2826375.45, NNZs: 33, Bias: 26769.986996, T: 235545768, Avg. loss: 78179942877.465347\n",
            "Total training time: 61.68 seconds.\n",
            "-- Epoch 633\n",
            "Norm: 2824440.60, NNZs: 33, Bias: 26771.586441, T: 235918467, Avg. loss: 78413597672.197266\n",
            "Total training time: 61.77 seconds.\n",
            "-- Epoch 634\n",
            "Norm: 2822161.09, NNZs: 33, Bias: 26773.149123, T: 236291166, Avg. loss: 78391592514.606201\n",
            "Total training time: 61.87 seconds.\n",
            "-- Epoch 635\n",
            "Norm: 2820056.73, NNZs: 33, Bias: 26774.683974, T: 236663865, Avg. loss: 77859848968.453522\n",
            "Total training time: 61.97 seconds.\n",
            "-- Epoch 636\n",
            "Norm: 2817687.44, NNZs: 33, Bias: 26776.245477, T: 237036564, Avg. loss: 77996507240.282608\n",
            "Total training time: 62.06 seconds.\n",
            "-- Epoch 637\n",
            "Norm: 2815552.21, NNZs: 33, Bias: 26777.810749, T: 237409263, Avg. loss: 77367137063.956985\n",
            "Total training time: 62.16 seconds.\n",
            "-- Epoch 638\n",
            "Norm: 2813164.35, NNZs: 33, Bias: 26779.317839, T: 237781962, Avg. loss: 77367534700.514328\n",
            "Total training time: 62.26 seconds.\n",
            "-- Epoch 639\n",
            "Norm: 2810779.77, NNZs: 33, Bias: 26780.854164, T: 238154661, Avg. loss: 77237861239.875275\n",
            "Total training time: 62.35 seconds.\n",
            "-- Epoch 640\n",
            "Norm: 2808598.47, NNZs: 33, Bias: 26782.416789, T: 238527360, Avg. loss: 77297686535.189346\n",
            "Total training time: 62.45 seconds.\n",
            "-- Epoch 641\n",
            "Norm: 2806353.72, NNZs: 33, Bias: 26783.943107, T: 238900059, Avg. loss: 76915188751.623535\n",
            "Total training time: 62.55 seconds.\n",
            "-- Epoch 642\n",
            "Norm: 2804342.76, NNZs: 33, Bias: 26785.472094, T: 239272758, Avg. loss: 76907392306.088745\n",
            "Total training time: 62.64 seconds.\n",
            "-- Epoch 643\n",
            "Norm: 2802290.62, NNZs: 33, Bias: 26787.007893, T: 239645457, Avg. loss: 76856485917.102982\n",
            "Total training time: 62.74 seconds.\n",
            "-- Epoch 644\n",
            "Norm: 2800162.25, NNZs: 33, Bias: 26788.550309, T: 240018156, Avg. loss: 76915949087.320374\n",
            "Total training time: 62.84 seconds.\n",
            "-- Epoch 645\n",
            "Norm: 2797835.85, NNZs: 33, Bias: 26790.017701, T: 240390855, Avg. loss: 76551022149.695770\n",
            "Total training time: 62.94 seconds.\n",
            "-- Epoch 646\n",
            "Norm: 2795532.46, NNZs: 33, Bias: 26791.533053, T: 240763554, Avg. loss: 76727824176.549789\n",
            "Total training time: 63.03 seconds.\n",
            "-- Epoch 647\n",
            "Norm: 2793426.73, NNZs: 33, Bias: 26793.076297, T: 241136253, Avg. loss: 76375713917.457169\n",
            "Total training time: 63.13 seconds.\n",
            "-- Epoch 648\n",
            "Norm: 2791542.15, NNZs: 33, Bias: 26794.611502, T: 241508952, Avg. loss: 76248329311.774811\n",
            "Total training time: 63.23 seconds.\n",
            "-- Epoch 649\n",
            "Norm: 2789402.36, NNZs: 33, Bias: 26796.087261, T: 241881651, Avg. loss: 76072369194.913971\n",
            "Total training time: 63.32 seconds.\n",
            "-- Epoch 650\n",
            "Norm: 2787148.80, NNZs: 33, Bias: 26797.580217, T: 242254350, Avg. loss: 75809687007.387054\n",
            "Total training time: 63.42 seconds.\n",
            "-- Epoch 651\n",
            "Norm: 2785030.77, NNZs: 33, Bias: 26799.127956, T: 242627049, Avg. loss: 76062249594.438217\n",
            "Total training time: 63.52 seconds.\n",
            "-- Epoch 652\n",
            "Norm: 2783049.21, NNZs: 33, Bias: 26800.652756, T: 242999748, Avg. loss: 75812203719.868378\n",
            "Total training time: 63.62 seconds.\n",
            "-- Epoch 653\n",
            "Norm: 2781065.19, NNZs: 33, Bias: 26802.154458, T: 243372447, Avg. loss: 75751401149.107437\n",
            "Total training time: 63.72 seconds.\n",
            "-- Epoch 654\n",
            "Norm: 2778571.45, NNZs: 33, Bias: 26803.632196, T: 243745146, Avg. loss: 75355850026.048615\n",
            "Total training time: 63.82 seconds.\n",
            "-- Epoch 655\n",
            "Norm: 2776646.85, NNZs: 33, Bias: 26805.153760, T: 244117845, Avg. loss: 75481186041.162918\n",
            "Total training time: 63.91 seconds.\n",
            "-- Epoch 656\n",
            "Norm: 2774726.87, NNZs: 33, Bias: 26806.649589, T: 244490544, Avg. loss: 75337578018.269485\n",
            "Total training time: 64.02 seconds.\n",
            "-- Epoch 657\n",
            "Norm: 2772580.31, NNZs: 33, Bias: 26808.123188, T: 244863243, Avg. loss: 75262359176.236359\n",
            "Total training time: 64.11 seconds.\n",
            "-- Epoch 658\n",
            "Norm: 2770446.08, NNZs: 33, Bias: 26809.644347, T: 245235942, Avg. loss: 75344849471.538193\n",
            "Total training time: 64.21 seconds.\n",
            "-- Epoch 659\n",
            "Norm: 2768265.34, NNZs: 33, Bias: 26811.134844, T: 245608641, Avg. loss: 74995176496.494522\n",
            "Total training time: 64.31 seconds.\n",
            "-- Epoch 660\n",
            "Norm: 2765913.46, NNZs: 33, Bias: 26812.598832, T: 245981340, Avg. loss: 74836289911.669586\n",
            "Total training time: 64.41 seconds.\n",
            "-- Epoch 661\n",
            "Norm: 2763701.99, NNZs: 33, Bias: 26814.076663, T: 246354039, Avg. loss: 74578521996.577942\n",
            "Total training time: 64.51 seconds.\n",
            "-- Epoch 662\n",
            "Norm: 2761851.48, NNZs: 33, Bias: 26815.564303, T: 246726738, Avg. loss: 74694485841.806900\n",
            "Total training time: 64.60 seconds.\n",
            "-- Epoch 663\n",
            "Norm: 2759559.20, NNZs: 33, Bias: 26817.040943, T: 247099437, Avg. loss: 74480266358.730499\n",
            "Total training time: 64.70 seconds.\n",
            "-- Epoch 664\n",
            "Norm: 2757674.95, NNZs: 33, Bias: 26818.521807, T: 247472136, Avg. loss: 74729726862.851044\n",
            "Total training time: 64.79 seconds.\n",
            "-- Epoch 665\n",
            "Norm: 2755434.92, NNZs: 33, Bias: 26820.006440, T: 247844835, Avg. loss: 74466583686.525253\n",
            "Total training time: 64.89 seconds.\n",
            "-- Epoch 666\n",
            "Norm: 2753385.65, NNZs: 33, Bias: 26821.502713, T: 248217534, Avg. loss: 74021258741.471497\n",
            "Total training time: 64.99 seconds.\n",
            "-- Epoch 667\n",
            "Norm: 2751566.33, NNZs: 33, Bias: 26823.039240, T: 248590233, Avg. loss: 74302179061.587769\n",
            "Total training time: 65.08 seconds.\n",
            "-- Epoch 668\n",
            "Norm: 2749536.28, NNZs: 33, Bias: 26824.522659, T: 248962932, Avg. loss: 74290579643.606064\n",
            "Total training time: 65.19 seconds.\n",
            "-- Epoch 669\n",
            "Norm: 2747092.61, NNZs: 33, Bias: 26825.947809, T: 249335631, Avg. loss: 73860206829.041061\n",
            "Total training time: 65.28 seconds.\n",
            "-- Epoch 670\n",
            "Norm: 2744869.18, NNZs: 33, Bias: 26827.417646, T: 249708330, Avg. loss: 73880955495.385910\n",
            "Total training time: 65.38 seconds.\n",
            "-- Epoch 671\n",
            "Norm: 2742718.79, NNZs: 33, Bias: 26828.879526, T: 250081029, Avg. loss: 73533695360.485138\n",
            "Total training time: 65.48 seconds.\n",
            "-- Epoch 672\n",
            "Norm: 2740903.99, NNZs: 33, Bias: 26830.367690, T: 250453728, Avg. loss: 73561951592.506851\n",
            "Total training time: 65.58 seconds.\n",
            "-- Epoch 673\n",
            "Norm: 2738910.97, NNZs: 33, Bias: 26831.820129, T: 250826427, Avg. loss: 73683853860.625610\n",
            "Total training time: 65.67 seconds.\n",
            "-- Epoch 674\n",
            "Norm: 2736808.88, NNZs: 33, Bias: 26833.272324, T: 251199126, Avg. loss: 73576200888.934830\n",
            "Total training time: 65.77 seconds.\n",
            "-- Epoch 675\n",
            "Norm: 2734543.25, NNZs: 33, Bias: 26834.728408, T: 251571825, Avg. loss: 73119310143.672272\n",
            "Total training time: 65.87 seconds.\n",
            "-- Epoch 676\n",
            "Norm: 2732417.63, NNZs: 33, Bias: 26836.184248, T: 251944524, Avg. loss: 73049145366.604736\n",
            "Total training time: 65.97 seconds.\n",
            "-- Epoch 677\n",
            "Norm: 2730321.21, NNZs: 33, Bias: 26837.610026, T: 252317223, Avg. loss: 72869384778.928391\n",
            "Total training time: 66.06 seconds.\n",
            "-- Epoch 678\n",
            "Norm: 2728187.51, NNZs: 33, Bias: 26839.053968, T: 252689922, Avg. loss: 73136367736.064209\n",
            "Total training time: 66.15 seconds.\n",
            "-- Epoch 679\n",
            "Norm: 2726226.81, NNZs: 33, Bias: 26840.519817, T: 253062621, Avg. loss: 72629202801.088242\n",
            "Total training time: 66.25 seconds.\n",
            "-- Epoch 680\n",
            "Norm: 2724214.52, NNZs: 33, Bias: 26841.984277, T: 253435320, Avg. loss: 73007320306.391098\n",
            "Total training time: 66.35 seconds.\n",
            "-- Epoch 681\n",
            "Norm: 2722325.34, NNZs: 33, Bias: 26843.451961, T: 253808019, Avg. loss: 72266046677.440033\n",
            "Total training time: 66.44 seconds.\n",
            "-- Epoch 682\n",
            "Norm: 2720306.84, NNZs: 33, Bias: 26844.886947, T: 254180718, Avg. loss: 72028997927.927048\n",
            "Total training time: 66.54 seconds.\n",
            "-- Epoch 683\n",
            "Norm: 2718083.25, NNZs: 33, Bias: 26846.332271, T: 254553417, Avg. loss: 72330818414.390945\n",
            "Total training time: 66.64 seconds.\n",
            "-- Epoch 684\n",
            "Norm: 2715875.31, NNZs: 33, Bias: 26847.743440, T: 254926116, Avg. loss: 72238435054.684402\n",
            "Total training time: 66.74 seconds.\n",
            "-- Epoch 685\n",
            "Norm: 2713835.01, NNZs: 33, Bias: 26849.184523, T: 255298815, Avg. loss: 71980438381.967316\n",
            "Total training time: 66.83 seconds.\n",
            "-- Epoch 686\n",
            "Norm: 2711614.19, NNZs: 33, Bias: 26850.595180, T: 255671514, Avg. loss: 72215972853.564484\n",
            "Total training time: 66.94 seconds.\n",
            "-- Epoch 687\n",
            "Norm: 2709787.79, NNZs: 33, Bias: 26852.017376, T: 256044213, Avg. loss: 71971119216.441696\n",
            "Total training time: 67.03 seconds.\n",
            "-- Epoch 688\n",
            "Norm: 2707571.13, NNZs: 33, Bias: 26853.450134, T: 256416912, Avg. loss: 71767865334.412170\n",
            "Total training time: 67.13 seconds.\n",
            "-- Epoch 689\n",
            "Norm: 2705652.58, NNZs: 33, Bias: 26854.899675, T: 256789611, Avg. loss: 71548820810.034439\n",
            "Total training time: 67.23 seconds.\n",
            "-- Epoch 690\n",
            "Norm: 2703718.11, NNZs: 33, Bias: 26856.338551, T: 257162310, Avg. loss: 71919601298.418655\n",
            "Total training time: 67.34 seconds.\n",
            "-- Epoch 691\n",
            "Norm: 2701787.57, NNZs: 33, Bias: 26857.755433, T: 257535009, Avg. loss: 71740565552.130798\n",
            "Total training time: 67.44 seconds.\n",
            "-- Epoch 692\n",
            "Norm: 2699520.86, NNZs: 33, Bias: 26859.140019, T: 257907708, Avg. loss: 71164719644.123947\n",
            "Total training time: 67.53 seconds.\n",
            "-- Epoch 693\n",
            "Norm: 2697744.72, NNZs: 33, Bias: 26860.603503, T: 258280407, Avg. loss: 71369094453.990204\n",
            "Total training time: 67.63 seconds.\n",
            "-- Epoch 694\n",
            "Norm: 2695922.92, NNZs: 33, Bias: 26862.026816, T: 258653106, Avg. loss: 71316720617.263855\n",
            "Total training time: 67.73 seconds.\n",
            "-- Epoch 695\n",
            "Norm: 2693958.81, NNZs: 33, Bias: 26863.423044, T: 259025805, Avg. loss: 71198499384.144943\n",
            "Total training time: 67.83 seconds.\n",
            "-- Epoch 696\n",
            "Norm: 2692140.99, NNZs: 33, Bias: 26864.857202, T: 259398504, Avg. loss: 70755211240.913803\n",
            "Total training time: 67.92 seconds.\n",
            "-- Epoch 697\n",
            "Norm: 2690239.75, NNZs: 33, Bias: 26866.242024, T: 259771203, Avg. loss: 71042934017.410141\n",
            "Total training time: 68.02 seconds.\n",
            "-- Epoch 698\n",
            "Norm: 2688064.31, NNZs: 33, Bias: 26867.667927, T: 260143902, Avg. loss: 71163332202.320557\n",
            "Total training time: 68.11 seconds.\n",
            "-- Epoch 699\n",
            "Norm: 2686089.16, NNZs: 33, Bias: 26869.067392, T: 260516601, Avg. loss: 70681622093.940582\n",
            "Total training time: 68.21 seconds.\n",
            "-- Epoch 700\n",
            "Norm: 2684019.69, NNZs: 33, Bias: 26870.493059, T: 260889300, Avg. loss: 70552337194.037659\n",
            "Total training time: 68.31 seconds.\n",
            "-- Epoch 701\n",
            "Norm: 2682175.49, NNZs: 33, Bias: 26871.882542, T: 261261999, Avg. loss: 70600129992.813202\n",
            "Total training time: 68.41 seconds.\n",
            "-- Epoch 702\n",
            "Norm: 2680236.93, NNZs: 33, Bias: 26873.260562, T: 261634698, Avg. loss: 70394041342.699356\n",
            "Total training time: 68.51 seconds.\n",
            "-- Epoch 703\n",
            "Norm: 2678252.96, NNZs: 33, Bias: 26874.654199, T: 262007397, Avg. loss: 70239194873.623260\n",
            "Total training time: 68.61 seconds.\n",
            "-- Epoch 704\n",
            "Norm: 2676234.93, NNZs: 33, Bias: 26876.056374, T: 262380096, Avg. loss: 70257691203.095825\n",
            "Total training time: 68.71 seconds.\n",
            "-- Epoch 705\n",
            "Norm: 2674007.98, NNZs: 33, Bias: 26877.443613, T: 262752795, Avg. loss: 70110050622.882233\n",
            "Total training time: 68.80 seconds.\n",
            "-- Epoch 706\n",
            "Norm: 2672057.94, NNZs: 33, Bias: 26878.838608, T: 263125494, Avg. loss: 69972578372.148483\n",
            "Total training time: 68.90 seconds.\n",
            "-- Epoch 707\n",
            "Norm: 2670658.01, NNZs: 33, Bias: 26880.193058, T: 263498193, Avg. loss: 69736790235.206039\n",
            "Total training time: 69.00 seconds.\n",
            "-- Epoch 708\n",
            "Norm: 2668748.06, NNZs: 33, Bias: 26881.611875, T: 263870892, Avg. loss: 69844633216.392014\n",
            "Total training time: 69.09 seconds.\n",
            "-- Epoch 709\n",
            "Norm: 2666821.77, NNZs: 33, Bias: 26883.025793, T: 264243591, Avg. loss: 69445810317.226166\n",
            "Total training time: 69.19 seconds.\n",
            "-- Epoch 710\n",
            "Norm: 2665066.12, NNZs: 33, Bias: 26884.406561, T: 264616290, Avg. loss: 69522821135.751022\n",
            "Total training time: 69.30 seconds.\n",
            "-- Epoch 711\n",
            "Norm: 2663096.31, NNZs: 33, Bias: 26885.760317, T: 264988989, Avg. loss: 69320200480.826935\n",
            "Total training time: 69.40 seconds.\n",
            "-- Epoch 712\n",
            "Norm: 2661020.62, NNZs: 33, Bias: 26887.140553, T: 265361688, Avg. loss: 69506839616.496094\n",
            "Total training time: 69.49 seconds.\n",
            "-- Epoch 713\n",
            "Norm: 2659206.00, NNZs: 33, Bias: 26888.532702, T: 265734387, Avg. loss: 69612563038.057373\n",
            "Total training time: 69.58 seconds.\n",
            "-- Epoch 714\n",
            "Norm: 2657213.67, NNZs: 33, Bias: 26889.936882, T: 266107086, Avg. loss: 69518275368.155228\n",
            "Total training time: 69.69 seconds.\n",
            "-- Epoch 715\n",
            "Norm: 2655225.91, NNZs: 33, Bias: 26891.325722, T: 266479785, Avg. loss: 68911755381.246307\n",
            "Total training time: 69.78 seconds.\n",
            "-- Epoch 716\n",
            "Norm: 2653279.40, NNZs: 33, Bias: 26892.667752, T: 266852484, Avg. loss: 69124470975.322937\n",
            "Total training time: 69.88 seconds.\n",
            "-- Epoch 717\n",
            "Norm: 2651465.90, NNZs: 33, Bias: 26894.069789, T: 267225183, Avg. loss: 68925464483.790710\n",
            "Total training time: 69.98 seconds.\n",
            "-- Epoch 718\n",
            "Norm: 2649618.26, NNZs: 33, Bias: 26895.426953, T: 267597882, Avg. loss: 68715180993.733978\n",
            "Total training time: 70.07 seconds.\n",
            "-- Epoch 719\n",
            "Norm: 2647754.93, NNZs: 33, Bias: 26896.781163, T: 267970581, Avg. loss: 68755408162.242889\n",
            "Total training time: 70.16 seconds.\n",
            "-- Epoch 720\n",
            "Norm: 2645848.78, NNZs: 33, Bias: 26898.131558, T: 268343280, Avg. loss: 68828218272.530823\n",
            "Total training time: 70.27 seconds.\n",
            "-- Epoch 721\n",
            "Norm: 2644021.66, NNZs: 33, Bias: 26899.529230, T: 268715979, Avg. loss: 68996932040.553848\n",
            "Total training time: 70.36 seconds.\n",
            "-- Epoch 722\n",
            "Norm: 2642245.20, NNZs: 33, Bias: 26900.874557, T: 269088678, Avg. loss: 68646789154.123390\n",
            "Total training time: 70.45 seconds.\n",
            "-- Epoch 723\n",
            "Norm: 2640200.15, NNZs: 33, Bias: 26902.221426, T: 269461377, Avg. loss: 68349524198.047043\n",
            "Total training time: 70.56 seconds.\n",
            "-- Epoch 724\n",
            "Norm: 2638237.44, NNZs: 33, Bias: 26903.580376, T: 269834076, Avg. loss: 68165496605.324577\n",
            "Total training time: 70.65 seconds.\n",
            "-- Epoch 725\n",
            "Norm: 2636354.64, NNZs: 33, Bias: 26904.939829, T: 270206775, Avg. loss: 68114525967.174812\n",
            "Total training time: 70.75 seconds.\n",
            "-- Epoch 726\n",
            "Norm: 2634682.45, NNZs: 33, Bias: 26906.340593, T: 270579474, Avg. loss: 68349046955.319046\n",
            "Total training time: 70.84 seconds.\n",
            "-- Epoch 727\n",
            "Norm: 2632608.14, NNZs: 33, Bias: 26907.674430, T: 270952173, Avg. loss: 67699157382.988846\n",
            "Total training time: 70.94 seconds.\n",
            "-- Epoch 728\n",
            "Norm: 2630673.34, NNZs: 33, Bias: 26909.035946, T: 271324872, Avg. loss: 68092904830.591270\n",
            "Total training time: 71.04 seconds.\n",
            "-- Epoch 729\n",
            "Norm: 2628744.59, NNZs: 33, Bias: 26910.389705, T: 271697571, Avg. loss: 67495941850.685287\n",
            "Total training time: 71.13 seconds.\n",
            "-- Epoch 730\n",
            "Norm: 2626822.48, NNZs: 33, Bias: 26911.729552, T: 272070270, Avg. loss: 67811441863.246788\n",
            "Total training time: 71.24 seconds.\n",
            "-- Epoch 731\n",
            "Norm: 2624874.08, NNZs: 33, Bias: 26913.117190, T: 272442969, Avg. loss: 67723792826.986130\n",
            "Total training time: 71.34 seconds.\n",
            "-- Epoch 732\n",
            "Norm: 2622881.34, NNZs: 33, Bias: 26914.446761, T: 272815668, Avg. loss: 67354439457.819710\n",
            "Total training time: 71.43 seconds.\n",
            "-- Epoch 733\n",
            "Norm: 2621388.64, NNZs: 33, Bias: 26915.838241, T: 273188367, Avg. loss: 67531784680.589203\n",
            "Total training time: 71.53 seconds.\n",
            "-- Epoch 734\n",
            "Norm: 2619486.53, NNZs: 33, Bias: 26917.148361, T: 273561066, Avg. loss: 67184718513.320122\n",
            "Total training time: 71.63 seconds.\n",
            "-- Epoch 735\n",
            "Norm: 2617448.09, NNZs: 33, Bias: 26918.482869, T: 273933765, Avg. loss: 67024223737.779205\n",
            "Total training time: 71.73 seconds.\n",
            "-- Epoch 736\n",
            "Norm: 2615833.59, NNZs: 33, Bias: 26919.822585, T: 274306464, Avg. loss: 67419697634.332443\n",
            "Total training time: 71.82 seconds.\n",
            "-- Epoch 737\n",
            "Norm: 2614115.16, NNZs: 33, Bias: 26921.157705, T: 274679163, Avg. loss: 67178228099.054672\n",
            "Total training time: 71.92 seconds.\n",
            "-- Epoch 738\n",
            "Norm: 2612207.77, NNZs: 33, Bias: 26922.491282, T: 275051862, Avg. loss: 66874232421.546555\n",
            "Total training time: 72.01 seconds.\n",
            "-- Epoch 739\n",
            "Norm: 2610063.19, NNZs: 33, Bias: 26923.810005, T: 275424561, Avg. loss: 66706258187.602486\n",
            "Total training time: 72.11 seconds.\n",
            "-- Epoch 740\n",
            "Norm: 2608247.15, NNZs: 33, Bias: 26925.171641, T: 275797260, Avg. loss: 66611948096.494179\n",
            "Total training time: 72.21 seconds.\n",
            "-- Epoch 741\n",
            "Norm: 2606456.11, NNZs: 33, Bias: 26926.508817, T: 276169959, Avg. loss: 66976362055.036110\n",
            "Total training time: 72.31 seconds.\n",
            "-- Epoch 742\n",
            "Norm: 2604525.99, NNZs: 33, Bias: 26927.805254, T: 276542658, Avg. loss: 66449608057.620117\n",
            "Total training time: 72.41 seconds.\n",
            "-- Epoch 743\n",
            "Norm: 2602813.29, NNZs: 33, Bias: 26929.131312, T: 276915357, Avg. loss: 66302446585.492722\n",
            "Total training time: 72.51 seconds.\n",
            "-- Epoch 744\n",
            "Norm: 2600759.35, NNZs: 33, Bias: 26930.463225, T: 277288056, Avg. loss: 66318613430.295517\n",
            "Total training time: 72.61 seconds.\n",
            "-- Epoch 745\n",
            "Norm: 2598992.15, NNZs: 33, Bias: 26931.755875, T: 277660755, Avg. loss: 66257278741.642899\n",
            "Total training time: 72.70 seconds.\n",
            "-- Epoch 746\n",
            "Norm: 2597135.19, NNZs: 33, Bias: 26933.086957, T: 278033454, Avg. loss: 66334263393.259308\n",
            "Total training time: 72.80 seconds.\n",
            "-- Epoch 747\n",
            "Norm: 2595230.71, NNZs: 33, Bias: 26934.392266, T: 278406153, Avg. loss: 65963221451.570396\n",
            "Total training time: 72.90 seconds.\n",
            "-- Epoch 748\n",
            "Norm: 2593407.49, NNZs: 33, Bias: 26935.743938, T: 278778852, Avg. loss: 65874051362.546867\n",
            "Total training time: 72.99 seconds.\n",
            "-- Epoch 749\n",
            "Norm: 2591708.83, NNZs: 33, Bias: 26937.077727, T: 279151551, Avg. loss: 66222864945.609955\n",
            "Total training time: 73.09 seconds.\n",
            "-- Epoch 750\n",
            "Norm: 2589730.99, NNZs: 33, Bias: 26938.390952, T: 279524250, Avg. loss: 66139659902.117439\n",
            "Total training time: 73.19 seconds.\n",
            "-- Epoch 751\n",
            "Norm: 2587942.17, NNZs: 33, Bias: 26939.725896, T: 279896949, Avg. loss: 65767567313.767708\n",
            "Total training time: 73.29 seconds.\n",
            "-- Epoch 752\n",
            "Norm: 2586165.58, NNZs: 33, Bias: 26941.041639, T: 280269648, Avg. loss: 65904854380.622711\n",
            "Total training time: 73.39 seconds.\n",
            "-- Epoch 753\n",
            "Norm: 2584135.74, NNZs: 33, Bias: 26942.346212, T: 280642347, Avg. loss: 65689400783.484047\n",
            "Total training time: 73.48 seconds.\n",
            "-- Epoch 754\n",
            "Norm: 2582543.62, NNZs: 33, Bias: 26943.699610, T: 281015046, Avg. loss: 65702504254.404182\n",
            "Total training time: 73.58 seconds.\n",
            "-- Epoch 755\n",
            "Norm: 2580439.78, NNZs: 33, Bias: 26944.981376, T: 281387745, Avg. loss: 65235327776.827133\n",
            "Total training time: 73.69 seconds.\n",
            "-- Epoch 756\n",
            "Norm: 2578549.73, NNZs: 33, Bias: 26946.284188, T: 281760444, Avg. loss: 65328196839.531441\n",
            "Total training time: 73.78 seconds.\n",
            "-- Epoch 757\n",
            "Norm: 2576739.16, NNZs: 33, Bias: 26947.608837, T: 282133143, Avg. loss: 65139403204.100067\n",
            "Total training time: 73.87 seconds.\n",
            "-- Epoch 758\n",
            "Norm: 2575076.70, NNZs: 33, Bias: 26948.929758, T: 282505842, Avg. loss: 65240111510.545593\n",
            "Total training time: 73.97 seconds.\n",
            "-- Epoch 759\n",
            "Norm: 2573143.46, NNZs: 33, Bias: 26950.203640, T: 282878541, Avg. loss: 65195254637.832932\n",
            "Total training time: 74.07 seconds.\n",
            "-- Epoch 760\n",
            "Norm: 2571541.69, NNZs: 33, Bias: 26951.498889, T: 283251240, Avg. loss: 64862236390.189728\n",
            "Total training time: 74.16 seconds.\n",
            "-- Epoch 761\n",
            "Norm: 2569477.10, NNZs: 33, Bias: 26952.792982, T: 283623939, Avg. loss: 64861045121.072723\n",
            "Total training time: 74.26 seconds.\n",
            "-- Epoch 762\n",
            "Norm: 2567746.95, NNZs: 33, Bias: 26954.118233, T: 283996638, Avg. loss: 65061206627.507065\n",
            "Total training time: 74.36 seconds.\n",
            "-- Epoch 763\n",
            "Norm: 2565893.82, NNZs: 33, Bias: 26955.403036, T: 284369337, Avg. loss: 64812003211.448509\n",
            "Total training time: 74.45 seconds.\n",
            "-- Epoch 764\n",
            "Norm: 2564308.91, NNZs: 33, Bias: 26956.727059, T: 284742036, Avg. loss: 64841376248.845444\n",
            "Total training time: 74.55 seconds.\n",
            "-- Epoch 765\n",
            "Norm: 2562325.76, NNZs: 33, Bias: 26957.993882, T: 285114735, Avg. loss: 64304365928.673164\n",
            "Total training time: 74.65 seconds.\n",
            "-- Epoch 766\n",
            "Norm: 2560868.96, NNZs: 33, Bias: 26959.304359, T: 285487434, Avg. loss: 64643828645.375740\n",
            "Total training time: 74.75 seconds.\n",
            "-- Epoch 767\n",
            "Norm: 2558825.07, NNZs: 33, Bias: 26960.562010, T: 285860133, Avg. loss: 64201996000.046951\n",
            "Total training time: 74.84 seconds.\n",
            "-- Epoch 768\n",
            "Norm: 2556970.73, NNZs: 33, Bias: 26961.821537, T: 286232832, Avg. loss: 64148124570.418251\n",
            "Total training time: 74.94 seconds.\n",
            "-- Epoch 769\n",
            "Norm: 2555241.53, NNZs: 33, Bias: 26963.140168, T: 286605531, Avg. loss: 64764134243.022995\n",
            "Total training time: 75.03 seconds.\n",
            "-- Epoch 770\n",
            "Norm: 2553370.70, NNZs: 33, Bias: 26964.437957, T: 286978230, Avg. loss: 64188386820.230377\n",
            "Total training time: 75.13 seconds.\n",
            "-- Epoch 771\n",
            "Norm: 2551637.32, NNZs: 33, Bias: 26965.709555, T: 287350929, Avg. loss: 63965681161.416153\n",
            "Total training time: 75.23 seconds.\n",
            "-- Epoch 772\n",
            "Norm: 2549852.44, NNZs: 33, Bias: 26966.954052, T: 287723628, Avg. loss: 63950058657.009125\n",
            "Total training time: 75.33 seconds.\n",
            "-- Epoch 773\n",
            "Norm: 2548272.22, NNZs: 33, Bias: 26968.241805, T: 288096327, Avg. loss: 64248322604.835709\n",
            "Total training time: 75.43 seconds.\n",
            "-- Epoch 774\n",
            "Norm: 2546357.39, NNZs: 33, Bias: 26969.525802, T: 288469026, Avg. loss: 63771059654.266228\n",
            "Total training time: 75.53 seconds.\n",
            "-- Epoch 775\n",
            "Norm: 2544570.80, NNZs: 33, Bias: 26970.797492, T: 288841725, Avg. loss: 63723492792.231499\n",
            "Total training time: 75.63 seconds.\n",
            "-- Epoch 776\n",
            "Norm: 2542798.78, NNZs: 33, Bias: 26972.096568, T: 289214424, Avg. loss: 63752132520.978745\n",
            "Total training time: 75.73 seconds.\n",
            "-- Epoch 777\n",
            "Norm: 2541235.52, NNZs: 33, Bias: 26973.404789, T: 289587123, Avg. loss: 63641243820.248573\n",
            "Total training time: 75.83 seconds.\n",
            "-- Epoch 778\n",
            "Norm: 2539440.64, NNZs: 33, Bias: 26974.667005, T: 289959822, Avg. loss: 63656780651.671883\n",
            "Total training time: 75.93 seconds.\n",
            "-- Epoch 779\n",
            "Norm: 2537654.71, NNZs: 33, Bias: 26975.909141, T: 290332521, Avg. loss: 63389036884.897209\n",
            "Total training time: 76.03 seconds.\n",
            "-- Epoch 780\n",
            "Norm: 2536097.35, NNZs: 33, Bias: 26977.200075, T: 290705220, Avg. loss: 63371514707.240921\n",
            "Total training time: 76.12 seconds.\n",
            "-- Epoch 781\n",
            "Norm: 2534347.78, NNZs: 33, Bias: 26978.484524, T: 291077919, Avg. loss: 63137171610.102844\n",
            "Total training time: 76.23 seconds.\n",
            "-- Epoch 782\n",
            "Norm: 2532464.76, NNZs: 33, Bias: 26979.743872, T: 291450618, Avg. loss: 63053046439.427795\n",
            "Total training time: 76.32 seconds.\n",
            "-- Epoch 783\n",
            "Norm: 2530713.39, NNZs: 33, Bias: 26981.019288, T: 291823317, Avg. loss: 63063092528.370140\n",
            "Total training time: 76.42 seconds.\n",
            "-- Epoch 784\n",
            "Norm: 2528909.74, NNZs: 33, Bias: 26982.289392, T: 292196016, Avg. loss: 63092293069.374115\n",
            "Total training time: 76.52 seconds.\n",
            "-- Epoch 785\n",
            "Norm: 2527365.49, NNZs: 33, Bias: 26983.584000, T: 292568715, Avg. loss: 63063294650.051048\n",
            "Total training time: 76.61 seconds.\n",
            "-- Epoch 786\n",
            "Norm: 2525760.02, NNZs: 33, Bias: 26984.860419, T: 292941414, Avg. loss: 62805749190.391670\n",
            "Total training time: 76.71 seconds.\n",
            "-- Epoch 787\n",
            "Norm: 2524014.87, NNZs: 33, Bias: 26986.124019, T: 293314113, Avg. loss: 62842675048.043350\n",
            "Total training time: 76.81 seconds.\n",
            "-- Epoch 788\n",
            "Norm: 2522445.20, NNZs: 33, Bias: 26987.370079, T: 293686812, Avg. loss: 62556260748.609886\n",
            "Total training time: 76.90 seconds.\n",
            "-- Epoch 789\n",
            "Norm: 2520775.42, NNZs: 33, Bias: 26988.607884, T: 294059511, Avg. loss: 62394580321.239372\n",
            "Total training time: 77.00 seconds.\n",
            "-- Epoch 790\n",
            "Norm: 2518847.49, NNZs: 33, Bias: 26989.859911, T: 294432210, Avg. loss: 62841841601.699364\n",
            "Total training time: 77.10 seconds.\n",
            "-- Epoch 791\n",
            "Norm: 2517304.78, NNZs: 33, Bias: 26991.155554, T: 294804909, Avg. loss: 62486052449.758812\n",
            "Total training time: 77.19 seconds.\n",
            "-- Epoch 792\n",
            "Norm: 2515411.54, NNZs: 33, Bias: 26992.384193, T: 295177608, Avg. loss: 62420899698.889839\n",
            "Total training time: 77.29 seconds.\n",
            "-- Epoch 793\n",
            "Norm: 2513668.87, NNZs: 33, Bias: 26993.622523, T: 295550307, Avg. loss: 62080274789.280060\n",
            "Total training time: 77.40 seconds.\n",
            "-- Epoch 794\n",
            "Norm: 2511909.76, NNZs: 33, Bias: 26994.870381, T: 295923006, Avg. loss: 62134137248.950386\n",
            "Total training time: 77.50 seconds.\n",
            "-- Epoch 795\n",
            "Norm: 2510278.74, NNZs: 33, Bias: 26996.116407, T: 296295705, Avg. loss: 62269721265.515167\n",
            "Total training time: 77.60 seconds.\n",
            "-- Epoch 796\n",
            "Norm: 2508565.00, NNZs: 33, Bias: 26997.362736, T: 296668404, Avg. loss: 62145145332.664040\n",
            "Total training time: 77.70 seconds.\n",
            "-- Epoch 797\n",
            "Norm: 2506897.35, NNZs: 33, Bias: 26998.593647, T: 297041103, Avg. loss: 61929922753.704926\n",
            "Total training time: 77.79 seconds.\n",
            "-- Epoch 798\n",
            "Norm: 2505286.91, NNZs: 33, Bias: 26999.855577, T: 297413802, Avg. loss: 61993461110.065796\n",
            "Total training time: 77.90 seconds.\n",
            "-- Epoch 799\n",
            "Norm: 2503481.89, NNZs: 33, Bias: 27001.085816, T: 297786501, Avg. loss: 61863308288.776169\n",
            "Total training time: 78.00 seconds.\n",
            "-- Epoch 800\n",
            "Norm: 2501848.43, NNZs: 33, Bias: 27002.327255, T: 298159200, Avg. loss: 61589318920.394463\n",
            "Total training time: 78.10 seconds.\n",
            "-- Epoch 801\n",
            "Norm: 2499936.57, NNZs: 33, Bias: 27003.543971, T: 298531899, Avg. loss: 61618731498.025444\n",
            "Total training time: 78.20 seconds.\n",
            "-- Epoch 802\n",
            "Norm: 2498256.44, NNZs: 33, Bias: 27004.769894, T: 298904598, Avg. loss: 61714230909.204567\n",
            "Total training time: 78.30 seconds.\n",
            "-- Epoch 803\n",
            "Norm: 2496592.42, NNZs: 33, Bias: 27006.002289, T: 299277297, Avg. loss: 61450536809.925316\n",
            "Total training time: 78.39 seconds.\n",
            "-- Epoch 804\n",
            "Norm: 2494672.58, NNZs: 33, Bias: 27007.208981, T: 299649996, Avg. loss: 61226876390.433846\n",
            "Total training time: 78.50 seconds.\n",
            "-- Epoch 805\n",
            "Norm: 2492896.22, NNZs: 33, Bias: 27008.449524, T: 300022695, Avg. loss: 61338553008.600250\n",
            "Total training time: 78.59 seconds.\n",
            "-- Epoch 806\n",
            "Norm: 2491254.86, NNZs: 33, Bias: 27009.669881, T: 300395394, Avg. loss: 61180666432.187210\n",
            "Total training time: 78.68 seconds.\n",
            "-- Epoch 807\n",
            "Norm: 2489596.69, NNZs: 33, Bias: 27010.879664, T: 300768093, Avg. loss: 60999396916.446388\n",
            "Total training time: 78.79 seconds.\n",
            "-- Epoch 808\n",
            "Norm: 2487862.89, NNZs: 33, Bias: 27012.102185, T: 301140792, Avg. loss: 61353281090.460922\n",
            "Total training time: 78.88 seconds.\n",
            "-- Epoch 809\n",
            "Norm: 2486222.47, NNZs: 33, Bias: 27013.335131, T: 301513491, Avg. loss: 61005161827.502129\n",
            "Total training time: 78.98 seconds.\n",
            "-- Epoch 810\n",
            "Norm: 2484519.24, NNZs: 33, Bias: 27014.538047, T: 301886190, Avg. loss: 60923779756.223373\n",
            "Total training time: 79.08 seconds.\n",
            "-- Epoch 811\n",
            "Norm: 2483005.51, NNZs: 33, Bias: 27015.747026, T: 302258889, Avg. loss: 60829593423.624321\n",
            "Total training time: 79.17 seconds.\n",
            "-- Epoch 812\n",
            "Norm: 2481449.70, NNZs: 33, Bias: 27016.967460, T: 302631588, Avg. loss: 60915681367.202660\n",
            "Total training time: 79.27 seconds.\n",
            "-- Epoch 813\n",
            "Norm: 2479909.99, NNZs: 33, Bias: 27018.195790, T: 303004287, Avg. loss: 60585447709.313797\n",
            "Total training time: 79.37 seconds.\n",
            "-- Epoch 814\n",
            "Norm: 2478460.67, NNZs: 33, Bias: 27019.419700, T: 303376986, Avg. loss: 60935366729.564751\n",
            "Total training time: 79.47 seconds.\n",
            "-- Epoch 815\n",
            "Norm: 2476847.93, NNZs: 33, Bias: 27020.637751, T: 303749685, Avg. loss: 60531932967.136826\n",
            "Total training time: 79.56 seconds.\n",
            "-- Epoch 816\n",
            "Norm: 2475254.44, NNZs: 33, Bias: 27021.857343, T: 304122384, Avg. loss: 60524697808.718842\n",
            "Total training time: 79.67 seconds.\n",
            "-- Epoch 817\n",
            "Norm: 2473598.79, NNZs: 33, Bias: 27023.069783, T: 304495083, Avg. loss: 60535171254.709488\n",
            "Total training time: 79.77 seconds.\n",
            "-- Epoch 818\n",
            "Norm: 2471913.99, NNZs: 33, Bias: 27024.261311, T: 304867782, Avg. loss: 60497016607.959419\n",
            "Total training time: 79.87 seconds.\n",
            "-- Epoch 819\n",
            "Norm: 2470311.30, NNZs: 33, Bias: 27025.475141, T: 305240481, Avg. loss: 60373430872.797211\n",
            "Total training time: 79.97 seconds.\n",
            "-- Epoch 820\n",
            "Norm: 2468751.84, NNZs: 33, Bias: 27026.709465, T: 305613180, Avg. loss: 60163713761.420677\n",
            "Total training time: 80.07 seconds.\n",
            "-- Epoch 821\n",
            "Norm: 2466901.96, NNZs: 33, Bias: 27027.910633, T: 305985879, Avg. loss: 60161855418.538757\n",
            "Total training time: 80.16 seconds.\n",
            "-- Epoch 822\n",
            "Norm: 2465240.11, NNZs: 33, Bias: 27029.113206, T: 306358578, Avg. loss: 60162604543.292412\n",
            "Total training time: 80.26 seconds.\n",
            "-- Epoch 823\n",
            "Norm: 2463647.00, NNZs: 33, Bias: 27030.313281, T: 306731277, Avg. loss: 60130133035.391884\n",
            "Total training time: 80.36 seconds.\n",
            "-- Epoch 824\n",
            "Norm: 2462153.23, NNZs: 33, Bias: 27031.529894, T: 307103976, Avg. loss: 59935707907.299690\n",
            "Total training time: 80.46 seconds.\n",
            "-- Epoch 825\n",
            "Norm: 2460591.49, NNZs: 33, Bias: 27032.726143, T: 307476675, Avg. loss: 59929026994.012001\n",
            "Total training time: 80.56 seconds.\n",
            "-- Epoch 826\n",
            "Norm: 2458904.52, NNZs: 33, Bias: 27033.902491, T: 307849374, Avg. loss: 59430427454.936142\n",
            "Total training time: 80.65 seconds.\n",
            "-- Epoch 827\n",
            "Norm: 2457134.97, NNZs: 33, Bias: 27035.099881, T: 308222073, Avg. loss: 59725118128.928169\n",
            "Total training time: 80.75 seconds.\n",
            "-- Epoch 828\n",
            "Norm: 2455578.54, NNZs: 33, Bias: 27036.301395, T: 308594772, Avg. loss: 59716358790.864647\n",
            "Total training time: 80.85 seconds.\n",
            "-- Epoch 829\n",
            "Norm: 2454104.56, NNZs: 33, Bias: 27037.503277, T: 308967471, Avg. loss: 59612126750.555412\n",
            "Total training time: 80.94 seconds.\n",
            "-- Epoch 830\n",
            "Norm: 2452486.15, NNZs: 33, Bias: 27038.693867, T: 309340170, Avg. loss: 59444987202.244179\n",
            "Total training time: 81.04 seconds.\n",
            "-- Epoch 831\n",
            "Norm: 2450805.69, NNZs: 33, Bias: 27039.896596, T: 309712869, Avg. loss: 59492362393.872673\n",
            "Total training time: 81.14 seconds.\n",
            "Convergence after 831 epochs took 81.14 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
              "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
              "              l1_ratio=0.15, learning_rate='optimal', loss='modified_huber',\n",
              "              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',\n",
              "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
              "              validation_fraction=0.1, verbose=2, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BvSzQ09b5zN",
        "outputId": "b7156863-09c3-4ffc-f93c-b2b825a4c0e4"
      },
      "source": [
        "sgd_perf = roc_auc_score(y_test, sgd.predict(X_test))\n",
        "performance.append(sgd_perf)\n",
        "sgd_perf"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6883404668290218"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlkOsiSPc2GB"
      },
      "source": [
        "## KNeighbors Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiDbaxQ6cAk2",
        "outputId": "07609699-342c-48df-886e-e7dc648c00b5"
      },
      "source": [
        "knc = KNeighborsClassifier(n_neighbors=20, n_jobs=-1)\n",
        "knc.fit(X_train, y_train)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=-1, n_neighbors=20, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xL-xEVGtcDEw",
        "outputId": "aaaf70f9-4c08-484d-c511-498cfd5e34e8"
      },
      "source": [
        "knc_perf = roc_auc_score(y_test, knc.predict(X_test))\n",
        "performance.append(knc_perf)\n",
        "knc_perf "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6898588816507618"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCFO-vqQc6tG"
      },
      "source": [
        "## Gradient Boosting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSe5h6vHcKfR",
        "outputId": "9147905d-2626-4c7f-e678-5dfe54fd987d"
      },
      "source": [
        "gbc = GradientBoostingClassifier(n_estimators=1000, max_depth=3, verbose=2)\n",
        "gbc.fit(X_train, y_train)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py:1454: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "      Iter       Train Loss   Remaining Time \n",
            "         1           1.0014           46.94m\n",
            "         2           0.9386           46.62m\n",
            "         3           0.8825           46.44m\n",
            "         4           0.8409           46.07m\n",
            "         5           0.8063           45.89m\n",
            "         6           0.7758           45.82m\n",
            "         7           0.7502           45.64m\n",
            "         8           0.7278           45.59m\n",
            "         9           0.7094           45.53m\n",
            "        10           0.6930           45.43m\n",
            "        11           0.6809           45.27m\n",
            "        12           0.6690           45.23m\n",
            "        13           0.6593           45.13m\n",
            "        14           0.6509           45.09m\n",
            "        15           0.6355           45.05m\n",
            "        16           0.6288           44.97m\n",
            "        17           0.6181           44.91m\n",
            "        18           0.6103           44.84m\n",
            "        19           0.6055           45.04m\n",
            "        20           0.5955           45.12m\n",
            "        21           0.5881           45.19m\n",
            "        22           0.5818           45.43m\n",
            "        23           0.5779           45.59m\n",
            "        24           0.5683           45.78m\n",
            "        25           0.5654           45.89m\n",
            "        26           0.5555           46.00m\n",
            "        27           0.5518           46.06m\n",
            "        28           0.5479           46.15m\n",
            "        29           0.5454           46.18m\n",
            "        30           0.5385           46.25m\n",
            "        31           0.5359           46.28m\n",
            "        32           0.5319           46.34m\n",
            "        33           0.5289           46.37m\n",
            "        34           0.5267           46.38m\n",
            "        35           0.5242           46.43m\n",
            "        36           0.5211           46.45m\n",
            "        37           0.5195           46.46m\n",
            "        38           0.5142           46.48m\n",
            "        39           0.5112           46.47m\n",
            "        40           0.5089           46.51m\n",
            "        41           0.5058           46.49m\n",
            "        42           0.5042           46.49m\n",
            "        43           0.5025           46.52m\n",
            "        44           0.4995           46.50m\n",
            "        45           0.4978           46.51m\n",
            "        46           0.4964           46.50m\n",
            "        47           0.4945           46.51m\n",
            "        48           0.4923           46.52m\n",
            "        49           0.4911           46.50m\n",
            "        50           0.4892           46.47m\n",
            "        51           0.4876           46.46m\n",
            "        52           0.4859           46.44m\n",
            "        53           0.4840           46.41m\n",
            "        54           0.4812           46.39m\n",
            "        55           0.4800           46.39m\n",
            "        56           0.4790           46.37m\n",
            "        57           0.4773           46.35m\n",
            "        58           0.4726           46.32m\n",
            "        59           0.4716           46.30m\n",
            "        60           0.4700           46.27m\n",
            "        61           0.4694           46.26m\n",
            "        62           0.4685           46.22m\n",
            "        63           0.4671           46.18m\n",
            "        64           0.4654           46.16m\n",
            "        65           0.4646           46.14m\n",
            "        66           0.4638           46.13m\n",
            "        67           0.4625           46.10m\n",
            "        68           0.4616           46.06m\n",
            "        69           0.4605           46.01m\n",
            "        70           0.4592           45.97m\n",
            "        71           0.4582           45.93m\n",
            "        72           0.4577           45.91m\n",
            "        73           0.4572           45.88m\n",
            "        74           0.4563           45.85m\n",
            "        75           0.4551           45.79m\n",
            "        76           0.4547           45.75m\n",
            "        77           0.4541           45.73m\n",
            "        78           0.4538           45.71m\n",
            "        79           0.4525           45.68m\n",
            "        80           0.4512           45.64m\n",
            "        81           0.4502           45.62m\n",
            "        82           0.4493           45.58m\n",
            "        83           0.4483           45.55m\n",
            "        84           0.4476           45.51m\n",
            "        85           0.4470           45.48m\n",
            "        86           0.4463           45.44m\n",
            "        87           0.4458           45.40m\n",
            "        88           0.4455           45.35m\n",
            "        89           0.4448           45.31m\n",
            "        90           0.4445           45.27m\n",
            "        91           0.4432           45.23m\n",
            "        92           0.4425           45.19m\n",
            "        93           0.4413           45.15m\n",
            "        94           0.4391           45.10m\n",
            "        95           0.4367           45.06m\n",
            "        96           0.4360           45.03m\n",
            "        97           0.4341           44.97m\n",
            "        98           0.4322           44.93m\n",
            "        99           0.4315           44.88m\n",
            "       100           0.4312           44.84m\n",
            "       101           0.4310           44.79m\n",
            "       102           0.4300           44.73m\n",
            "       103           0.4298           44.69m\n",
            "       104           0.4291           44.64m\n",
            "       105           0.4287           44.61m\n",
            "       106           0.4285           44.57m\n",
            "       107           0.4283           44.53m\n",
            "       108           0.4277           44.47m\n",
            "       109           0.4269           44.43m\n",
            "       110           0.4263           44.38m\n",
            "       111           0.4260           44.34m\n",
            "       112           0.4258           44.29m\n",
            "       113           0.4256           44.25m\n",
            "       114           0.4249           44.20m\n",
            "       115           0.4244           44.15m\n",
            "       116           0.4242           44.11m\n",
            "       117           0.4238           44.06m\n",
            "       118           0.4229           44.01m\n",
            "       119           0.4214           43.96m\n",
            "       120           0.4205           43.91m\n",
            "       121           0.4192           43.86m\n",
            "       122           0.4180           43.82m\n",
            "       123           0.4178           43.77m\n",
            "       124           0.4173           43.72m\n",
            "       125           0.4165           43.66m\n",
            "       126           0.4164           43.61m\n",
            "       127           0.4158           43.57m\n",
            "       128           0.4153           43.51m\n",
            "       129           0.4144           43.47m\n",
            "       130           0.4137           43.42m\n",
            "       131           0.4132           43.37m\n",
            "       132           0.4129           43.32m\n",
            "       133           0.4124           43.27m\n",
            "       134           0.4120           43.22m\n",
            "       135           0.4114           43.17m\n",
            "       136           0.4111           43.11m\n",
            "       137           0.4107           43.06m\n",
            "       138           0.4102           43.01m\n",
            "       139           0.4097           42.96m\n",
            "       140           0.4084           42.91m\n",
            "       141           0.4083           42.86m\n",
            "       142           0.4080           42.82m\n",
            "       143           0.4078           42.77m\n",
            "       144           0.4075           42.71m\n",
            "       145           0.4069           42.66m\n",
            "       146           0.4066           42.60m\n",
            "       147           0.4064           42.54m\n",
            "       148           0.4063           42.50m\n",
            "       149           0.4058           42.45m\n",
            "       150           0.4057           42.41m\n",
            "       151           0.4055           42.36m\n",
            "       152           0.4053           42.31m\n",
            "       153           0.4052           42.26m\n",
            "       154           0.4049           42.21m\n",
            "       155           0.4047           42.17m\n",
            "       156           0.4040           42.12m\n",
            "       157           0.4035           42.06m\n",
            "       158           0.4028           42.01m\n",
            "       159           0.4021           41.96m\n",
            "       160           0.4015           41.90m\n",
            "       161           0.4002           41.86m\n",
            "       162           0.3998           41.81m\n",
            "       163           0.3993           41.75m\n",
            "       164           0.3991           41.70m\n",
            "       165           0.3988           41.66m\n",
            "       166           0.3985           41.61m\n",
            "       167           0.3980           41.56m\n",
            "       168           0.3976           41.51m\n",
            "       169           0.3973           41.46m\n",
            "       170           0.3969           41.41m\n",
            "       171           0.3965           41.36m\n",
            "       172           0.3961           41.31m\n",
            "       173           0.3959           41.25m\n",
            "       174           0.3957           41.20m\n",
            "       175           0.3956           41.15m\n",
            "       176           0.3955           41.10m\n",
            "       177           0.3949           41.04m\n",
            "       178           0.3944           40.99m\n",
            "       179           0.3943           40.94m\n",
            "       180           0.3942           40.89m\n",
            "       181           0.3938           40.84m\n",
            "       182           0.3935           40.79m\n",
            "       183           0.3933           40.74m\n",
            "       184           0.3932           40.70m\n",
            "       185           0.3931           40.65m\n",
            "       186           0.3930           40.60m\n",
            "       187           0.3929           40.56m\n",
            "       188           0.3928           40.51m\n",
            "       189           0.3927           40.45m\n",
            "       190           0.3926           40.40m\n",
            "       191           0.3924           40.35m\n",
            "       192           0.3923           40.30m\n",
            "       193           0.3922           40.25m\n",
            "       194           0.3921           40.20m\n",
            "       195           0.3920           40.14m\n",
            "       196           0.3918           40.09m\n",
            "       197           0.3908           40.04m\n",
            "       198           0.3907           39.99m\n",
            "       199           0.3904           39.95m\n",
            "       200           0.3903           39.90m\n",
            "       201           0.3902           39.86m\n",
            "       202           0.3901           39.81m\n",
            "       203           0.3900           39.76m\n",
            "       204           0.3900           39.71m\n",
            "       205           0.3897           39.66m\n",
            "       206           0.3897           39.61m\n",
            "       207           0.3896           39.57m\n",
            "       208           0.3894           39.52m\n",
            "       209           0.3885           39.47m\n",
            "       210           0.3876           39.42m\n",
            "       211           0.3875           39.36m\n",
            "       212           0.3873           39.31m\n",
            "       213           0.3873           39.25m\n",
            "       214           0.3872           39.20m\n",
            "       215           0.3871           39.15m\n",
            "       216           0.3868           39.11m\n",
            "       217           0.3864           39.08m\n",
            "       218           0.3863           39.12m\n",
            "       219           0.3860           39.08m\n",
            "       220           0.3855           39.04m\n",
            "       221           0.3845           38.99m\n",
            "       222           0.3844           38.94m\n",
            "       223           0.3836           38.89m\n",
            "       224           0.3833           38.84m\n",
            "       225           0.3829           38.80m\n",
            "       226           0.3825           38.75m\n",
            "       227           0.3822           38.70m\n",
            "       228           0.3816           38.65m\n",
            "       229           0.3812           38.60m\n",
            "       230           0.3810           38.55m\n",
            "       231           0.3807           38.50m\n",
            "       232           0.3798           38.45m\n",
            "       233           0.3796           38.41m\n",
            "       234           0.3791           38.36m\n",
            "       235           0.3788           38.31m\n",
            "       236           0.3785           38.26m\n",
            "       237           0.3784           38.21m\n",
            "       238           0.3782           38.16m\n",
            "       239           0.3779           38.11m\n",
            "       240           0.3778           38.06m\n",
            "       241           0.3775           38.01m\n",
            "       242           0.3775           37.96m\n",
            "       243           0.3768           37.91m\n",
            "       244           0.3767           37.87m\n",
            "       245           0.3764           37.82m\n",
            "       246           0.3758           37.77m\n",
            "       247           0.3750           37.72m\n",
            "       248           0.3749           37.67m\n",
            "       249           0.3747           37.62m\n",
            "       250           0.3740           37.57m\n",
            "       251           0.3737           37.52m\n",
            "       252           0.3735           37.47m\n",
            "       253           0.3734           37.42m\n",
            "       254           0.3728           37.37m\n",
            "       255           0.3720           37.32m\n",
            "       256           0.3720           37.27m\n",
            "       257           0.3713           37.22m\n",
            "       258           0.3707           37.17m\n",
            "       259           0.3705           37.11m\n",
            "       260           0.3702           37.06m\n",
            "       261           0.3701           37.01m\n",
            "       262           0.3698           36.96m\n",
            "       263           0.3696           36.91m\n",
            "       264           0.3694           36.86m\n",
            "       265           0.3692           36.81m\n",
            "       266           0.3691           36.76m\n",
            "       267           0.3689           36.72m\n",
            "       268           0.3689           36.67m\n",
            "       269           0.3688           36.62m\n",
            "       270           0.3688           36.57m\n",
            "       271           0.3687           36.53m\n",
            "       272           0.3686           36.48m\n",
            "       273           0.3686           36.43m\n",
            "       274           0.3684           36.39m\n",
            "       275           0.3682           36.34m\n",
            "       276           0.3681           36.29m\n",
            "       277           0.3681           36.25m\n",
            "       278           0.3679           36.20m\n",
            "       279           0.3678           36.15m\n",
            "       280           0.3678           36.10m\n",
            "       281           0.3677           36.06m\n",
            "       282           0.3676           36.01m\n",
            "       283           0.3676           35.96m\n",
            "       284           0.3675           35.91m\n",
            "       285           0.3673           35.86m\n",
            "       286           0.3672           35.82m\n",
            "       287           0.3671           35.77m\n",
            "       288           0.3670           35.73m\n",
            "       289           0.3669           35.68m\n",
            "       290           0.3668           35.64m\n",
            "       291           0.3667           35.59m\n",
            "       292           0.3666           35.55m\n",
            "       293           0.3665           35.51m\n",
            "       294           0.3661           35.46m\n",
            "       295           0.3661           35.41m\n",
            "       296           0.3660           35.36m\n",
            "       297           0.3660           35.32m\n",
            "       298           0.3660           35.27m\n",
            "       299           0.3659           35.22m\n",
            "       300           0.3656           35.18m\n",
            "       301           0.3654           35.13m\n",
            "       302           0.3652           35.08m\n",
            "       303           0.3652           35.03m\n",
            "       304           0.3651           34.99m\n",
            "       305           0.3651           34.94m\n",
            "       306           0.3650           34.89m\n",
            "       307           0.3650           34.85m\n",
            "       308           0.3649           34.80m\n",
            "       309           0.3645           34.75m\n",
            "       310           0.3644           34.70m\n",
            "       311           0.3643           34.66m\n",
            "       312           0.3641           34.61m\n",
            "       313           0.3640           34.56m\n",
            "       314           0.3640           34.51m\n",
            "       315           0.3639           34.46m\n",
            "       316           0.3639           34.41m\n",
            "       317           0.3637           34.37m\n",
            "       318           0.3635           34.32m\n",
            "       319           0.3630           34.27m\n",
            "       320           0.3630           34.22m\n",
            "       321           0.3629           34.17m\n",
            "       322           0.3628           34.13m\n",
            "       323           0.3627           34.08m\n",
            "       324           0.3627           34.04m\n",
            "       325           0.3626           33.99m\n",
            "       326           0.3626           33.94m\n",
            "       327           0.3622           33.89m\n",
            "       328           0.3620           33.84m\n",
            "       329           0.3620           33.80m\n",
            "       330           0.3620           33.75m\n",
            "       331           0.3619           33.70m\n",
            "       332           0.3619           33.65m\n",
            "       333           0.3618           33.61m\n",
            "       334           0.3616           33.56m\n",
            "       335           0.3616           33.51m\n",
            "       336           0.3616           33.46m\n",
            "       337           0.3616           33.41m\n",
            "       338           0.3614           33.36m\n",
            "       339           0.3612           33.31m\n",
            "       340           0.3611           33.26m\n",
            "       341           0.3610           33.22m\n",
            "       342           0.3610           33.17m\n",
            "       343           0.3609           33.12m\n",
            "       344           0.3609           33.07m\n",
            "       345           0.3608           33.02m\n",
            "       346           0.3608           32.97m\n",
            "       347           0.3607           32.92m\n",
            "       348           0.3605           32.87m\n",
            "       349           0.3605           32.83m\n",
            "       350           0.3601           32.78m\n",
            "       351           0.3598           32.73m\n",
            "       352           0.3597           32.68m\n",
            "       353           0.3597           32.63m\n",
            "       354           0.3596           32.58m\n",
            "       355           0.3596           32.53m\n",
            "       356           0.3595           32.48m\n",
            "       357           0.3594           32.43m\n",
            "       358           0.3594           32.39m\n",
            "       359           0.3593           32.34m\n",
            "       360           0.3588           32.29m\n",
            "       361           0.3585           32.24m\n",
            "       362           0.3583           32.19m\n",
            "       363           0.3581           32.15m\n",
            "       364           0.3580           32.10m\n",
            "       365           0.3580           32.05m\n",
            "       366           0.3578           32.00m\n",
            "       367           0.3577           31.95m\n",
            "       368           0.3576           31.90m\n",
            "       369           0.3576           31.85m\n",
            "       370           0.3573           31.81m\n",
            "       371           0.3569           31.76m\n",
            "       372           0.3568           31.71m\n",
            "       373           0.3568           31.66m\n",
            "       374           0.3568           31.61m\n",
            "       375           0.3567           31.56m\n",
            "       376           0.3567           31.51m\n",
            "       377           0.3567           31.46m\n",
            "       378           0.3564           31.41m\n",
            "       379           0.3564           31.36m\n",
            "       380           0.3563           31.31m\n",
            "       381           0.3563           31.26m\n",
            "       382           0.3563           31.21m\n",
            "       383           0.3563           31.16m\n",
            "       384           0.3561           31.12m\n",
            "       385           0.3561           31.07m\n",
            "       386           0.3560           31.02m\n",
            "       387           0.3560           30.97m\n",
            "       388           0.3556           30.92m\n",
            "       389           0.3556           30.87m\n",
            "       390           0.3555           30.82m\n",
            "       391           0.3555           30.77m\n",
            "       392           0.3554           30.72m\n",
            "       393           0.3554           30.68m\n",
            "       394           0.3552           30.63m\n",
            "       395           0.3547           30.58m\n",
            "       396           0.3547           30.53m\n",
            "       397           0.3547           30.48m\n",
            "       398           0.3546           30.43m\n",
            "       399           0.3546           30.37m\n",
            "       400           0.3546           30.32m\n",
            "       401           0.3546           30.27m\n",
            "       402           0.3545           30.22m\n",
            "       403           0.3545           30.17m\n",
            "       404           0.3545           30.12m\n",
            "       405           0.3543           30.07m\n",
            "       406           0.3543           30.02m\n",
            "       407           0.3542           29.97m\n",
            "       408           0.3542           29.92m\n",
            "       409           0.3542           29.87m\n",
            "       410           0.3541           29.82m\n",
            "       411           0.3541           29.78m\n",
            "       412           0.3540           29.73m\n",
            "       413           0.3540           29.68m\n",
            "       414           0.3540           29.63m\n",
            "       415           0.3539           29.58m\n",
            "       416           0.3539           29.53m\n",
            "       417           0.3538           29.48m\n",
            "       418           0.3537           29.43m\n",
            "       419           0.3537           29.37m\n",
            "       420           0.3536           29.32m\n",
            "       421           0.3536           29.27m\n",
            "       422           0.3536           29.22m\n",
            "       423           0.3535           29.17m\n",
            "       424           0.3533           29.12m\n",
            "       425           0.3533           29.07m\n",
            "       426           0.3532           29.01m\n",
            "       427           0.3531           28.96m\n",
            "       428           0.3530           28.91m\n",
            "       429           0.3530           28.86m\n",
            "       430           0.3530           28.81m\n",
            "       431           0.3529           28.76m\n",
            "       432           0.3529           28.70m\n",
            "       433           0.3529           28.65m\n",
            "       434           0.3525           28.60m\n",
            "       435           0.3522           28.55m\n",
            "       436           0.3522           28.50m\n",
            "       437           0.3521           28.45m\n",
            "       438           0.3519           28.40m\n",
            "       439           0.3519           28.35m\n",
            "       440           0.3519           28.30m\n",
            "       441           0.3518           28.24m\n",
            "       442           0.3518           28.19m\n",
            "       443           0.3518           28.14m\n",
            "       444           0.3518           28.09m\n",
            "       445           0.3517           28.04m\n",
            "       446           0.3517           27.99m\n",
            "       447           0.3516           27.94m\n",
            "       448           0.3514           27.88m\n",
            "       449           0.3511           27.83m\n",
            "       450           0.3509           27.78m\n",
            "       451           0.3508           27.73m\n",
            "       452           0.3507           27.68m\n",
            "       453           0.3505           27.62m\n",
            "       454           0.3505           27.57m\n",
            "       455           0.3504           27.52m\n",
            "       456           0.3504           27.47m\n",
            "       457           0.3502           27.42m\n",
            "       458           0.3501           27.37m\n",
            "       459           0.3499           27.32m\n",
            "       460           0.3498           27.26m\n",
            "       461           0.3497           27.21m\n",
            "       462           0.3495           27.16m\n",
            "       463           0.3494           27.11m\n",
            "       464           0.3492           27.06m\n",
            "       465           0.3488           27.00m\n",
            "       466           0.3487           26.95m\n",
            "       467           0.3486           26.90m\n",
            "       468           0.3485           26.85m\n",
            "       469           0.3484           26.80m\n",
            "       470           0.3483           26.75m\n",
            "       471           0.3483           26.70m\n",
            "       472           0.3482           26.64m\n",
            "       473           0.3481           26.59m\n",
            "       474           0.3480           26.54m\n",
            "       475           0.3480           26.49m\n",
            "       476           0.3479           26.44m\n",
            "       477           0.3478           26.39m\n",
            "       478           0.3475           26.34m\n",
            "       479           0.3474           26.29m\n",
            "       480           0.3472           26.24m\n",
            "       481           0.3471           26.19m\n",
            "       482           0.3470           26.14m\n",
            "       483           0.3469           26.09m\n",
            "       484           0.3467           26.03m\n",
            "       485           0.3465           25.98m\n",
            "       486           0.3464           25.93m\n",
            "       487           0.3463           25.88m\n",
            "       488           0.3462           25.83m\n",
            "       489           0.3462           25.78m\n",
            "       490           0.3462           25.73m\n",
            "       491           0.3461           25.68m\n",
            "       492           0.3461           25.63m\n",
            "       493           0.3460           25.58m\n",
            "       494           0.3460           25.53m\n",
            "       495           0.3460           25.48m\n",
            "       496           0.3459           25.43m\n",
            "       497           0.3459           25.38m\n",
            "       498           0.3458           25.32m\n",
            "       499           0.3458           25.27m\n",
            "       500           0.3458           25.22m\n",
            "       501           0.3456           25.18m\n",
            "       502           0.3456           25.12m\n",
            "       503           0.3456           25.07m\n",
            "       504           0.3453           25.02m\n",
            "       505           0.3452           24.97m\n",
            "       506           0.3452           24.92m\n",
            "       507           0.3452           24.87m\n",
            "       508           0.3449           24.82m\n",
            "       509           0.3448           24.77m\n",
            "       510           0.3448           24.72m\n",
            "       511           0.3448           24.67m\n",
            "       512           0.3448           24.62m\n",
            "       513           0.3447           24.57m\n",
            "       514           0.3447           24.51m\n",
            "       515           0.3445           24.46m\n",
            "       516           0.3444           24.41m\n",
            "       517           0.3442           24.36m\n",
            "       518           0.3442           24.31m\n",
            "       519           0.3441           24.26m\n",
            "       520           0.3441           24.21m\n",
            "       521           0.3440           24.16m\n",
            "       522           0.3437           24.11m\n",
            "       523           0.3437           24.05m\n",
            "       524           0.3437           24.00m\n",
            "       525           0.3437           23.95m\n",
            "       526           0.3437           23.90m\n",
            "       527           0.3436           23.85m\n",
            "       528           0.3436           23.80m\n",
            "       529           0.3436           23.74m\n",
            "       530           0.3435           23.69m\n",
            "       531           0.3435           23.64m\n",
            "       532           0.3434           23.59m\n",
            "       533           0.3434           23.54m\n",
            "       534           0.3434           23.49m\n",
            "       535           0.3434           23.44m\n",
            "       536           0.3433           23.38m\n",
            "       537           0.3433           23.33m\n",
            "       538           0.3431           23.28m\n",
            "       539           0.3429           23.23m\n",
            "       540           0.3429           23.18m\n",
            "       541           0.3427           23.13m\n",
            "       542           0.3427           23.08m\n",
            "       543           0.3426           23.03m\n",
            "       544           0.3426           22.98m\n",
            "       545           0.3426           22.92m\n",
            "       546           0.3426           22.87m\n",
            "       547           0.3426           22.82m\n",
            "       548           0.3425           22.77m\n",
            "       549           0.3425           22.72m\n",
            "       550           0.3424           22.67m\n",
            "       551           0.3424           22.62m\n",
            "       552           0.3422           22.57m\n",
            "       553           0.3422           22.51m\n",
            "       554           0.3422           22.46m\n",
            "       555           0.3422           22.41m\n",
            "       556           0.3421           22.36m\n",
            "       557           0.3421           22.31m\n",
            "       558           0.3421           22.26m\n",
            "       559           0.3421           22.20m\n",
            "       560           0.3420           22.15m\n",
            "       561           0.3418           22.10m\n",
            "       562           0.3418           22.05m\n",
            "       563           0.3418           22.00m\n",
            "       564           0.3417           21.95m\n",
            "       565           0.3417           21.90m\n",
            "       566           0.3416           21.85m\n",
            "       567           0.3416           21.80m\n",
            "       568           0.3415           21.74m\n",
            "       569           0.3415           21.69m\n",
            "       570           0.3415           21.64m\n",
            "       571           0.3415           21.59m\n",
            "       572           0.3415           21.54m\n",
            "       573           0.3415           21.49m\n",
            "       574           0.3414           21.44m\n",
            "       575           0.3414           21.39m\n",
            "       576           0.3414           21.34m\n",
            "       577           0.3414           21.29m\n",
            "       578           0.3413           21.24m\n",
            "       579           0.3413           21.19m\n",
            "       580           0.3413           21.14m\n",
            "       581           0.3413           21.09m\n",
            "       582           0.3413           21.04m\n",
            "       583           0.3413           20.99m\n",
            "       584           0.3412           20.93m\n",
            "       585           0.3412           20.88m\n",
            "       586           0.3411           20.83m\n",
            "       587           0.3411           20.78m\n",
            "       588           0.3411           20.73m\n",
            "       589           0.3411           20.68m\n",
            "       590           0.3410           20.63m\n",
            "       591           0.3410           20.58m\n",
            "       592           0.3410           20.53m\n",
            "       593           0.3410           20.48m\n",
            "       594           0.3409           20.43m\n",
            "       595           0.3409           20.37m\n",
            "       596           0.3408           20.32m\n",
            "       597           0.3408           20.27m\n",
            "       598           0.3408           20.22m\n",
            "       599           0.3408           20.17m\n",
            "       600           0.3408           20.12m\n",
            "       601           0.3408           20.07m\n",
            "       602           0.3407           20.02m\n",
            "       603           0.3407           19.97m\n",
            "       604           0.3407           19.92m\n",
            "       605           0.3407           19.87m\n",
            "       606           0.3406           19.82m\n",
            "       607           0.3405           19.76m\n",
            "       608           0.3405           19.71m\n",
            "       609           0.3405           19.66m\n",
            "       610           0.3405           19.61m\n",
            "       611           0.3404           19.56m\n",
            "       612           0.3404           19.51m\n",
            "       613           0.3403           19.46m\n",
            "       614           0.3403           19.41m\n",
            "       615           0.3403           19.36m\n",
            "       616           0.3402           19.31m\n",
            "       617           0.3402           19.26m\n",
            "       618           0.3402           19.21m\n",
            "       619           0.3402           19.16m\n",
            "       620           0.3401           19.11m\n",
            "       621           0.3399           19.06m\n",
            "       622           0.3397           19.01m\n",
            "       623           0.3396           18.96m\n",
            "       624           0.3396           18.91m\n",
            "       625           0.3395           18.85m\n",
            "       626           0.3395           18.80m\n",
            "       627           0.3395           18.75m\n",
            "       628           0.3395           18.70m\n",
            "       629           0.3394           18.65m\n",
            "       630           0.3394           18.60m\n",
            "       631           0.3394           18.55m\n",
            "       632           0.3394           18.50m\n",
            "       633           0.3393           18.45m\n",
            "       634           0.3393           18.40m\n",
            "       635           0.3393           18.35m\n",
            "       636           0.3393           18.30m\n",
            "       637           0.3393           18.24m\n",
            "       638           0.3392           18.19m\n",
            "       639           0.3391           18.14m\n",
            "       640           0.3391           18.09m\n",
            "       641           0.3391           18.04m\n",
            "       642           0.3391           17.99m\n",
            "       643           0.3391           17.94m\n",
            "       644           0.3390           17.89m\n",
            "       645           0.3390           17.84m\n",
            "       646           0.3390           17.79m\n",
            "       647           0.3390           17.74m\n",
            "       648           0.3390           17.69m\n",
            "       649           0.3389           17.64m\n",
            "       650           0.3389           17.59m\n",
            "       651           0.3389           17.54m\n",
            "       652           0.3388           17.49m\n",
            "       653           0.3388           17.44m\n",
            "       654           0.3388           17.39m\n",
            "       655           0.3387           17.34m\n",
            "       656           0.3387           17.28m\n",
            "       657           0.3387           17.23m\n",
            "       658           0.3387           17.18m\n",
            "       659           0.3386           17.13m\n",
            "       660           0.3386           17.08m\n",
            "       661           0.3386           17.03m\n",
            "       662           0.3386           16.98m\n",
            "       663           0.3385           16.93m\n",
            "       664           0.3385           16.88m\n",
            "       665           0.3385           16.83m\n",
            "       666           0.3385           16.78m\n",
            "       667           0.3384           16.73m\n",
            "       668           0.3384           16.67m\n",
            "       669           0.3384           16.62m\n",
            "       670           0.3383           16.57m\n",
            "       671           0.3383           16.52m\n",
            "       672           0.3383           16.47m\n",
            "       673           0.3382           16.42m\n",
            "       674           0.3382           16.37m\n",
            "       675           0.3382           16.32m\n",
            "       676           0.3382           16.27m\n",
            "       677           0.3382           16.22m\n",
            "       678           0.3382           16.17m\n",
            "       679           0.3382           16.12m\n",
            "       680           0.3381           16.07m\n",
            "       681           0.3381           16.01m\n",
            "       682           0.3381           15.96m\n",
            "       683           0.3381           15.91m\n",
            "       684           0.3381           15.86m\n",
            "       685           0.3380           15.81m\n",
            "       686           0.3380           15.76m\n",
            "       687           0.3380           15.71m\n",
            "       688           0.3380           15.66m\n",
            "       689           0.3379           15.61m\n",
            "       690           0.3379           15.56m\n",
            "       691           0.3379           15.51m\n",
            "       692           0.3379           15.46m\n",
            "       693           0.3379           15.41m\n",
            "       694           0.3378           15.36m\n",
            "       695           0.3378           15.31m\n",
            "       696           0.3378           15.26m\n",
            "       697           0.3378           15.21m\n",
            "       698           0.3377           15.16m\n",
            "       699           0.3376           15.11m\n",
            "       700           0.3374           15.05m\n",
            "       701           0.3373           15.00m\n",
            "       702           0.3373           14.95m\n",
            "       703           0.3372           14.90m\n",
            "       704           0.3372           14.85m\n",
            "       705           0.3372           14.80m\n",
            "       706           0.3372           14.75m\n",
            "       707           0.3371           14.70m\n",
            "       708           0.3371           14.65m\n",
            "       709           0.3371           14.60m\n",
            "       710           0.3371           14.55m\n",
            "       711           0.3370           14.50m\n",
            "       712           0.3370           14.45m\n",
            "       713           0.3370           14.40m\n",
            "       714           0.3370           14.35m\n",
            "       715           0.3370           14.30m\n",
            "       716           0.3369           14.25m\n",
            "       717           0.3369           14.20m\n",
            "       718           0.3369           14.15m\n",
            "       719           0.3369           14.09m\n",
            "       720           0.3368           14.04m\n",
            "       721           0.3368           13.99m\n",
            "       722           0.3368           13.94m\n",
            "       723           0.3367           13.89m\n",
            "       724           0.3367           13.84m\n",
            "       725           0.3365           13.79m\n",
            "       726           0.3364           13.74m\n",
            "       727           0.3364           13.69m\n",
            "       728           0.3364           13.64m\n",
            "       729           0.3364           13.59m\n",
            "       730           0.3364           13.54m\n",
            "       731           0.3364           13.48m\n",
            "       732           0.3363           13.43m\n",
            "       733           0.3363           13.38m\n",
            "       734           0.3363           13.33m\n",
            "       735           0.3361           13.28m\n",
            "       736           0.3361           13.23m\n",
            "       737           0.3360           13.18m\n",
            "       738           0.3360           13.13m\n",
            "       739           0.3360           13.08m\n",
            "       740           0.3359           13.03m\n",
            "       741           0.3359           12.98m\n",
            "       742           0.3359           12.93m\n",
            "       743           0.3359           12.88m\n",
            "       744           0.3359           12.83m\n",
            "       745           0.3358           12.78m\n",
            "       746           0.3358           12.73m\n",
            "       747           0.3357           12.67m\n",
            "       748           0.3357           12.62m\n",
            "       749           0.3357           12.57m\n",
            "       750           0.3357           12.52m\n",
            "       751           0.3357           12.47m\n",
            "       752           0.3356           12.42m\n",
            "       753           0.3356           12.37m\n",
            "       754           0.3356           12.32m\n",
            "       755           0.3356           12.27m\n",
            "       756           0.3355           12.22m\n",
            "       757           0.3355           12.17m\n",
            "       758           0.3355           12.12m\n",
            "       759           0.3355           12.07m\n",
            "       760           0.3354           12.02m\n",
            "       761           0.3354           11.97m\n",
            "       762           0.3354           11.92m\n",
            "       763           0.3354           11.87m\n",
            "       764           0.3354           11.82m\n",
            "       765           0.3353           11.76m\n",
            "       766           0.3353           11.71m\n",
            "       767           0.3353           11.66m\n",
            "       768           0.3353           11.61m\n",
            "       769           0.3352           11.56m\n",
            "       770           0.3351           11.51m\n",
            "       771           0.3351           11.46m\n",
            "       772           0.3351           11.41m\n",
            "       773           0.3351           11.36m\n",
            "       774           0.3350           11.31m\n",
            "       775           0.3350           11.26m\n",
            "       776           0.3350           11.21m\n",
            "       777           0.3350           11.16m\n",
            "       778           0.3349           11.11m\n",
            "       779           0.3349           11.06m\n",
            "       780           0.3348           11.01m\n",
            "       781           0.3348           10.95m\n",
            "       782           0.3347           10.90m\n",
            "       783           0.3347           10.85m\n",
            "       784           0.3347           10.80m\n",
            "       785           0.3347           10.75m\n",
            "       786           0.3346           10.70m\n",
            "       787           0.3346           10.65m\n",
            "       788           0.3346           10.60m\n",
            "       789           0.3346           10.55m\n",
            "       790           0.3346           10.50m\n",
            "       791           0.3346           10.45m\n",
            "       792           0.3345           10.40m\n",
            "       793           0.3345           10.35m\n",
            "       794           0.3345           10.30m\n",
            "       795           0.3344           10.25m\n",
            "       796           0.3344           10.20m\n",
            "       797           0.3344           10.15m\n",
            "       798           0.3344           10.09m\n",
            "       799           0.3344           10.04m\n",
            "       800           0.3344            9.99m\n",
            "       801           0.3343            9.94m\n",
            "       802           0.3343            9.89m\n",
            "       803           0.3343            9.84m\n",
            "       804           0.3342            9.79m\n",
            "       805           0.3342            9.74m\n",
            "       806           0.3342            9.69m\n",
            "       807           0.3342            9.64m\n",
            "       808           0.3342            9.59m\n",
            "       809           0.3341            9.54m\n",
            "       810           0.3341            9.49m\n",
            "       811           0.3341            9.44m\n",
            "       812           0.3341            9.39m\n",
            "       813           0.3341            9.34m\n",
            "       814           0.3340            9.29m\n",
            "       815           0.3340            9.24m\n",
            "       816           0.3340            9.19m\n",
            "       817           0.3340            9.14m\n",
            "       818           0.3339            9.09m\n",
            "       819           0.3339            9.04m\n",
            "       820           0.3338            8.99m\n",
            "       821           0.3338            8.94m\n",
            "       822           0.3338            8.89m\n",
            "       823           0.3338            8.83m\n",
            "       824           0.3338            8.78m\n",
            "       825           0.3337            8.73m\n",
            "       826           0.3337            8.68m\n",
            "       827           0.3337            8.63m\n",
            "       828           0.3337            8.58m\n",
            "       829           0.3337            8.53m\n",
            "       830           0.3336            8.48m\n",
            "       831           0.3336            8.43m\n",
            "       832           0.3335            8.38m\n",
            "       833           0.3334            8.33m\n",
            "       834           0.3333            8.28m\n",
            "       835           0.3332            8.23m\n",
            "       836           0.3331            8.18m\n",
            "       837           0.3331            8.13m\n",
            "       838           0.3330            8.08m\n",
            "       839           0.3330            8.03m\n",
            "       840           0.3330            7.98m\n",
            "       841           0.3329            7.93m\n",
            "       842           0.3329            7.88m\n",
            "       843           0.3329            7.83m\n",
            "       844           0.3329            7.78m\n",
            "       845           0.3328            7.73m\n",
            "       846           0.3328            7.68m\n",
            "       847           0.3328            7.63m\n",
            "       848           0.3328            7.58m\n",
            "       849           0.3328            7.53m\n",
            "       850           0.3327            7.48m\n",
            "       851           0.3327            7.43m\n",
            "       852           0.3327            7.38m\n",
            "       853           0.3327            7.33m\n",
            "       854           0.3327            7.28m\n",
            "       855           0.3326            7.23m\n",
            "       856           0.3325            7.18m\n",
            "       857           0.3324            7.13m\n",
            "       858           0.3324            7.08m\n",
            "       859           0.3323            7.03m\n",
            "       860           0.3323            6.98m\n",
            "       861           0.3323            6.93m\n",
            "       862           0.3323            6.88m\n",
            "       863           0.3323            6.83m\n",
            "       864           0.3322            6.78m\n",
            "       865           0.3322            6.73m\n",
            "       866           0.3322            6.68m\n",
            "       867           0.3322            6.63m\n",
            "       868           0.3321            6.58m\n",
            "       869           0.3321            6.53m\n",
            "       870           0.3321            6.48m\n",
            "       871           0.3320            6.43m\n",
            "       872           0.3320            6.38m\n",
            "       873           0.3320            6.33m\n",
            "       874           0.3320            6.28m\n",
            "       875           0.3320            6.23m\n",
            "       876           0.3320            6.18m\n",
            "       877           0.3320            6.13m\n",
            "       878           0.3320            6.08m\n",
            "       879           0.3319            6.04m\n",
            "       880           0.3319            5.99m\n",
            "       881           0.3319            5.94m\n",
            "       882           0.3318            5.89m\n",
            "       883           0.3318            5.84m\n",
            "       884           0.3318            5.79m\n",
            "       885           0.3318            5.74m\n",
            "       886           0.3318            5.69m\n",
            "       887           0.3317            5.64m\n",
            "       888           0.3317            5.59m\n",
            "       889           0.3317            5.54m\n",
            "       890           0.3317            5.49m\n",
            "       891           0.3316            5.44m\n",
            "       892           0.3316            5.39m\n",
            "       893           0.3316            5.34m\n",
            "       894           0.3316            5.29m\n",
            "       895           0.3316            5.24m\n",
            "       896           0.3316            5.19m\n",
            "       897           0.3316            5.14m\n",
            "       898           0.3315            5.09m\n",
            "       899           0.3314            5.04m\n",
            "       900           0.3314            4.99m\n",
            "       901           0.3313            4.94m\n",
            "       902           0.3313            4.89m\n",
            "       903           0.3313            4.84m\n",
            "       904           0.3312            4.79m\n",
            "       905           0.3312            4.74m\n",
            "       906           0.3312            4.69m\n",
            "       907           0.3312            4.64m\n",
            "       908           0.3312            4.59m\n",
            "       909           0.3312            4.54m\n",
            "       910           0.3311            4.49m\n",
            "       911           0.3311            4.44m\n",
            "       912           0.3311            4.39m\n",
            "       913           0.3311            4.34m\n",
            "       914           0.3311            4.29m\n",
            "       915           0.3311            4.24m\n",
            "       916           0.3311            4.19m\n",
            "       917           0.3310            4.14m\n",
            "       918           0.3310            4.09m\n",
            "       919           0.3310            4.04m\n",
            "       920           0.3310            3.99m\n",
            "       921           0.3309            3.94m\n",
            "       922           0.3309            3.89m\n",
            "       923           0.3309            3.84m\n",
            "       924           0.3309            3.79m\n",
            "       925           0.3309            3.74m\n",
            "       926           0.3309            3.69m\n",
            "       927           0.3308            3.64m\n",
            "       928           0.3308            3.59m\n",
            "       929           0.3308            3.54m\n",
            "       930           0.3307            3.49m\n",
            "       931           0.3307            3.44m\n",
            "       932           0.3307            3.39m\n",
            "       933           0.3307            3.34m\n",
            "       934           0.3307            3.29m\n",
            "       935           0.3306            3.24m\n",
            "       936           0.3306            3.19m\n",
            "       937           0.3306            3.14m\n",
            "       938           0.3306            3.09m\n",
            "       939           0.3306            3.04m\n",
            "       940           0.3305            2.99m\n",
            "       941           0.3305            2.94m\n",
            "       942           0.3305            2.89m\n",
            "       943           0.3305            2.84m\n",
            "       944           0.3304            2.79m\n",
            "       945           0.3304            2.74m\n",
            "       946           0.3304            2.69m\n",
            "       947           0.3304            2.64m\n",
            "       948           0.3304            2.59m\n",
            "       949           0.3303            2.54m\n",
            "       950           0.3303            2.49m\n",
            "       951           0.3303            2.44m\n",
            "       952           0.3303            2.39m\n",
            "       953           0.3303            2.34m\n",
            "       954           0.3303            2.29m\n",
            "       955           0.3302            2.24m\n",
            "       956           0.3302            2.19m\n",
            "       957           0.3302            2.14m\n",
            "       958           0.3302            2.09m\n",
            "       959           0.3301            2.04m\n",
            "       960           0.3301            1.99m\n",
            "       961           0.3301            1.94m\n",
            "       962           0.3301            1.90m\n",
            "       963           0.3301            1.85m\n",
            "       964           0.3301            1.80m\n",
            "       965           0.3301            1.75m\n",
            "       966           0.3301            1.70m\n",
            "       967           0.3300            1.65m\n",
            "       968           0.3300            1.60m\n",
            "       969           0.3300            1.55m\n",
            "       970           0.3300            1.50m\n",
            "       971           0.3300            1.45m\n",
            "       972           0.3300            1.40m\n",
            "       973           0.3299            1.35m\n",
            "       974           0.3299            1.30m\n",
            "       975           0.3299            1.25m\n",
            "       976           0.3299            1.20m\n",
            "       977           0.3298            1.15m\n",
            "       978           0.3298            1.10m\n",
            "       979           0.3298            1.05m\n",
            "       980           0.3298           59.83s\n",
            "       981           0.3297           56.84s\n",
            "       982           0.3297           53.85s\n",
            "       983           0.3296           50.85s\n",
            "       984           0.3296           47.86s\n",
            "       985           0.3296           44.87s\n",
            "       986           0.3296           41.88s\n",
            "       987           0.3295           38.89s\n",
            "       988           0.3295           35.90s\n",
            "       989           0.3295           32.91s\n",
            "       990           0.3295           29.92s\n",
            "       991           0.3294           26.92s\n",
            "       992           0.3294           23.93s\n",
            "       993           0.3294           20.94s\n",
            "       994           0.3294           17.95s\n",
            "       995           0.3294           14.96s\n",
            "       996           0.3294           11.97s\n",
            "       997           0.3293            8.97s\n",
            "       998           0.3293            5.98s\n",
            "       999           0.3293            2.99s\n",
            "      1000           0.3291            0.00s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
              "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
              "                           max_features=None, max_leaf_nodes=None,\n",
              "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                           min_samples_leaf=1, min_samples_split=2,\n",
              "                           min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
              "                           n_iter_no_change=None, presort='deprecated',\n",
              "                           random_state=None, subsample=1.0, tol=0.0001,\n",
              "                           validation_fraction=0.1, verbose=2,\n",
              "                           warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyGCGSW5cgTp",
        "outputId": "dd8bc842-6b42-4489-b26f-29bf6369ad78"
      },
      "source": [
        "gbc_perf = roc_auc_score(y_test, gbc.predict(X_test))\n",
        "performance.append(gbc_perf)\n",
        "gbc_perf "
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9058578537702224"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN5AArTEq4da"
      },
      "source": [
        "## Comparing the performance of the algorithms "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "LFhtJTa0cqHh",
        "outputId": "28ebed3a-aea9-4888-ee26-4c5db6ded1b0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar(algos, [perf_xgboost,rf_perf, sgd_perf, knc_perf, gbc_perf])\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzEwFtPVuSoP"
      },
      "source": [
        "We can clearly see that XGBoost is the best performing model, and hence we are using it for predicting on the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCEs4v2Mq9pr"
      },
      "source": [
        "## Generating the predictions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40qH0Md9qX_P"
      },
      "source": [
        "predictions = clf.predict(np.array(final_test))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGG9XzgTrDE7"
      },
      "source": [
        "submission = pd.DataFrame()\n",
        "submission['member_id'] = final_test['member_id']\n",
        "submission['loan_status'] = predictions "
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSvNt5Y0sOlr"
      },
      "source": [
        "submission.to_csv('ML_Artivatic_dataset/submission2.csv', index=False)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZu44YEcsg6L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}